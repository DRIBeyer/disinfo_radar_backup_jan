{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See:\n",
    "https://github.com/karpathy/arxiv-sanity-preserver\n",
    "\n",
    "Package does much we need, \n",
    "but we would lack internal understanding,\n",
    "until we break it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = '../data/'\n",
    "OUTPUT_PATH = '../data/raw/'\n",
    "CREDS_PATH = '../collection/credentials/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic search\n",
    "IMO use search below, this seems to artificially truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"deepfake\"\n",
    "N =  2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_by : relevance, lastUpdatedDate, or submittedDate\n",
    "# max_results : large limit\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = QUERY,\n",
    "  max_results = N, \n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in search.__dict__.keys():\n",
    "      print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick results check, can still be long to run with many results\n",
    "\n",
    "for result in search.results():\n",
    "  print(result.title + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all data, not recommended\n",
    "\n",
    "for result in search.results():     \n",
    "      for key, value in result._raw.items(): #__dict__.items(): ### this is usual way, but here dict has raw\n",
    "            print(key)\n",
    "            print(value)\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to \n",
    "\n",
    "# overwrite 'w' or append 'a'\n",
    "action = 'a'\n",
    "\n",
    "#save as \n",
    "save_as = 'arxiv_' + QUERY\n",
    "\n",
    "for result in search.results():\n",
    "      with open (OUTPUT_PATH + save_as + '.jsonl', action) as f:\n",
    "            json.dump(result._raw, f, default=str) # use raw as __dict__ has raw in it, thus more data\n",
    "            f.write('\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client search (for larger searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friendly_client = arxiv.Client(\n",
    "  page_size = 1000, # 2000 max\n",
    "  delay_seconds = 10, #3 min\n",
    "  num_retries = 5\n",
    ")\n",
    "\n",
    "action = 'a'\n",
    "\n",
    "#save as \n",
    "save_as = 'arxiv_' + QUERY\n",
    "\n",
    "# Prints 1000 titles before needing to make another request.\n",
    "for result in friendly_client.results(arxiv.Search(query=QUERY, sort_by = arxiv.SortCriterion.SubmittedDate)):\n",
    "      with open (OUTPUT_PATH + save_as + '.jsonl', action) as f:\n",
    "            json.dump(result._raw, f, default=str) # use raw as __dict__ has raw in it, thus more data\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here for loading existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check files in data folder\n",
    "datafiles = [f for f in listdir(OUTPUT_PATH) if isfile(join(OUTPUT_PATH, f))]\n",
    "print(datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df\n",
    "load_file = datafiles[1]\n",
    "\n",
    "df = pd.read_json(OUTPUT_PATH + load_file, convert_dates=True, lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da0017f62db7e548d7b812a9c7093f1ca47065612340949c7e49f676fc86bc13"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('dr_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
