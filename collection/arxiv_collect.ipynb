{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See:\n",
    "https://github.com/karpathy/arxiv-sanity-preserver\n",
    "\n",
    "Package does much we need, \n",
    "but we would lack internal understanding,\n",
    "until we break it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = '../data/'\n",
    "OUTPUT_PATH = '../data/raw/'\n",
    "CREDS_PATH = '../collection/credentials/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic search\n",
    "IMO use search below, this seems to artificially truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"deepfake\"\n",
    "N =  2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_by : relevance, lastUpdatedDate, or submittedDate\n",
    "# max_results : large limit\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = QUERY,\n",
    "  max_results = N, \n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      "id_list\n",
      "max_results\n",
      "sort_by\n",
      "sort_order\n"
     ]
    }
   ],
   "source": [
    "for key in search.__dict__.keys():\n",
    "      print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive Frequency Learning in Two-branch Face Forgery Detection\n",
      "\n",
      "Fusing Global and Local Features for Generalized AI-Synthesized Image Detection\n",
      "\n",
      "Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection\n",
      "\n",
      "A New Approach to Improve Learning-based Deepfake Detection in Realistic Conditions\n",
      "\n",
      "A Novel Framework for Assessment of Learning-based Detectors in Realistic Conditions with Application to Deepfake Detection\n",
      "\n",
      "Making DeepFakes more spurious: evading deep face forgery detection via trace removal attack\n",
      "\n",
      "Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on Synthetic Images\n",
      "\n",
      "Transferable Class-Modelling for Decentralized Source Attribution of GAN-Generated Images\n",
      "\n",
      "Fairness Evaluation in Deepfake Detection Models using Metamorphic Testing\n",
      "\n",
      "An Audio-Visual Attention Based Multimodal Network for Fake Talking Face Videos Detection\n",
      "\n",
      "Voice-Face Homogeneity Tells Deepfake\n",
      "\n",
      "Towards Benchmarking and Evaluating Deepfake Detection\n",
      "\n",
      "The Vicomtech Audio Deepfake Detection System based on Wav2Vec2 for the 2022 ADD Challenge\n",
      "\n",
      "Protecting Celebrities with Identity Consistency Transformer\n",
      "\n",
      "Self-supervised Transformer for Deepfake Detection\n",
      "\n",
      "MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual Image Assessment\n",
      "\n",
      "Deepfake Network Architecture Attribution\n",
      "\n",
      "Explainable deepfake and spoofing detection: an attack analysis using SHapley Additive exPlanations\n",
      "\n",
      "Model Attribution of Face-swap Deepfake Videos\n",
      "\n",
      "Human Detection of Political Deepfakes across Transcripts, Audio, and Video\n",
      "\n",
      "Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation\n",
      "\n",
      "Deepfake Detection for Facial Images with Facemasks\n",
      "\n",
      "Seeing is Living? Rethinking the Security of Facial Liveness Verification in the Deepfake Era\n",
      "\n",
      "ADD 2022: the First Audio Deep Synthesis Detection Challenge\n",
      "\n",
      "Robust Deepfake On Unrestricted Media: Generation And Detection\n",
      "\n",
      "A Review of Deep Learning-based Approaches for Deepfake Content Detection\n",
      "\n",
      "Towards Adversarially Robust Deepfake Detection: An Ensemble Approach\n",
      "\n",
      "Detecting and Localizing Copy-Move and Image-Splicing Forgery\n",
      "\n",
      "FrePGAN: Robust Deepfake Detection Using Frequency-level Perturbations\n",
      "\n",
      "Block shuffling learning for Deepfake Detection\n",
      "\n",
      "Deepfake pornography as a male gaze on fan culture\n",
      "\n",
      "Detection of fake faces in videos\n",
      "\n",
      "StolenEncoder: Stealing Pre-trained Encoders\n",
      "\n",
      "DA-FDFtNet: Dual Attention Fake Detection Fine-tuning Network to Detect Various AI-Generated Fake Images\n",
      "\n",
      "Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion\n",
      "\n",
      "ADD: Frequency Attention and Multi-View based Knowledge Distillation to Detect Low-Quality Compressed Deepfake Images\n",
      "\n",
      "Physics guided deep learning generative models for crystal materials discovery\n",
      "\n",
      "Audio Deepfake Perceptions in College Going Populations\n",
      "\n",
      "How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey\n",
      "\n",
      "Investigating self-supervised front ends for speech spoofing countermeasures\n",
      "\n",
      "FakeTransformer: Exposing Face Forgery From Spatial-Temporal Representation Modeled By Facial Pixel Variations\n",
      "\n",
      "Impact of Benign Modifications on Discriminative Performance of Deepfake Detectors\n",
      "\n",
      "WaveFake: A Data Set to Facilitate Audio Deepfake Detection\n",
      "\n",
      "Exposing Deepfake with Pixel-wise AR and PPG Correlation from Faint Signals\n",
      "\n",
      "Explaining deep learning models for spoofing and deepfake detection with SHapley Additive exPlanations\n",
      "\n",
      "Attack as the Best Defense: Nullifying Image-to-image Translation GANs via Limit-aware Adversarial Attack\n",
      "\n",
      "An Experimental Evaluation on Deepfake Detection using Deep Face Recognition\n",
      "\n",
      "Improved Xception with Dual Attention Mechanism and Feature Fusion for Face Forgery Detection\n",
      "\n",
      "Machine Learning based Medical Image Deepfake Detection: A Comparative Study\n",
      "\n",
      "Finding Facial Forgery Artifacts with Parts-Based Detectors\n",
      "\n",
      "MD-CSDNetwork: Multi-Domain Cross Stitched Network for Deepfake Detection\n",
      "\n",
      "FaceGuard: Proactive Deepfake Detection\n",
      "\n",
      "Challenges and Solutions in DeepFakes\n",
      "\n",
      "Detection of GAN-synthesized street videos\n",
      "\n",
      "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors\n",
      "\n",
      "DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning\n",
      "\n",
      "Spatiotemporal Inconsistency Learning for DeepFake Video Detection\n",
      "\n",
      "ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection\n",
      "\n",
      "ASVspoof 2021: Automatic Speaker Verification Spoofing and Countermeasures Challenge Evaluation Plan\n",
      "\n",
      "DeepFake Detection with Inconsistent Head Poses: Reproducibility and Analysis\n",
      "\n",
      "DeepFake MNIST+: A DeepFake Facial Animation Dataset\n",
      "\n",
      "BiHPF: Bilateral High-Pass Filters for Robust Deepfake Detection\n",
      "\n",
      "Deepfake Representation with Multilinear Regression\n",
      "\n",
      "Video Transformer for Deepfake Detection with Incremental Learning\n",
      "\n",
      "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset\n",
      "\n",
      "Creation and Detection of German Voice Deepfakes\n",
      "\n",
      "OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild\n",
      "\n",
      "End-to-End Spectro-Temporal Graph Attention Networks for Speaker Verification Anti-Spoofing and Speech Deepfake Detection\n",
      "\n",
      "Raw Differentiable Architecture Search for Speech Deepfake and Spoofing Detection\n",
      "\n",
      "UR Channel-Robust Synthetic Speech Detection System for ASVspoof 2021\n",
      "\n",
      "Generative Models for Security: Attacks, Defenses, and Opportunities\n",
      "\n",
      "Human Perception of Audio Deepfakes\n",
      "\n",
      "MMSys'21 Grand Challenge on Detecting Cheapfakes\n",
      "\n",
      "Combining EfficientNet and Vision Transformers for Video Deepfake Detection\n",
      "\n",
      "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation\n",
      "\n",
      "Understanding the Security of Deepfake Detection\n",
      "\n",
      "FFR_FD: Effective and Fast Detection of DeepFakes Based on Feature Point Defects\n",
      "\n",
      "Detection of Deepfake Videos Using Long Distance Attention\n",
      "\n",
      "Reporting Revenge Porn: a Preliminary Expert Analysis\n",
      "\n",
      "Automated Deepfake Detection\n",
      "\n",
      "Wavelet-Packets for Deepfake Image Analysis and Detection\n",
      "\n",
      "Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images\n",
      "\n",
      "Imperceptible Adversarial Examples for Fake Image Detection\n",
      "\n",
      "DFGC 2021: A DeepFake Game Competition\n",
      "\n",
      "Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis\n",
      "\n",
      "FReTAL: Generalizing Deepfake Detection using Knowledge Distillation and Representation Learning\n",
      "\n",
      "Multi-Modal Semantic Inconsistency Detection in Social Media News Posts\n",
      "\n",
      "CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes\n",
      "\n",
      "Deepfake Detection by Human Crowds, Machines, and Machine-informed Crowds\n",
      "\n",
      "TAR: Generalized Forensic Framework to Detect Deepfakes using Weakly Supervised Learning\n",
      "\n",
      "What's wrong with this video? Comparing Explainers for Deepfake Detection\n",
      "\n",
      "One Shot Face Swapping on Megapixels\n",
      "\n",
      "An Examination of Fairness of AI Models for Deepfake Detection\n",
      "\n",
      "Deep Insights of Deepfake Technology : A Review\n",
      "\n",
      "One Detector to Rule Them All: Towards a General Deepfake Attack Detection Framework\n",
      "\n",
      "DeepfakeUCL: Deepfake Detection via Unsupervised Contrastive Learning\n",
      "\n",
      "M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection\n",
      "\n",
      "Improving the Efficiency and Robustness of Deepfakes Detection through Precise Geometric Features\n",
      "\n",
      "Generalized Spoofing Detection Inspired from Audio Generation Artifacts\n",
      "\n",
      "Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection\n",
      "\n",
      "Contrastive Learning of Global-Local Video Representations\n",
      "\n",
      "Towards Measuring Fairness in AI: the Casual Conversations Dataset\n",
      "\n",
      "Deepfake Detection Scheme Based on Vision Transformer and Distillation\n",
      "\n",
      "MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes\n",
      "\n",
      "Deepfake Forensics via An Adversarial Game\n",
      "\n",
      "KoDF: A Large-scale Korean DeepFake Detection Dataset\n",
      "\n",
      "Pros and Cons of GAN Evaluation Measures: New Developments\n",
      "\n",
      "DefakeHop: A Light-Weight High-Performance Deepfake Detector\n",
      "\n",
      "Deepfake Videos in the Wild: Analysis and Detection\n",
      "\n",
      "Multi-attentional Deepfake Detection\n",
      "\n",
      "DeepFake-o-meter: An Open Platform for DeepFake Detection\n",
      "\n",
      "Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web APIs under Deepfake Impersonation Attack\n",
      "\n",
      "Countering Malicious DeepFakes: Survey, Battleground, and Horizon\n",
      "\n",
      "Deepfakes Generation and Detection: State-of-the-art, open challenges, countermeasures, and way forward\n",
      "\n",
      "Deepfake Video Detection Using Convolutional Vision Transformer\n",
      "\n",
      "Towards Solving the DeepFake Problem : An Analysis on Improving DeepFake Detection using Dynamic Face Augmentation\n",
      "\n",
      "Could you become more credible by being White? Assessing Impact of Race on Credibility with Deepfakes\n",
      "\n",
      "The Deepfake Detection Dilemma: A Multistakeholder Exploration of Adversarial Dynamics in Synthetic Media\n",
      "\n",
      "Adversarially robust deepfake media detection using fused convolutional neural network predictions\n",
      "\n",
      "Landmark Breaker: Obstructing DeepFake By Disturbing Landmark Extraction\n",
      "\n",
      "Detecting Deepfake Videos Using Euler Video Magnification\n",
      "\n",
      "Fighting deepfakes by detecting GAN DCT anomalies\n",
      "\n",
      "BERT Transformer model for Detecting Arabic GPT2 Auto-Generated Tweets\n",
      "\n",
      "Deepfakes and the 2020 US elections: what (did not) happen\n",
      "\n",
      "COSMOS: Catching Out-of-Context Misinformation with Self-Supervised Learning\n",
      "\n",
      "FakeBuster: A DeepFakes Detection Tool for Video Conferencing Scenarios\n",
      "\n",
      "Exploring Adversarial Fake Images on Face Manifold\n",
      "\n",
      "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection\n",
      "\n",
      "Identifying Invariant Texture Violation for Robust Deepfake Detection\n",
      "\n",
      "Learning Self-Consistency for Deepfake Detection\n",
      "\n",
      "The Emerging Threats of Deepfake Attacks and Countermeasures\n",
      "\n",
      "Edited Media Understanding: Reasoning About Implications of Manipulated Images\n",
      "\n",
      "Cost Sensitive Optimization of Deepfake Detector\n",
      "\n",
      "Technology-driven Alteration of Nonverbal Cues and its Effects on Negotiation\n",
      "\n",
      "Identity-Driven DeepFake Detection\n",
      "\n",
      "ID-Reveal: Identity-aware DeepFake Video Detection\n",
      "\n",
      "Adversarial Threats to DeepFake Detection: A Practical Perspective\n",
      "\n",
      "Training Strategies and Data Augmentations in CNN-based DeepFake Video Detection\n",
      "\n",
      "Using GANs to Synthesise Minimum Training Data for Deepfake Generation\n",
      "\n",
      "Detection and Evaluation of human and machine generated speech in spoofing attacks on automatic speaker verification systems\n",
      "\n",
      "AOT: Appearance Optimal Transport Based Identity Swapping for Forgery Detection\n",
      "\n",
      "Spatio-temporal Features for Generalized Detection of Deepfake Videos\n",
      "\n",
      "Neural Deepfake Detection with Factual Structure of Text\n",
      "\n",
      "DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation\n",
      "\n",
      "FakeTagger: Robust Safeguards against DeepFake Dissemination via Provenance Tracking\n",
      "\n",
      "Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering\n",
      "\n",
      "A Convolutional LSTM based Residual Network for Deepfake Video Detection\n",
      "\n",
      "Deepfake detection: humans vs. machines\n",
      "\n",
      "DeepFake Detection Based on the Discrepancy Between the Face and its Context\n",
      "\n",
      "Privacy Intelligence: A Survey on Image Privacy in Online Social Networks\n",
      "\n",
      "On Attribution of Deepfakes\n",
      "\n",
      "Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection\n",
      "\n",
      "Sharp Multiple Instance Learning for DeepFake Video Detection\n",
      "\n",
      "Two-branch Recurrent Network for Isolating Deepfakes in Videos\n",
      "\n",
      "Fighting Deepfake by Exposing the Convolutional Traces on Images\n",
      "\n",
      "TweepFake: about Detecting Deepfake Tweets\n",
      "\n",
      "Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data\n",
      "\n",
      "Detecting Deepfake Videos: An Analysis of Three Techniques\n",
      "\n",
      "Interpretable and Trustworthy Deepfake Detection via Dynamic Prototypes\n",
      "\n",
      "Deepfake Detection using Spatiotemporal Convolutional Networks\n",
      "\n",
      "OGAN: Disrupting Deepfakes with an Adversarial Attack that Survives Training\n",
      "\n",
      "DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms\n",
      "\n",
      "FakePolisher: Making DeepFakes More Detection-Evasive by Shallow Reconstruction\n",
      "\n",
      "Defending against GAN-based Deepfake Attacks via Transformation-aware Adversarial Faces\n",
      "\n",
      "The DeepFake Detection Challenge (DFDC) Dataset\n",
      "\n",
      "Investigating the Impact of Pre-processing and Prediction Aggregation on the DeepFake Detection Task\n",
      "\n",
      "The eyes know it: FakeET -- An Eye-tracking Database to Understand Deepfake Perception\n",
      "\n",
      "Protecting Against Image Translation Deepfakes by Leaking Universal Perturbations from Black-Box Neural Networks\n",
      "\n",
      "A Note on Deepfake Detection with Low-Resources\n",
      "\n",
      "Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization\n",
      "\n",
      "DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake Voices\n",
      "\n",
      "DeepFake: Deep Dueling-based Deception Strategy to Defeat Reactive Jammers\n",
      "\n",
      "DeepFaceLab: Integrated, flexible and extensible face-swapping framework\n",
      "\n",
      "Fake face detection via adaptive manipulation traces extraction network\n",
      "\n",
      "Deepfake Forensics Using Recurrent Neural Networks\n",
      "\n",
      "Deepfake Video Forensics based on Transfer Learning\n",
      "\n",
      "Preliminary Forensics Analysis of DeepFake Images\n",
      "\n",
      "Deepfakes Detection with Automatic Face Weighting\n",
      "\n",
      "The Creation and Detection of Deepfakes: A Survey\n",
      "\n",
      "DeepFake Detection by Analyzing Convolutional Traces\n",
      "\n",
      "Video Face Manipulation Detection Through Ensemble of CNNs\n",
      "\n",
      "DeepFakes Evolution: Analysis of Facial Regions and Fake Detection Performance\n",
      "\n",
      "Evading Deepfake-Image Detectors with White- and Black-Box Attacks\n",
      "\n",
      "Adversarial Perturbations Fool Deepfake Detectors\n",
      "\n",
      "Detecting Deepfakes with Metric Learning\n",
      "\n",
      "Emotions Don't Lie: An Audio-Visual Deepfake Detection Method Using Affective Cues\n",
      "\n",
      "DeepFake Detection: Current Challenges and Next Steps\n",
      "\n",
      "Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are Failing to Reproduce Spectral Distributions\n",
      "\n",
      "Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems\n",
      "\n",
      "Amplifying The Uncanny\n",
      "\n",
      "Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples\n",
      "\n",
      "Deepfakes for Medical Video De-Identification: Privacy Protection and Diagnostic Information Preservation\n",
      "\n",
      "FakeLocator: Robust Localization of GAN-Based Face Manipulations\n",
      "\n",
      "Detecting Face2Face Facial Reenactment in Videos\n",
      "\n",
      "Media Forensics and DeepFakes: an overview\n",
      "\n",
      "Advbox: a toolbox to generate adversarial examples that fool neural networks\n",
      "\n",
      "FDFtNet: Facing Off Fake Images using Fake Detection Fine-tuning Network\n",
      "\n",
      "DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection\n",
      "\n",
      "Face X-ray for More General Face Forgery Detection\n",
      "\n",
      "CNN-generated images are surprisingly easy to spot... for now\n",
      "\n",
      "Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation\n",
      "\n",
      "Unmasking DeepFakes with simple Features\n",
      "\n",
      "Use of a Capsule Network to Detect Fake Images and Videos\n",
      "\n",
      "The Deepfake Detection Challenge (DFDC) Preview Dataset\n",
      "\n",
      "Adversarial Learning of Deepfakes in Accounting\n",
      "\n",
      "Vulnerability of Face Recognition to Deep Morphing\n",
      "\n",
      "Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics\n",
      "\n",
      "Deep Learning for Deepfakes Creation and Detection: A Survey\n",
      "\n",
      "Towards Generalizable Deepfake Detection with Locality-aware AutoEncoder\n",
      "\n",
      "Limits of Deepfake Detection: A Robust Estimation Viewpoint\n",
      "\n",
      "Recurrent Convolutional Strategies for Face Manipulation Detection in Videos\n",
      "\n",
      "Fake News, Disinformation, and Deepfakes: Leveraging Distributed Ledger Technologies and Blockchain to Combat Digital Deception and Counterfeit Reality\n",
      "\n",
      "Detecting GAN generated Fake Images using Co-occurrence Matrices\n",
      "\n",
      "FaceForensics++: Learning to Detect Manipulated Facial Images\n",
      "\n",
      "DeepFakes: a New Threat to Face Recognition? Assessment and Detection\n",
      "\n",
      "Detecting GAN-generated Imagery using Color Cues\n",
      "\n",
      "Exposing DeepFake Videos By Detecting Face Warping Artifacts\n",
      "\n",
      "MesoNet: a Compact Facial Video Forgery Detection Network\n",
      "\n",
      "In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye Blinking\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick results check, can still be long to run with many results\n",
    "\n",
    "for result in search.results():\n",
    "  print(result.title + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "http://arxiv.org/abs/2203.14315v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.14315v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-27T14:25:52Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=27, tm_hour=14, tm_min=25, tm_sec=52, tm_wday=6, tm_yday=86, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-27T14:25:52Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=27, tm_hour=14, tm_min=25, tm_sec=52, tm_wday=6, tm_yday=86, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Adaptive Frequency Learning in Two-branch Face Forgery Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Adaptive Frequency Learning in Two-branch Face Forgery Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Face forgery has attracted increasing attention in recent applications of\n",
      "computer vision. Existing detection techniques using the two-branch framework\n",
      "benefit a lot from a frequency perspective, yet are restricted by their fixed\n",
      "frequency decomposition and transform. In this paper, we propose to Adaptively\n",
      "learn Frequency information in the two-branch Detection framework, dubbed AFD.\n",
      "To be specific, we automatically learn decomposition in the frequency domain by\n",
      "introducing heterogeneity constraints, and propose an attention-based module to\n",
      "adaptively incorporate frequency features into spatial clues. Then we liberate\n",
      "our network from the fixed frequency transforms, and achieve better performance\n",
      "with our data- and task-dependent transform layers. Extensive experiments show\n",
      "that AFD generally outperforms.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Face forgery has attracted increasing attention in recent applications of\\ncomputer vision. Existing detection techniques using the two-branch framework\\nbenefit a lot from a frequency perspective, yet are restricted by their fixed\\nfrequency decomposition and transform. In this paper, we propose to Adaptively\\nlearn Frequency information in the two-branch Detection framework, dubbed AFD.\\nTo be specific, we automatically learn decomposition in the frequency domain by\\nintroducing heterogeneity constraints, and propose an attention-based module to\\nadaptively incorporate frequency features into spatial clues. Then we liberate\\nour network from the fixed frequency transforms, and achieve better performance\\nwith our data- and task-dependent transform layers. Extensive experiments show\\nthat AFD generally outperforms.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Neng Wang'}, {'name': 'Yang Bai'}, {'name': 'Kun Yu'}, {'name': 'Yong Jiang'}, {'name': 'Shu-tao Xia'}, {'name': 'Yan Wang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yan Wang'}\n",
      "\n",
      "\n",
      "author\n",
      "Yan Wang\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Deepfake Detection\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.14315v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.14315v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.13964v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.13964v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-26T01:55:37Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=26, tm_hour=1, tm_min=55, tm_sec=37, tm_wday=5, tm_yday=85, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-26T01:55:37Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=26, tm_hour=1, tm_min=55, tm_sec=37, tm_wday=5, tm_yday=85, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Fusing Global and Local Features for Generalized AI-Synthesized Image\n",
      "  Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Fusing Global and Local Features for Generalized AI-Synthesized Image\\n  Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the development of the Generative Adversarial Networks (GANs) and\n",
      "DeepFakes, AI-synthesized images are now of such high quality that humans can\n",
      "hardly distinguish them from real images. It is imperative for media forensics\n",
      "to develop detectors to expose them accurately. Existing detection methods have\n",
      "shown high performance in generated images detection, but they tend to\n",
      "generalize poorly in the real-world scenarios, where the synthetic images are\n",
      "usually generated with unseen models using unknown source data. In this work,\n",
      "we emphasize the importance of combining information from the whole image and\n",
      "informative patches in improving the generalization ability of AI-synthesized\n",
      "image detection. Specifically, we design a two-branch model to combine global\n",
      "spatial information from the whole image and local informative features from\n",
      "multiple patches selected by a novel patch selection module. Multi-head\n",
      "attention mechanism is further utilized to fuse the global and local features.\n",
      "We collect a highly diverse dataset synthesized by 19 models with various\n",
      "objects and resolutions to evaluate our model. Experimental results demonstrate\n",
      "the high accuracy and good generalization ability of our method in detecting\n",
      "generated images.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'With the development of the Generative Adversarial Networks (GANs) and\\nDeepFakes, AI-synthesized images are now of such high quality that humans can\\nhardly distinguish them from real images. It is imperative for media forensics\\nto develop detectors to expose them accurately. Existing detection methods have\\nshown high performance in generated images detection, but they tend to\\ngeneralize poorly in the real-world scenarios, where the synthetic images are\\nusually generated with unseen models using unknown source data. In this work,\\nwe emphasize the importance of combining information from the whole image and\\ninformative patches in improving the generalization ability of AI-synthesized\\nimage detection. Specifically, we design a two-branch model to combine global\\nspatial information from the whole image and local informative features from\\nmultiple patches selected by a novel patch selection module. Multi-head\\nattention mechanism is further utilized to fuse the global and local features.\\nWe collect a highly diverse dataset synthesized by 19 models with various\\nobjects and resolutions to evaluate our model. Experimental results demonstrate\\nthe high accuracy and good generalization ability of our method in detecting\\ngenerated images.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yan Ju'}, {'name': 'Shan Jia'}, {'name': 'Lipeng Ke'}, {'name': 'Hongfei Xue'}, {'name': 'Koki Nagano'}, {'name': 'Siwei Lyu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Siwei Lyu'}\n",
      "\n",
      "\n",
      "author\n",
      "Siwei Lyu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "6 pages, 3 figures, 2 tables\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.13964v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.13964v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.12208v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.12208v2\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-25T16:00:07Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=25, tm_hour=16, tm_min=0, tm_sec=7, tm_wday=4, tm_yday=84, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-23T05:52:23Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=5, tm_min=52, tm_sec=23, tm_wday=2, tm_yday=82, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Self-supervised Learning of Adversarial Example: Towards Good\n",
      "  Generalizations for Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Self-supervised Learning of Adversarial Example: Towards Good\\n  Generalizations for Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent studies in deepfake detection have yielded promising results when the\n",
      "training and testing face forgeries are from the same dataset. However, the\n",
      "problem remains challenging when one tries to generalize the detector to\n",
      "forgeries created by unseen methods in the training dataset. This work\n",
      "addresses the generalizable deepfake detection from a simple principle: a\n",
      "generalizable representation should be sensitive to diverse types of forgeries.\n",
      "Following this principle, we propose to enrich the \"diversity\" of forgeries by\n",
      "synthesizing augmented forgeries with a pool of forgery configurations and\n",
      "strengthen the \"sensitivity\" to the forgeries by enforcing the model to predict\n",
      "the forgery configurations. To effectively explore the large forgery\n",
      "augmentation space, we further propose to use the adversarial training strategy\n",
      "to dynamically synthesize the most challenging forgeries to the current model.\n",
      "Through extensive experiments, we show that the proposed strategies are\n",
      "surprisingly effective (see Figure 1), and they could achieve superior\n",
      "performance than the current state-of-the-art methods. Code is available at\n",
      "\\url{https://github.com/liangchen527/SLADD}.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Recent studies in deepfake detection have yielded promising results when the\\ntraining and testing face forgeries are from the same dataset. However, the\\nproblem remains challenging when one tries to generalize the detector to\\nforgeries created by unseen methods in the training dataset. This work\\naddresses the generalizable deepfake detection from a simple principle: a\\ngeneralizable representation should be sensitive to diverse types of forgeries.\\nFollowing this principle, we propose to enrich the \"diversity\" of forgeries by\\nsynthesizing augmented forgeries with a pool of forgery configurations and\\nstrengthen the \"sensitivity\" to the forgeries by enforcing the model to predict\\nthe forgery configurations. To effectively explore the large forgery\\naugmentation space, we further propose to use the adversarial training strategy\\nto dynamically synthesize the most challenging forgeries to the current model.\\nThrough extensive experiments, we show that the proposed strategies are\\nsurprisingly effective (see Figure 1), and they could achieve superior\\nperformance than the current state-of-the-art methods. Code is available at\\n\\\\url{https://github.com/liangchen527/SLADD}.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Liang Chen'}, {'name': 'Yong Zhang'}, {'name': 'Yibing Song'}, {'name': 'Lingqiao Liu'}, {'name': 'Jue Wang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jue Wang'}\n",
      "\n",
      "\n",
      "author\n",
      "Jue Wang\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To appear in CVPR 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.12208v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.12208v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.11807v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.11807v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-22T15:16:54Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=22, tm_hour=15, tm_min=16, tm_sec=54, tm_wday=1, tm_yday=81, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-22T15:16:54Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=22, tm_hour=15, tm_min=16, tm_sec=54, tm_wday=1, tm_yday=81, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "A New Approach to Improve Learning-based Deepfake Detection in Realistic\n",
      "  Conditions\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'A New Approach to Improve Learning-based Deepfake Detection in Realistic\\n  Conditions'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deep convolutional neural networks have achieved exceptional results on\n",
      "multiple detection and recognition tasks. However, the performance of such\n",
      "detectors are often evaluated in public benchmarks under constrained and\n",
      "non-realistic situations. The impact of conventional distortions and processing\n",
      "operations found in imaging workflows such as compression, noise, and\n",
      "enhancement are not sufficiently studied. Currently, only a few researches have\n",
      "been done to improve the detector robustness to unseen perturbations. This\n",
      "paper proposes a more effective data augmentation scheme based on real-world\n",
      "image degradation process. This novel technique is deployed for deepfake\n",
      "detection tasks and has been evaluated by a more realistic assessment\n",
      "framework. Extensive experiments show that the proposed data augmentation\n",
      "scheme improves generalization ability to unpredictable data distortions and\n",
      "unseen datasets.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deep convolutional neural networks have achieved exceptional results on\\nmultiple detection and recognition tasks. However, the performance of such\\ndetectors are often evaluated in public benchmarks under constrained and\\nnon-realistic situations. The impact of conventional distortions and processing\\noperations found in imaging workflows such as compression, noise, and\\nenhancement are not sufficiently studied. Currently, only a few researches have\\nbeen done to improve the detector robustness to unseen perturbations. This\\npaper proposes a more effective data augmentation scheme based on real-world\\nimage degradation process. This novel technique is deployed for deepfake\\ndetection tasks and has been evaluated by a more realistic assessment\\nframework. Extensive experiments show that the proposed data augmentation\\nscheme improves generalization ability to unpredictable data distortions and\\nunseen datasets.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yuhang Lu'}, {'name': 'Touradj Ebrahimi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Touradj Ebrahimi'}\n",
      "\n",
      "\n",
      "author\n",
      "Touradj Ebrahimi\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.11807v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.11807v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.11797v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.11797v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-22T15:03:56Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=22, tm_hour=15, tm_min=3, tm_sec=56, tm_wday=1, tm_yday=81, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-22T15:03:56Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=22, tm_hour=15, tm_min=3, tm_sec=56, tm_wday=1, tm_yday=81, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "A Novel Framework for Assessment of Learning-based Detectors in\n",
      "  Realistic Conditions with Application to Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'A Novel Framework for Assessment of Learning-based Detectors in\\n  Realistic Conditions with Application to Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deep convolutional neural networks have shown remarkable results on multiple\n",
      "detection tasks. Despite the significant progress, the performance of such\n",
      "detectors are often assessed in public benchmarks under non-realistic\n",
      "conditions. Specifically, impact of conventional distortions and processing\n",
      "operations such as compression, noise, and enhancement are not sufficiently\n",
      "studied. This paper proposes a rigorous framework to assess performance of\n",
      "learning-based detectors in more realistic situations. An illustrative example\n",
      "is shown under deepfake detection context. Inspired by the assessment results,\n",
      "a data augmentation strategy based on natural image degradation process is\n",
      "designed, which significantly improves the generalization ability of two\n",
      "deepfake detectors.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deep convolutional neural networks have shown remarkable results on multiple\\ndetection tasks. Despite the significant progress, the performance of such\\ndetectors are often assessed in public benchmarks under non-realistic\\nconditions. Specifically, impact of conventional distortions and processing\\noperations such as compression, noise, and enhancement are not sufficiently\\nstudied. This paper proposes a rigorous framework to assess performance of\\nlearning-based detectors in more realistic situations. An illustrative example\\nis shown under deepfake detection context. Inspired by the assessment results,\\na data augmentation strategy based on natural image degradation process is\\ndesigned, which significantly improves the generalization ability of two\\ndeepfake detectors.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yuhang Lu'}, {'name': 'Ruizhi Luo'}, {'name': 'Touradj Ebrahimi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Touradj Ebrahimi'}\n",
      "\n",
      "\n",
      "author\n",
      "Touradj Ebrahimi\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.11797v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.11797v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.11433v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.11433v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-22T03:13:33Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=22, tm_hour=3, tm_min=13, tm_sec=33, tm_wday=1, tm_yday=81, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-22T03:13:33Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=22, tm_hour=3, tm_min=13, tm_sec=33, tm_wday=1, tm_yday=81, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Making DeepFakes more spurious: evading deep face forgery detection via\n",
      "  trace removal attack\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Making DeepFakes more spurious: evading deep face forgery detection via\\n  trace removal attack'}\n",
      "\n",
      "\n",
      "summary\n",
      "DeepFakes are raising significant social concerns. Although various DeepFake\n",
      "detectors have been developed as forensic countermeasures, these detectors are\n",
      "still vulnerable to attacks. Recently, a few attacks, principally adversarial\n",
      "attacks, have succeeded in cloaking DeepFake images to evade detection.\n",
      "However, these attacks have typical detector-specific designs, which require\n",
      "prior knowledge about the detector, leading to poor transferability. Moreover,\n",
      "these attacks only consider simple security scenarios. Less is known about how\n",
      "effective they are in high-level scenarios where either the detectors or the\n",
      "attacker's knowledge varies. In this paper, we solve the above challenges with\n",
      "presenting a novel detector-agnostic trace removal attack for DeepFake\n",
      "anti-forensics. Instead of investigating the detector side, our attack looks\n",
      "into the original DeepFake creation pipeline, attempting to remove all\n",
      "detectable natural DeepFake traces to render the fake images more \"authentic\".\n",
      "To implement this attack, first, we perform a DeepFake trace discovery,\n",
      "identifying three discernible traces. Then a trace removal network (TR-Net) is\n",
      "proposed based on an adversarial learning framework involving one generator and\n",
      "multiple discriminators. Each discriminator is responsible for one individual\n",
      "trace representation to avoid cross-trace interference. These discriminators\n",
      "are arranged in parallel, which prompts the generator to remove various traces\n",
      "simultaneously. To evaluate the attack efficacy, we crafted heterogeneous\n",
      "security scenarios where the detectors were embedded with different levels of\n",
      "defense and the attackers' background knowledge of data varies. The\n",
      "experimental results show that the proposed attack can significantly compromise\n",
      "the detection accuracy of six state-of-the-art DeepFake detectors while causing\n",
      "only a negligible loss in visual quality to the original DeepFake samples.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'DeepFakes are raising significant social concerns. Although various DeepFake\\ndetectors have been developed as forensic countermeasures, these detectors are\\nstill vulnerable to attacks. Recently, a few attacks, principally adversarial\\nattacks, have succeeded in cloaking DeepFake images to evade detection.\\nHowever, these attacks have typical detector-specific designs, which require\\nprior knowledge about the detector, leading to poor transferability. Moreover,\\nthese attacks only consider simple security scenarios. Less is known about how\\neffective they are in high-level scenarios where either the detectors or the\\nattacker\\'s knowledge varies. In this paper, we solve the above challenges with\\npresenting a novel detector-agnostic trace removal attack for DeepFake\\nanti-forensics. Instead of investigating the detector side, our attack looks\\ninto the original DeepFake creation pipeline, attempting to remove all\\ndetectable natural DeepFake traces to render the fake images more \"authentic\".\\nTo implement this attack, first, we perform a DeepFake trace discovery,\\nidentifying three discernible traces. Then a trace removal network (TR-Net) is\\nproposed based on an adversarial learning framework involving one generator and\\nmultiple discriminators. Each discriminator is responsible for one individual\\ntrace representation to avoid cross-trace interference. These discriminators\\nare arranged in parallel, which prompts the generator to remove various traces\\nsimultaneously. To evaluate the attack efficacy, we crafted heterogeneous\\nsecurity scenarios where the detectors were embedded with different levels of\\ndefense and the attackers\\' background knowledge of data varies. The\\nexperimental results show that the proposed attack can significantly compromise\\nthe detection accuracy of six state-of-the-art DeepFake detectors while causing\\nonly a negligible loss in visual quality to the original DeepFake samples.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Chi Liu'}, {'name': 'Huajie Chen'}, {'name': 'Tianqing Zhu'}, {'name': 'Jun Zhang'}, {'name': 'Wanlei Zhou'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Wanlei Zhou'}\n",
      "\n",
      "\n",
      "author\n",
      "Wanlei Zhou\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.11433v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.11433v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.09928v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.09928v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-18T13:11:54Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=18, tm_hour=13, tm_min=11, tm_sec=54, tm_wday=4, tm_yday=77, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-18T13:11:54Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=18, tm_hour=13, tm_min=11, tm_sec=54, tm_wday=4, tm_yday=77, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on\n",
      "  Synthetic Images\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on\\n  Synthetic Images'}\n",
      "\n",
      "\n",
      "summary\n",
      "Most recent style-transfer techniques based on generative architectures are\n",
      "able to obtain synthetic multimedia contents, or commonly called deepfakes,\n",
      "with almost no artifacts. Researchers already demonstrated that synthetic\n",
      "images contain patterns that can determine not only if it is a deepfake but\n",
      "also the generative architecture employed to create the image data itself.\n",
      "These traces can be exploited to study problems that have never been addressed\n",
      "in the context of deepfakes. To this aim, in this paper a first approach to\n",
      "investigate the image ballistics on deepfake images subject to style-transfer\n",
      "manipulations is proposed. Specifically, this paper describes a study on\n",
      "detecting how many times a digital image has been processed by a generative\n",
      "architecture for style transfer. Moreover, in order to address and study\n",
      "accurately forensic ballistics on deepfake images, some mathematical properties\n",
      "of style-transfer operations were investigated.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Most recent style-transfer techniques based on generative architectures are\\nable to obtain synthetic multimedia contents, or commonly called deepfakes,\\nwith almost no artifacts. Researchers already demonstrated that synthetic\\nimages contain patterns that can determine not only if it is a deepfake but\\nalso the generative architecture employed to create the image data itself.\\nThese traces can be exploited to study problems that have never been addressed\\nin the context of deepfakes. To this aim, in this paper a first approach to\\ninvestigate the image ballistics on deepfake images subject to style-transfer\\nmanipulations is proposed. Specifically, this paper describes a study on\\ndetecting how many times a digital image has been processed by a generative\\narchitecture for style transfer. Moreover, in order to address and study\\naccurately forensic ballistics on deepfake images, some mathematical properties\\nof style-transfer operations were investigated.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Luca Guarnera'}, {'name': 'Oliver Giudice'}, {'name': 'Sebastiano Battiato'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Sebastiano Battiato'}\n",
      "\n",
      "\n",
      "arxiv_affiliation\n",
      "iCTLab s.r.l. - Spin-off of University of Catania\n",
      "\n",
      "\n",
      "author\n",
      "Sebastiano Battiato\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.09928v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.09928v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.09777v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.09777v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-18T07:43:03Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=18, tm_hour=7, tm_min=43, tm_sec=3, tm_wday=4, tm_yday=77, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-18T07:43:03Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=18, tm_hour=7, tm_min=43, tm_sec=3, tm_wday=4, tm_yday=77, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Transferable Class-Modelling for Decentralized Source Attribution of\n",
      "  GAN-Generated Images\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Transferable Class-Modelling for Decentralized Source Attribution of\\n  GAN-Generated Images'}\n",
      "\n",
      "\n",
      "summary\n",
      "GAN-generated deepfakes as a genre of digital images are gaining ground as\n",
      "both catalysts of artistic expression and malicious forms of deception,\n",
      "therefore demanding systems to enforce and accredit their ethical use. Existing\n",
      "techniques for the source attribution of synthetic images identify subtle\n",
      "intrinsic fingerprints using multiclass classification neural nets limited in\n",
      "functionality and scalability. Hence, we redefine the deepfake detection and\n",
      "source attribution problems as a series of related binary classification tasks.\n",
      "We leverage transfer learning to rapidly adapt forgery detection networks for\n",
      "multiple independent attribution problems, by proposing a semi-decentralized\n",
      "modular design to solve them simultaneously and efficiently. Class activation\n",
      "mapping is also demonstrated as an effective means of feature localization for\n",
      "model interpretation. Our models are determined via experimentation to be\n",
      "competitive with current benchmarks, and capable of decent performance on human\n",
      "portraits in ideal conditions. Decentralized fingerprint-based attribution is\n",
      "found to retain validity in the presence of novel sources, but is more\n",
      "susceptible to type II errors that intensify with image perturbations and\n",
      "attributive uncertainty. We describe both our conceptual framework and model\n",
      "prototypes for further enhancement when investigating the technical limits of\n",
      "reactive deepfake attribution.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'GAN-generated deepfakes as a genre of digital images are gaining ground as\\nboth catalysts of artistic expression and malicious forms of deception,\\ntherefore demanding systems to enforce and accredit their ethical use. Existing\\ntechniques for the source attribution of synthetic images identify subtle\\nintrinsic fingerprints using multiclass classification neural nets limited in\\nfunctionality and scalability. Hence, we redefine the deepfake detection and\\nsource attribution problems as a series of related binary classification tasks.\\nWe leverage transfer learning to rapidly adapt forgery detection networks for\\nmultiple independent attribution problems, by proposing a semi-decentralized\\nmodular design to solve them simultaneously and efficiently. Class activation\\nmapping is also demonstrated as an effective means of feature localization for\\nmodel interpretation. Our models are determined via experimentation to be\\ncompetitive with current benchmarks, and capable of decent performance on human\\nportraits in ideal conditions. Decentralized fingerprint-based attribution is\\nfound to retain validity in the presence of novel sources, but is more\\nsusceptible to type II errors that intensify with image perturbations and\\nattributive uncertainty. We describe both our conceptual framework and model\\nprototypes for further enhancement when investigating the technical limits of\\nreactive deepfake attribution.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Brandon B. G. Khoo'}, {'name': 'Chern Hong Lim'}, {'name': 'Raphael C. -W. Phan'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Raphael C. -W. Phan'}\n",
      "\n",
      "\n",
      "author\n",
      "Raphael C. -W. Phan\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "21 pages, 8 figures. Code:\n",
      "  https://github.com/quarxilon/Generator_Attribution\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.09777v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.09777v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.10; I.5.4; K.6.5', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.06825v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.06825v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-14T02:44:56Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=14, tm_hour=2, tm_min=44, tm_sec=56, tm_wday=0, tm_yday=73, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-14T02:44:56Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=14, tm_hour=2, tm_min=44, tm_sec=56, tm_wday=0, tm_yday=73, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Fairness Evaluation in Deepfake Detection Models using Metamorphic\n",
      "  Testing\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Fairness Evaluation in Deepfake Detection Models using Metamorphic\\n  Testing'}\n",
      "\n",
      "\n",
      "summary\n",
      "Fairness of deepfake detectors in the presence of anomalies are not well\n",
      "investigated, especially if those anomalies are more prominent in either male\n",
      "or female subjects. The primary motivation for this work is to evaluate how\n",
      "deepfake detection model behaves under such anomalies. However, due to the\n",
      "black-box nature of deep learning (DL) and artificial intelligence (AI)\n",
      "systems, it is hard to predict the performance of a model when the input data\n",
      "is modified. Crucially, if this defect is not addressed properly, it will\n",
      "adversely affect the fairness of the model and result in discrimination of\n",
      "certain sub-population unintentionally. Therefore, the objective of this work\n",
      "is to adopt metamorphic testing to examine the reliability of the selected\n",
      "deepfake detection model, and how the transformation of input variation places\n",
      "influence on the output. We have chosen MesoInception-4, a state-of-the-art\n",
      "deepfake detection model, as the target model and makeup as the anomalies.\n",
      "Makeups are applied through utilizing the Dlib library to obtain the 68 facial\n",
      "landmarks prior to filling in the RGB values. Metamorphic relations are derived\n",
      "based on the notion that realistic perturbations of the input images, such as\n",
      "makeup, involving eyeliners, eyeshadows, blushes, and lipsticks (which are\n",
      "common cosmetic appearance) applied to male and female images, should not alter\n",
      "the output of the model by a huge margin. Furthermore, we narrow down the scope\n",
      "to focus on revealing potential gender biases in DL and AI systems.\n",
      "Specifically, we are interested to examine whether MesoInception-4 model\n",
      "produces unfair decisions, which should be considered as a consequence of\n",
      "robustness issues. The findings from our work have the potential to pave the\n",
      "way for new research directions in the quality assurance and fairness in DL and\n",
      "AI systems.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Fairness of deepfake detectors in the presence of anomalies are not well\\ninvestigated, especially if those anomalies are more prominent in either male\\nor female subjects. The primary motivation for this work is to evaluate how\\ndeepfake detection model behaves under such anomalies. However, due to the\\nblack-box nature of deep learning (DL) and artificial intelligence (AI)\\nsystems, it is hard to predict the performance of a model when the input data\\nis modified. Crucially, if this defect is not addressed properly, it will\\nadversely affect the fairness of the model and result in discrimination of\\ncertain sub-population unintentionally. Therefore, the objective of this work\\nis to adopt metamorphic testing to examine the reliability of the selected\\ndeepfake detection model, and how the transformation of input variation places\\ninfluence on the output. We have chosen MesoInception-4, a state-of-the-art\\ndeepfake detection model, as the target model and makeup as the anomalies.\\nMakeups are applied through utilizing the Dlib library to obtain the 68 facial\\nlandmarks prior to filling in the RGB values. Metamorphic relations are derived\\nbased on the notion that realistic perturbations of the input images, such as\\nmakeup, involving eyeliners, eyeshadows, blushes, and lipsticks (which are\\ncommon cosmetic appearance) applied to male and female images, should not alter\\nthe output of the model by a huge margin. Furthermore, we narrow down the scope\\nto focus on revealing potential gender biases in DL and AI systems.\\nSpecifically, we are interested to examine whether MesoInception-4 model\\nproduces unfair decisions, which should be considered as a consequence of\\nrobustness issues. The findings from our work have the potential to pave the\\nway for new research directions in the quality assurance and fairness in DL and\\nAI systems.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Muxin Pu'}, {'name': 'Meng Yi Kuan'}, {'name': 'Nyee Thoang Lim'}, {'name': 'Chun Yong Chong'}, {'name': 'Mei Kuan Lim'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Mei Kuan Lim'}\n",
      "\n",
      "\n",
      "author\n",
      "Mei Kuan Lim\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "8 pages, accepted at 7th International Workshop on Metamorphic\n",
      "  Testing (MET22)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.06825v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.06825v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.05178v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.05178v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-10T06:16:11Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=10, tm_hour=6, tm_min=16, tm_sec=11, tm_wday=3, tm_yday=69, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-10T06:16:11Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=10, tm_hour=6, tm_min=16, tm_sec=11, tm_wday=3, tm_yday=69, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "An Audio-Visual Attention Based Multimodal Network for Fake Talking Face\n",
      "  Videos Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'An Audio-Visual Attention Based Multimodal Network for Fake Talking Face\\n  Videos Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "DeepFake based digital facial forgery is threatening the public media\n",
      "security, especially when lip manipulation has been used in talking face\n",
      "generation, the difficulty of fake video detection is further improved. By only\n",
      "changing lip shape to match the given speech, the facial features of identity\n",
      "is hard to be discriminated in such fake talking face videos. Together with the\n",
      "lack of attention on audio stream as the prior knowledge, the detection failure\n",
      "of fake talking face generation also becomes inevitable. Inspired by the\n",
      "decision-making mechanism of human multisensory perception system, which\n",
      "enables the auditory information to enhance post-sensory visual evidence for\n",
      "informed decisions output, in this study, a fake talking face detection\n",
      "framework FTFDNet is proposed by incorporating audio and visual representation\n",
      "to achieve more accurate fake talking face videos detection. Furthermore, an\n",
      "audio-visual attention mechanism (AVAM) is proposed to discover more\n",
      "informative features, which can be seamlessly integrated into any audio-visual\n",
      "CNN architectures by modularization. With the additional AVAM, the proposed\n",
      "FTFDNet is able to achieve a better detection performance on the established\n",
      "dataset (FTFDD). The evaluation of the proposed work has shown an excellent\n",
      "performance on the detection of fake talking face videos, which is able to\n",
      "arrive at a detection rate above 97%.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'DeepFake based digital facial forgery is threatening the public media\\nsecurity, especially when lip manipulation has been used in talking face\\ngeneration, the difficulty of fake video detection is further improved. By only\\nchanging lip shape to match the given speech, the facial features of identity\\nis hard to be discriminated in such fake talking face videos. Together with the\\nlack of attention on audio stream as the prior knowledge, the detection failure\\nof fake talking face generation also becomes inevitable. Inspired by the\\ndecision-making mechanism of human multisensory perception system, which\\nenables the auditory information to enhance post-sensory visual evidence for\\ninformed decisions output, in this study, a fake talking face detection\\nframework FTFDNet is proposed by incorporating audio and visual representation\\nto achieve more accurate fake talking face videos detection. Furthermore, an\\naudio-visual attention mechanism (AVAM) is proposed to discover more\\ninformative features, which can be seamlessly integrated into any audio-visual\\nCNN architectures by modularization. With the additional AVAM, the proposed\\nFTFDNet is able to achieve a better detection performance on the established\\ndataset (FTFDD). The evaluation of the proposed work has shown an excellent\\nperformance on the detection of fake talking face videos, which is able to\\narrive at a detection rate above 97%.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ganglai Wang'}, {'name': 'Peng Zhang'}, {'name': 'Lei Xie'}, {'name': 'Wei Huang'}, {'name': 'Yufei Zha'}, {'name': 'Yanning Zhang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yanning Zhang'}\n",
      "\n",
      "\n",
      "author\n",
      "Yanning Zhang\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.05178v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.05178v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.02195v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.02195v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-04T09:08:50Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=4, tm_hour=9, tm_min=8, tm_sec=50, tm_wday=4, tm_yday=63, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-04T09:08:50Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=4, tm_hour=9, tm_min=8, tm_sec=50, tm_wday=4, tm_yday=63, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Voice-Face Homogeneity Tells Deepfake\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Voice-Face Homogeneity Tells Deepfake'}\n",
      "\n",
      "\n",
      "summary\n",
      "Detecting forgery videos is highly desired due to the abuse of deepfake.\n",
      "Existing detection approaches contribute to exploring the specific artifacts in\n",
      "deepfake videos and fit well on certain data. However, the growing technique on\n",
      "these artifacts keeps challenging the robustness of traditional deepfake\n",
      "detectors. As a result, the development of generalizability of these approaches\n",
      "has reached a blockage. To address this issue, given the empirical results that\n",
      "the identities behind voices and faces are often mismatched in deepfake videos,\n",
      "and the voices and faces have homogeneity to some extent, in this paper, we\n",
      "propose to perform the deepfake detection from an unexplored voice-face\n",
      "matching view. To this end, a voice-face matching detection model is devised to\n",
      "measure the matching degree of these two on a generic audio-visual dataset.\n",
      "Thereafter, this model can be smoothly transferred to deepfake datasets without\n",
      "any fine-tuning, and the generalization across datasets is accordingly\n",
      "enhanced. We conduct extensive experiments over two widely exploited datasets -\n",
      "DFDC and FakeAVCeleb. Our model obtains significantly improved performance as\n",
      "compared to other state-of-the-art competitors and maintains favorable\n",
      "generalizability. The code has been released at\n",
      "https://github.com/xaCheng1996/VFD.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Detecting forgery videos is highly desired due to the abuse of deepfake.\\nExisting detection approaches contribute to exploring the specific artifacts in\\ndeepfake videos and fit well on certain data. However, the growing technique on\\nthese artifacts keeps challenging the robustness of traditional deepfake\\ndetectors. As a result, the development of generalizability of these approaches\\nhas reached a blockage. To address this issue, given the empirical results that\\nthe identities behind voices and faces are often mismatched in deepfake videos,\\nand the voices and faces have homogeneity to some extent, in this paper, we\\npropose to perform the deepfake detection from an unexplored voice-face\\nmatching view. To this end, a voice-face matching detection model is devised to\\nmeasure the matching degree of these two on a generic audio-visual dataset.\\nThereafter, this model can be smoothly transferred to deepfake datasets without\\nany fine-tuning, and the generalization across datasets is accordingly\\nenhanced. We conduct extensive experiments over two widely exploited datasets -\\nDFDC and FakeAVCeleb. Our model obtains significantly improved performance as\\ncompared to other state-of-the-art competitors and maintains favorable\\ngeneralizability. The code has been released at\\nhttps://github.com/xaCheng1996/VFD.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Harry Cheng'}, {'name': 'Yangyang Guo'}, {'name': 'Tianyi Wang'}, {'name': 'Qi Li'}, {'name': 'Tao Ye'}, {'name': 'Liqiang Nie'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Liqiang Nie'}\n",
      "\n",
      "\n",
      "author\n",
      "Liqiang Nie\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "10pages, accepted by CVPR2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.02195v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.02195v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.02115v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.02115v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-04T03:12:15Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=4, tm_hour=3, tm_min=12, tm_sec=15, tm_wday=4, tm_yday=63, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-04T03:12:15Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=4, tm_hour=3, tm_min=12, tm_sec=15, tm_wday=4, tm_yday=63, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Towards Benchmarking and Evaluating Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Towards Benchmarking and Evaluating Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake detection automatically recognizes the manipulated medias through\n",
      "the analysis of the difference between manipulated and non-altered videos. It\n",
      "is natural to ask which are the top performers among the existing deepfake\n",
      "detection approaches to identify promising research directions and provide\n",
      "practical guidance. Unfortunately, it's difficult to conduct a sound\n",
      "benchmarking comparison of existing detection approaches using the results in\n",
      "the literature because evaluation conditions are inconsistent across studies.\n",
      "Our objective is to establish a comprehensive and consistent benchmark, to\n",
      "develop a repeatable evaluation procedure, and to measure the performance of a\n",
      "range of detection approaches so that the results can be compared soundly. A\n",
      "challenging dataset consisting of the manipulated samples generated by more\n",
      "than 13 different methods has been collected, and 11 popular detection\n",
      "approaches (9 algorithms) from the existing literature have been implemented\n",
      "and evaluated with 6 fair-minded and practical evaluation metrics. Finally, 92\n",
      "models have been trained and 644 experiments have been performed for the\n",
      "evaluation. The results along with the shared data and evaluation methodology\n",
      "constitute a benchmark for comparing deepfake detection approaches and\n",
      "measuring progress.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Deepfake detection automatically recognizes the manipulated medias through\\nthe analysis of the difference between manipulated and non-altered videos. It\\nis natural to ask which are the top performers among the existing deepfake\\ndetection approaches to identify promising research directions and provide\\npractical guidance. Unfortunately, it's difficult to conduct a sound\\nbenchmarking comparison of existing detection approaches using the results in\\nthe literature because evaluation conditions are inconsistent across studies.\\nOur objective is to establish a comprehensive and consistent benchmark, to\\ndevelop a repeatable evaluation procedure, and to measure the performance of a\\nrange of detection approaches so that the results can be compared soundly. A\\nchallenging dataset consisting of the manipulated samples generated by more\\nthan 13 different methods has been collected, and 11 popular detection\\napproaches (9 algorithms) from the existing literature have been implemented\\nand evaluated with 6 fair-minded and practical evaluation metrics. Finally, 92\\nmodels have been trained and 644 experiments have been performed for the\\nevaluation. The results along with the shared data and evaluation methodology\\nconstitute a benchmark for comparing deepfake detection approaches and\\nmeasuring progress.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Chenhao Lin'}, {'name': 'Jingyi Deng'}, {'name': 'Pengbin Hu'}, {'name': 'Chao Shen'}, {'name': 'Qian Wang'}, {'name': 'Qi Li'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Qi Li'}\n",
      "\n",
      "\n",
      "author\n",
      "Qi Li\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.02115v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.02115v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.01573v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.01573v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-03T08:49:17Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=3, tm_hour=8, tm_min=49, tm_sec=17, tm_wday=3, tm_yday=62, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-03T08:49:17Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=3, tm_hour=8, tm_min=49, tm_sec=17, tm_wday=3, tm_yday=62, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "The Vicomtech Audio Deepfake Detection System based on Wav2Vec2 for the\n",
      "  2022 ADD Challenge\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The Vicomtech Audio Deepfake Detection System based on Wav2Vec2 for the\\n  2022 ADD Challenge'}\n",
      "\n",
      "\n",
      "summary\n",
      "This paper describes our submitted systems to the 2022 ADD challenge withing\n",
      "the tracks 1 and 2. Our approach is based on the combination of a pre-trained\n",
      "wav2vec2 feature extractor and a downstream classifier to detect spoofed audio.\n",
      "This method exploits the contextualized speech representations at the different\n",
      "transformer layers to fully capture discriminative information. Furthermore,\n",
      "the classification model is adapted to the application scenario using different\n",
      "data augmentation techniques. We evaluate our system for audio synthesis\n",
      "detection in both the ASVspoof 2021 and the 2022 ADD challenges, showing its\n",
      "robustness and good performance in realistic challenging environments such as\n",
      "telephonic and audio codec systems, noisy audio, and partial deepfakes.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'This paper describes our submitted systems to the 2022 ADD challenge withing\\nthe tracks 1 and 2. Our approach is based on the combination of a pre-trained\\nwav2vec2 feature extractor and a downstream classifier to detect spoofed audio.\\nThis method exploits the contextualized speech representations at the different\\ntransformer layers to fully capture discriminative information. Furthermore,\\nthe classification model is adapted to the application scenario using different\\ndata augmentation techniques. We evaluate our system for audio synthesis\\ndetection in both the ASVspoof 2021 and the 2022 ADD challenges, showing its\\nrobustness and good performance in realistic challenging environments such as\\ntelephonic and audio codec systems, noisy audio, and partial deepfakes.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Juan M. Martín-Doñas'}, {'name': 'Aitor Álvarez'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Aitor Álvarez'}\n",
      "\n",
      "\n",
      "author\n",
      "Aitor Álvarez\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted by ICASSP 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.01573v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.01573v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.01318v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.01318v2\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-03T18:29:50Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=3, tm_hour=18, tm_min=29, tm_sec=50, tm_wday=3, tm_yday=62, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-02T18:59:58Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=2, tm_hour=18, tm_min=59, tm_sec=58, tm_wday=2, tm_yday=61, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Protecting Celebrities with Identity Consistency Transformer\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Protecting Celebrities with Identity Consistency Transformer'}\n",
      "\n",
      "\n",
      "summary\n",
      "In this work we propose Identity Consistency Transformer, a novel face\n",
      "forgery detection method that focuses on high-level semantics, specifically\n",
      "identity information, and detecting a suspect face by finding identity\n",
      "inconsistency in inner and outer face regions. The Identity Consistency\n",
      "Transformer incorporates a consistency loss for identity consistency\n",
      "determination. We show that Identity Consistency Transformer exhibits superior\n",
      "generalization ability not only across different datasets but also across\n",
      "various types of image degradation forms found in real-world applications\n",
      "including deepfake videos. The Identity Consistency Transformer can be easily\n",
      "enhanced with additional identity information when such information is\n",
      "available, and for this reason it is especially well-suited for detecting face\n",
      "forgeries involving celebrities.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'In this work we propose Identity Consistency Transformer, a novel face\\nforgery detection method that focuses on high-level semantics, specifically\\nidentity information, and detecting a suspect face by finding identity\\ninconsistency in inner and outer face regions. The Identity Consistency\\nTransformer incorporates a consistency loss for identity consistency\\ndetermination. We show that Identity Consistency Transformer exhibits superior\\ngeneralization ability not only across different datasets but also across\\nvarious types of image degradation forms found in real-world applications\\nincluding deepfake videos. The Identity Consistency Transformer can be easily\\nenhanced with additional identity information when such information is\\navailable, and for this reason it is especially well-suited for detecting face\\nforgeries involving celebrities.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Xiaoyi Dong'}, {'name': 'Jianmin Bao'}, {'name': 'Dongdong Chen'}, {'name': 'Ting Zhang'}, {'name': 'Weiming Zhang'}, {'name': 'Nenghai Yu'}, {'name': 'Dong Chen'}, {'name': 'Fang Wen'}, {'name': 'Baining Guo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Baining Guo'}\n",
      "\n",
      "\n",
      "author\n",
      "Baining Guo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To Appear at CVPR 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.01318v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.01318v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.01265v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.01265v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-02T17:44:40Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=2, tm_hour=17, tm_min=44, tm_sec=40, tm_wday=2, tm_yday=61, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-03-02T17:44:40Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=2, tm_hour=17, tm_min=44, tm_sec=40, tm_wday=2, tm_yday=61, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Self-supervised Transformer for Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Self-supervised Transformer for Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "The fast evolution and widespread of deepfake techniques in real-world\n",
      "scenarios require stronger generalization abilities of face forgery detectors.\n",
      "Some works capture the features that are unrelated to method-specific\n",
      "artifacts, such as clues of blending boundary, accumulated up-sampling, to\n",
      "strengthen the generalization ability. However, the effectiveness of these\n",
      "methods can be easily corrupted by post-processing operations such as\n",
      "compression. Inspired by transfer learning, neural networks pre-trained on\n",
      "other large-scale face-related tasks may provide useful features for deepfake\n",
      "detection. For example, lip movement has been proved to be a kind of robust and\n",
      "good-transferring highlevel semantic feature, which can be learned from the\n",
      "lipreading task. However, the existing method pre-trains the lip feature\n",
      "extraction model in a supervised manner, which requires plenty of human\n",
      "resources in data annotation and increases the difficulty of obtaining training\n",
      "data. In this paper, we propose a self-supervised transformer based\n",
      "audio-visual contrastive learning method. The proposed method learns mouth\n",
      "motion representations by encouraging the paired video and audio\n",
      "representations to be close while unpaired ones to be diverse. After\n",
      "pre-training with our method, the model will then be partially fine-tuned for\n",
      "deepfake detection task. Extensive experiments show that our self-supervised\n",
      "method performs comparably or even better than the supervised pre-training\n",
      "counterpart.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The fast evolution and widespread of deepfake techniques in real-world\\nscenarios require stronger generalization abilities of face forgery detectors.\\nSome works capture the features that are unrelated to method-specific\\nartifacts, such as clues of blending boundary, accumulated up-sampling, to\\nstrengthen the generalization ability. However, the effectiveness of these\\nmethods can be easily corrupted by post-processing operations such as\\ncompression. Inspired by transfer learning, neural networks pre-trained on\\nother large-scale face-related tasks may provide useful features for deepfake\\ndetection. For example, lip movement has been proved to be a kind of robust and\\ngood-transferring highlevel semantic feature, which can be learned from the\\nlipreading task. However, the existing method pre-trains the lip feature\\nextraction model in a supervised manner, which requires plenty of human\\nresources in data annotation and increases the difficulty of obtaining training\\ndata. In this paper, we propose a self-supervised transformer based\\naudio-visual contrastive learning method. The proposed method learns mouth\\nmotion representations by encouraging the paired video and audio\\nrepresentations to be close while unpaired ones to be diverse. After\\npre-training with our method, the model will then be partially fine-tuned for\\ndeepfake detection task. Extensive experiments show that our self-supervised\\nmethod performs comparably or even better than the supervised pre-training\\ncounterpart.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hanqing Zhao'}, {'name': 'Wenbo Zhou'}, {'name': 'Dongdong Chen'}, {'name': 'Weiming Zhang'}, {'name': 'Nenghai Yu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Nenghai Yu'}\n",
      "\n",
      "\n",
      "author\n",
      "Nenghai Yu\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.01265v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.01265v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2203.00108v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2203.00108v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-28T21:53:07Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=28, tm_hour=21, tm_min=53, tm_sec=7, tm_wday=0, tm_yday=59, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-28T21:53:07Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=28, tm_hour=21, tm_min=53, tm_sec=7, tm_wday=0, tm_yday=59, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual\n",
      "  Image Assessment\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual\\n  Image Assessment'}\n",
      "\n",
      "\n",
      "summary\n",
      "DeepFakes are synthetic videos generated by swapping a face of an original\n",
      "image with the face of somebody else. In this paper, we describe our work to\n",
      "develop general, deep learning-based models to classify DeepFake content. We\n",
      "propose a novel framework for using Generative Adversarial Network (GAN)-based\n",
      "models, we call MRI-GAN, that utilizes perceptual differences in images to\n",
      "detect synthesized videos. We test our MRI-GAN approach and a\n",
      "plain-frames-based model using the DeepFake Detection Challenge Dataset. Our\n",
      "plain frames-based-model achieves 91% test accuracy and a model which uses our\n",
      "MRI-GAN framework with Structural Similarity Index Measurement (SSIM) for the\n",
      "perceptual differences achieves 74% test accuracy. The results of MRI-GAN are\n",
      "preliminary and may be improved further by modifying the choice of loss\n",
      "function, tuning hyper-parameters, or by using a more advanced perceptual\n",
      "similarity metric.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'DeepFakes are synthetic videos generated by swapping a face of an original\\nimage with the face of somebody else. In this paper, we describe our work to\\ndevelop general, deep learning-based models to classify DeepFake content. We\\npropose a novel framework for using Generative Adversarial Network (GAN)-based\\nmodels, we call MRI-GAN, that utilizes perceptual differences in images to\\ndetect synthesized videos. We test our MRI-GAN approach and a\\nplain-frames-based model using the DeepFake Detection Challenge Dataset. Our\\nplain frames-based-model achieves 91% test accuracy and a model which uses our\\nMRI-GAN framework with Structural Similarity Index Measurement (SSIM) for the\\nperceptual differences achieves 74% test accuracy. The results of MRI-GAN are\\npreliminary and may be improved further by modifying the choice of loss\\nfunction, tuning hyper-parameters, or by using a more advanced perceptual\\nsimilarity metric.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Pratikkumar Prajapati'}, {'name': 'Chris Pollett'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Chris Pollett'}\n",
      "\n",
      "\n",
      "author\n",
      "Chris Pollett\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "9 pages, 11 figures, 2 tables\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2203.00108v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.00108v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.13843v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.13843v2\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-14T11:24:41Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=14, tm_hour=11, tm_min=24, tm_sec=41, tm_wday=0, tm_yday=73, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-28T14:54:30Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=28, tm_hour=14, tm_min=54, tm_sec=30, tm_wday=0, tm_yday=59, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Network Architecture Attribution\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfake Network Architecture Attribution'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the rapid progress of generation technology, it has become necessary to\n",
      "attribute the origin of fake images. Existing works on fake image attribution\n",
      "perform multi-class classification on several Generative Adversarial Network\n",
      "(GAN) models and obtain high accuracies. While encouraging, these works are\n",
      "restricted to model-level attribution, only capable of handling images\n",
      "generated by seen models with a specific seed, loss and dataset, which is\n",
      "limited in real-world scenarios when fake images may be generated by privately\n",
      "trained models. This motivates us to ask whether it is possible to attribute\n",
      "fake images to the source models' architectures even if they are finetuned or\n",
      "retrained under different configurations. In this work, we present the first\n",
      "study on Deepfake Network Architecture Attribution to attribute fake images on\n",
      "architecture-level. Based on an observation that GAN architecture is likely to\n",
      "leave globally consistent fingerprints while traces left by model weights vary\n",
      "in different regions, we provide a simple yet effective solution named DNA-Det\n",
      "for this problem. Extensive experiments on multiple cross-test setups and a\n",
      "large-scale dataset demonstrate the effectiveness of DNA-Det.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"With the rapid progress of generation technology, it has become necessary to\\nattribute the origin of fake images. Existing works on fake image attribution\\nperform multi-class classification on several Generative Adversarial Network\\n(GAN) models and obtain high accuracies. While encouraging, these works are\\nrestricted to model-level attribution, only capable of handling images\\ngenerated by seen models with a specific seed, loss and dataset, which is\\nlimited in real-world scenarios when fake images may be generated by privately\\ntrained models. This motivates us to ask whether it is possible to attribute\\nfake images to the source models' architectures even if they are finetuned or\\nretrained under different configurations. In this work, we present the first\\nstudy on Deepfake Network Architecture Attribution to attribute fake images on\\narchitecture-level. Based on an observation that GAN architecture is likely to\\nleave globally consistent fingerprints while traces left by model weights vary\\nin different regions, we provide a simple yet effective solution named DNA-Det\\nfor this problem. Extensive experiments on multiple cross-test setups and a\\nlarge-scale dataset demonstrate the effectiveness of DNA-Det.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Tianyun Yang'}, {'name': 'Ziyao Huang'}, {'name': 'Juan Cao'}, {'name': 'Lei Li'}, {'name': 'Xirong Li'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Xirong Li'}\n",
      "\n",
      "\n",
      "author\n",
      "Xirong Li\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to AAAI'22\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.13843v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.13843v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.13693v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.13693v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-28T11:22:05Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=28, tm_hour=11, tm_min=22, tm_sec=5, tm_wday=0, tm_yday=59, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-28T11:22:05Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=28, tm_hour=11, tm_min=22, tm_sec=5, tm_wday=0, tm_yday=59, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Explainable deepfake and spoofing detection: an attack analysis using\n",
      "  SHapley Additive exPlanations\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Explainable deepfake and spoofing detection: an attack analysis using\\n  SHapley Additive exPlanations'}\n",
      "\n",
      "\n",
      "summary\n",
      "Despite several years of research in deepfake and spoofing detection for\n",
      "automatic speaker verification, little is known about the artefacts that\n",
      "classifiers use to distinguish between bona fide and spoofed utterances. An\n",
      "understanding of these is crucial to the design of trustworthy, explainable\n",
      "solutions. In this paper we report an extension of our previous work to better\n",
      "understand classifier behaviour to the use of SHapley Additive exPlanations\n",
      "(SHAP) to attack analysis. Our goal is to identify the artefacts that\n",
      "characterise utterances generated by different attacks algorithms. Using a pair\n",
      "of classifiers which operate either upon raw waveforms or magnitude\n",
      "spectrograms, we show that visualisations of SHAP results can be used to\n",
      "identify attack-specific artefacts and the differences and consistencies\n",
      "between synthetic speech and converted voice spoofing attacks.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Despite several years of research in deepfake and spoofing detection for\\nautomatic speaker verification, little is known about the artefacts that\\nclassifiers use to distinguish between bona fide and spoofed utterances. An\\nunderstanding of these is crucial to the design of trustworthy, explainable\\nsolutions. In this paper we report an extension of our previous work to better\\nunderstand classifier behaviour to the use of SHapley Additive exPlanations\\n(SHAP) to attack analysis. Our goal is to identify the artefacts that\\ncharacterise utterances generated by different attacks algorithms. Using a pair\\nof classifiers which operate either upon raw waveforms or magnitude\\nspectrograms, we show that visualisations of SHAP results can be used to\\nidentify attack-specific artefacts and the differences and consistencies\\nbetween synthetic speech and converted voice spoofing attacks.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Wanying Ge'}, {'name': 'Massimiliano Todisco'}, {'name': 'Nicholas Evans'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Nicholas Evans'}\n",
      "\n",
      "\n",
      "author\n",
      "Nicholas Evans\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Submitted to Speaker Odyssey Workshop 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.13693v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.13693v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.12951v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.12951v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-25T20:05:18Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=25, tm_hour=20, tm_min=5, tm_sec=18, tm_wday=4, tm_yday=56, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-25T20:05:18Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=25, tm_hour=20, tm_min=5, tm_sec=18, tm_wday=4, tm_yday=56, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Model Attribution of Face-swap Deepfake Videos\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Model Attribution of Face-swap Deepfake Videos'}\n",
      "\n",
      "\n",
      "summary\n",
      "AI-created face-swap videos, commonly known as Deepfakes, have attracted wide\n",
      "attention as powerful impersonation attacks. Existing research on Deepfakes\n",
      "mostly focuses on binary detection to distinguish between real and fake videos.\n",
      "However, it is also important to determine the specific generation model for a\n",
      "fake video, which can help attribute it to the source for forensic\n",
      "investigation. In this paper, we fill this gap by studying the model\n",
      "attribution problem of Deepfake videos. We first introduce a new dataset with\n",
      "DeepFakes from Different Models (DFDM) based on several Autoencoder models.\n",
      "Specifically, five generation models with variations in encoder, decoder,\n",
      "intermediate layer, input resolution, and compression ratio have been used to\n",
      "generate a total of 6,450 Deepfake videos based on the same input. Then we take\n",
      "Deepfakes model attribution as a multiclass classification task and propose a\n",
      "spatial and temporal attention based method to explore the differences among\n",
      "Deepfakes in the new dataset. Experimental evaluation shows that most existing\n",
      "Deepfakes detection methods failed in Deepfakes model attribution, while the\n",
      "proposed method achieved over 70% accuracy on the high-quality DFDM dataset.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'AI-created face-swap videos, commonly known as Deepfakes, have attracted wide\\nattention as powerful impersonation attacks. Existing research on Deepfakes\\nmostly focuses on binary detection to distinguish between real and fake videos.\\nHowever, it is also important to determine the specific generation model for a\\nfake video, which can help attribute it to the source for forensic\\ninvestigation. In this paper, we fill this gap by studying the model\\nattribution problem of Deepfake videos. We first introduce a new dataset with\\nDeepFakes from Different Models (DFDM) based on several Autoencoder models.\\nSpecifically, five generation models with variations in encoder, decoder,\\nintermediate layer, input resolution, and compression ratio have been used to\\ngenerate a total of 6,450 Deepfake videos based on the same input. Then we take\\nDeepfakes model attribution as a multiclass classification task and propose a\\nspatial and temporal attention based method to explore the differences among\\nDeepfakes in the new dataset. Experimental evaluation shows that most existing\\nDeepfakes detection methods failed in Deepfakes model attribution, while the\\nproposed method achieved over 70% accuracy on the high-quality DFDM dataset.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shan Jia'}, {'name': 'Xin Li'}, {'name': 'Siwei Lyu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Siwei Lyu'}\n",
      "\n",
      "\n",
      "author\n",
      "Siwei Lyu\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.12951v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.12951v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.12883v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.12883v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-25T18:47:32Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=25, tm_hour=18, tm_min=47, tm_sec=32, tm_wday=4, tm_yday=56, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-25T18:47:32Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=25, tm_hour=18, tm_min=47, tm_sec=32, tm_wday=4, tm_yday=56, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Human Detection of Political Deepfakes across Transcripts, Audio, and\n",
      "  Video\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Human Detection of Political Deepfakes across Transcripts, Audio, and\\n  Video'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent advances in technology for hyper-realistic visual effects provoke the\n",
      "concern that deepfake videos of political speeches will soon be visually\n",
      "indistinguishable from authentic video recordings. Yet there exists little\n",
      "empirical research on how audio-visual information influences people's\n",
      "susceptibility to fall for political misinformation. The conventional wisdom in\n",
      "the field of communication research predicts that people will fall for fake\n",
      "news more often when the same version of a story is presented as a video as\n",
      "opposed to text. However, audio-visual manipulations often leave distortions\n",
      "that some but not all people may pick up on. Here, we evaluate how\n",
      "communication modalities influence people's ability to discern real political\n",
      "speeches from fabrications based on a randomized experiment with 5,727\n",
      "participants who provide 61,792 truth discernment judgments. We show\n",
      "participants soundbites from political speeches that are randomly assigned to\n",
      "appear using permutations of text, audio, and video modalities. We find that\n",
      "communication modalities mediate discernment accuracy: participants are more\n",
      "accurate on video with audio than silent video, and more accurate on silent\n",
      "video than text transcripts. Likewise, we find participants rely more on how\n",
      "something is said (the audio-visual cues) rather than what is said (the speech\n",
      "content itself). However, political speeches that do not match public\n",
      "perceptions of politicians' beliefs reduce participants' reliance on visual\n",
      "cues. In particular, we find that reflective reasoning moderates the degree to\n",
      "which participants consider visual information: low performance on the\n",
      "Cognitive Reflection Test is associated with an underreliance on visual cues\n",
      "and an overreliance on what is said.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Recent advances in technology for hyper-realistic visual effects provoke the\\nconcern that deepfake videos of political speeches will soon be visually\\nindistinguishable from authentic video recordings. Yet there exists little\\nempirical research on how audio-visual information influences people's\\nsusceptibility to fall for political misinformation. The conventional wisdom in\\nthe field of communication research predicts that people will fall for fake\\nnews more often when the same version of a story is presented as a video as\\nopposed to text. However, audio-visual manipulations often leave distortions\\nthat some but not all people may pick up on. Here, we evaluate how\\ncommunication modalities influence people's ability to discern real political\\nspeeches from fabrications based on a randomized experiment with 5,727\\nparticipants who provide 61,792 truth discernment judgments. We show\\nparticipants soundbites from political speeches that are randomly assigned to\\nappear using permutations of text, audio, and video modalities. We find that\\ncommunication modalities mediate discernment accuracy: participants are more\\naccurate on video with audio than silent video, and more accurate on silent\\nvideo than text transcripts. Likewise, we find participants rely more on how\\nsomething is said (the audio-visual cues) rather than what is said (the speech\\ncontent itself). However, political speeches that do not match public\\nperceptions of politicians' beliefs reduce participants' reliance on visual\\ncues. In particular, we find that reflective reasoning moderates the degree to\\nwhich participants consider visual information: low performance on the\\nCognitive Reflection Test is associated with an underreliance on visual cues\\nand an overreliance on what is said.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Matthew Groh'}, {'name': 'Aruna Sankaranarayanan'}, {'name': 'Rosalind Picard'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Rosalind Picard'}\n",
      "\n",
      "\n",
      "author\n",
      "Rosalind Picard\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.12883v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.12883v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.12233v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.12233v2\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-28T11:50:57Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=28, tm_hour=11, tm_min=50, tm_sec=57, tm_wday=0, tm_yday=59, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-24T17:55:00Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=24, tm_hour=17, tm_min=55, tm_sec=0, tm_wday=3, tm_yday=55, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Automatic speaker verification spoofing and deepfake detection using\n",
      "  wav2vec 2.0 and data augmentation\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Automatic speaker verification spoofing and deepfake detection using\\n  wav2vec 2.0 and data augmentation'}\n",
      "\n",
      "\n",
      "summary\n",
      "The performance of spoofing countermeasure systems depends fundamentally upon\n",
      "the use of sufficiently representative training data. With this usually being\n",
      "limited, current solutions typically lack generalisation to attacks encountered\n",
      "in the wild. Strategies to improve reliability in the face of uncontrolled,\n",
      "unpredictable attacks are hence needed. We report in this paper our efforts to\n",
      "use self-supervised learning in the form of a wav2vec 2.0 front-end with fine\n",
      "tuning. Despite initial base representations being learned using only bona fide\n",
      "data and no spoofed data, we obtain the lowest equal error rates reported in\n",
      "the literature for both the ASVspoof 2021 Logical Access and Deepfake\n",
      "databases. When combined with data augmentation,these results correspond to an\n",
      "improvement of almost 90% relative to our baseline system.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The performance of spoofing countermeasure systems depends fundamentally upon\\nthe use of sufficiently representative training data. With this usually being\\nlimited, current solutions typically lack generalisation to attacks encountered\\nin the wild. Strategies to improve reliability in the face of uncontrolled,\\nunpredictable attacks are hence needed. We report in this paper our efforts to\\nuse self-supervised learning in the form of a wav2vec 2.0 front-end with fine\\ntuning. Despite initial base representations being learned using only bona fide\\ndata and no spoofed data, we obtain the lowest equal error rates reported in\\nthe literature for both the ASVspoof 2021 Logical Access and Deepfake\\ndatabases. When combined with data augmentation,these results correspond to an\\nimprovement of almost 90% relative to our baseline system.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hemlata Tak'}, {'name': 'Massimiliano Todisco'}, {'name': 'Xin Wang'}, {'name': 'Jee-weon Jung'}, {'name': 'Junichi Yamagishi'}, {'name': 'Nicholas Evans'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Nicholas Evans'}\n",
      "\n",
      "\n",
      "author\n",
      "Nicholas Evans\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Submitted to Speaker Odyssey Workshop 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.12233v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.12233v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.11359v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.11359v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-23T09:01:27Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=23, tm_hour=9, tm_min=1, tm_sec=27, tm_wday=2, tm_yday=54, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-23T09:01:27Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=23, tm_hour=9, tm_min=1, tm_sec=27, tm_wday=2, tm_yday=54, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Detection for Facial Images with Facemasks\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfake Detection for Facial Images with Facemasks'}\n",
      "\n",
      "\n",
      "summary\n",
      "Hyper-realistic face image generation and manipulation have givenrise to\n",
      "numerous unethical social issues, e.g., invasion of privacy,threat of security,\n",
      "and malicious political maneuvering, which re-sulted in the development of\n",
      "recent deepfake detection methodswith the rising demands of deepfake forensics.\n",
      "Proposed deepfakedetection methods to date have shown remarkable detection\n",
      "perfor-mance and robustness. However, none of the suggested deepfakedetection\n",
      "methods assessed the performance of deepfakes withthe facemask during the\n",
      "pandemic crisis after the outbreak of theCovid-19. In this paper, we thoroughly\n",
      "evaluate the performance ofstate-of-the-art deepfake detection models on the\n",
      "deepfakes withthe facemask. Also, we propose two approaches to enhance\n",
      "themasked deepfakes detection:face-patchandface-crop. The experi-mental\n",
      "evaluations on both methods are assessed through the base-line deepfake\n",
      "detection models on the various deepfake datasets.Our extensive experiments\n",
      "show that, among the two methods,face-cropperforms better than theface-patch,\n",
      "and could be a trainmethod for deepfake detection models to detect fake faces\n",
      "withfacemask in real world.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Hyper-realistic face image generation and manipulation have givenrise to\\nnumerous unethical social issues, e.g., invasion of privacy,threat of security,\\nand malicious political maneuvering, which re-sulted in the development of\\nrecent deepfake detection methodswith the rising demands of deepfake forensics.\\nProposed deepfakedetection methods to date have shown remarkable detection\\nperfor-mance and robustness. However, none of the suggested deepfakedetection\\nmethods assessed the performance of deepfakes withthe facemask during the\\npandemic crisis after the outbreak of theCovid-19. In this paper, we thoroughly\\nevaluate the performance ofstate-of-the-art deepfake detection models on the\\ndeepfakes withthe facemask. Also, we propose two approaches to enhance\\nthemasked deepfakes detection:face-patchandface-crop. The experi-mental\\nevaluations on both methods are assessed through the base-line deepfake\\ndetection models on the various deepfake datasets.Our extensive experiments\\nshow that, among the two methods,face-cropperforms better than theface-patch,\\nand could be a trainmethod for deepfake detection models to detect fake faces\\nwithfacemask in real world.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Donggeun Ko'}, {'name': 'Sangjun Lee'}, {'name': 'Jinyong Park'}, {'name': 'Saebyeol Shin'}, {'name': 'Donghee Hong'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "This submission has been removed by arXiv administrators because the\n",
      "  submitter did not have the authority to grant the license at the time of\n",
      "  submission\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.11359v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.11359v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.10673v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.10673v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-22T05:19:30Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=22, tm_hour=5, tm_min=19, tm_sec=30, tm_wday=1, tm_yday=53, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-22T05:19:30Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=22, tm_hour=5, tm_min=19, tm_sec=30, tm_wday=1, tm_yday=53, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Seeing is Living? Rethinking the Security of Facial Liveness\n",
      "  Verification in the Deepfake Era\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Seeing is Living? Rethinking the Security of Facial Liveness\\n  Verification in the Deepfake Era'}\n",
      "\n",
      "\n",
      "summary\n",
      "Facial Liveness Verification (FLV) is widely used for identity authentication\n",
      "in many security-sensitive domains and offered as Platform-as-a-Service (PaaS)\n",
      "by leading cloud vendors. Yet, with the rapid advances in synthetic media\n",
      "techniques (e.g., deepfake), the security of FLV is facing unprecedented\n",
      "challenges, about which little is known thus far.\n",
      "  To bridge this gap, in this paper, we conduct the first systematic study on\n",
      "the security of FLV in real-world settings. Specifically, we present\n",
      "LiveBugger, a new deepfake-powered attack framework that enables customizable,\n",
      "automated security evaluation of FLV. Leveraging LiveBugger, we perform a\n",
      "comprehensive empirical assessment of representative FLV platforms, leading to\n",
      "a set of interesting findings. For instance, most FLV APIs do not use\n",
      "anti-deepfake detection; even for those with such defenses, their effectiveness\n",
      "is concerning (e.g., it may detect high-quality synthesized videos but fail to\n",
      "detect low-quality ones). We then conduct an in-depth analysis of the factors\n",
      "impacting the attack performance of LiveBugger: a) the bias (e.g., gender or\n",
      "race) in FLV can be exploited to select victims; b) adversarial training makes\n",
      "deepfake more effective to bypass FLV; c) the input quality has a varying\n",
      "influence on different deepfake techniques to bypass FLV. Based on these\n",
      "findings, we propose a customized, two-stage approach that can boost the attack\n",
      "success rate by up to 70%. Further, we run proof-of-concept attacks on several\n",
      "representative applications of FLV (i.e., the clients of FLV APIs) to\n",
      "illustrate the practical implications: due to the vulnerability of the APIs,\n",
      "many downstream applications are vulnerable to deepfake. Finally, we discuss\n",
      "potential countermeasures to improve the security of FLV. Our findings have\n",
      "been confirmed by the corresponding vendors.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Facial Liveness Verification (FLV) is widely used for identity authentication\\nin many security-sensitive domains and offered as Platform-as-a-Service (PaaS)\\nby leading cloud vendors. Yet, with the rapid advances in synthetic media\\ntechniques (e.g., deepfake), the security of FLV is facing unprecedented\\nchallenges, about which little is known thus far.\\n  To bridge this gap, in this paper, we conduct the first systematic study on\\nthe security of FLV in real-world settings. Specifically, we present\\nLiveBugger, a new deepfake-powered attack framework that enables customizable,\\nautomated security evaluation of FLV. Leveraging LiveBugger, we perform a\\ncomprehensive empirical assessment of representative FLV platforms, leading to\\na set of interesting findings. For instance, most FLV APIs do not use\\nanti-deepfake detection; even for those with such defenses, their effectiveness\\nis concerning (e.g., it may detect high-quality synthesized videos but fail to\\ndetect low-quality ones). We then conduct an in-depth analysis of the factors\\nimpacting the attack performance of LiveBugger: a) the bias (e.g., gender or\\nrace) in FLV can be exploited to select victims; b) adversarial training makes\\ndeepfake more effective to bypass FLV; c) the input quality has a varying\\ninfluence on different deepfake techniques to bypass FLV. Based on these\\nfindings, we propose a customized, two-stage approach that can boost the attack\\nsuccess rate by up to 70%. Further, we run proof-of-concept attacks on several\\nrepresentative applications of FLV (i.e., the clients of FLV APIs) to\\nillustrate the practical implications: due to the vulnerability of the APIs,\\nmany downstream applications are vulnerable to deepfake. Finally, we discuss\\npotential countermeasures to improve the security of FLV. Our findings have\\nbeen confirmed by the corresponding vendors.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Changjiang Li'}, {'name': 'Li Wang'}, {'name': 'Shouling Ji'}, {'name': 'Xuhong Zhang'}, {'name': 'Zhaohan Xi'}, {'name': 'Shanqing Guo'}, {'name': 'Ting Wang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ting Wang'}\n",
      "\n",
      "\n",
      "author\n",
      "Ting Wang\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted as a full paper at USENIX Security '22\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.10673v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.10673v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.08433v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.08433v2\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-26T07:06:58Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=26, tm_hour=7, tm_min=6, tm_sec=58, tm_wday=5, tm_yday=57, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-17T03:29:20Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=17, tm_hour=3, tm_min=29, tm_sec=20, tm_wday=3, tm_yday=48, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "ADD 2022: the First Audio Deep Synthesis Detection Challenge\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'ADD 2022: the First Audio Deep Synthesis Detection Challenge'}\n",
      "\n",
      "\n",
      "summary\n",
      "Audio deepfake detection is an emerging topic, which was included in the\n",
      "ASVspoof 2021. However, the recent shared tasks have not covered many real-life\n",
      "and challenging scenarios. The first Audio Deep synthesis Detection challenge\n",
      "(ADD) was motivated to fill in the gap. The ADD 2022 includes three tracks:\n",
      "low-quality fake audio detection (LF), partially fake audio detection (PF) and\n",
      "audio fake game (FG). The LF track focuses on dealing with bona fide and fully\n",
      "fake utterances with various real-world noises etc. The PF track aims to\n",
      "distinguish the partially fake audio from the real. The FG track is a rivalry\n",
      "game, which includes two tasks: an audio generation task and an audio fake\n",
      "detection task. In this paper, we describe the datasets, evaluation metrics,\n",
      "and protocols. We also report major findings that reflect the recent advances\n",
      "in audio deepfake detection tasks.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Audio deepfake detection is an emerging topic, which was included in the\\nASVspoof 2021. However, the recent shared tasks have not covered many real-life\\nand challenging scenarios. The first Audio Deep synthesis Detection challenge\\n(ADD) was motivated to fill in the gap. The ADD 2022 includes three tracks:\\nlow-quality fake audio detection (LF), partially fake audio detection (PF) and\\naudio fake game (FG). The LF track focuses on dealing with bona fide and fully\\nfake utterances with various real-world noises etc. The PF track aims to\\ndistinguish the partially fake audio from the real. The FG track is a rivalry\\ngame, which includes two tasks: an audio generation task and an audio fake\\ndetection task. In this paper, we describe the datasets, evaluation metrics,\\nand protocols. We also report major findings that reflect the recent advances\\nin audio deepfake detection tasks.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Jiangyan Yi'}, {'name': 'Ruibo Fu'}, {'name': 'Jianhua Tao'}, {'name': 'Shuai Nie'}, {'name': 'Haoxin Ma'}, {'name': 'Chenglong Wang'}, {'name': 'Tao Wang'}, {'name': 'Zhengkun Tian'}, {'name': 'Ye Bai'}, {'name': 'Cunhang Fan'}, {'name': 'Shan Liang'}, {'name': 'Shiming Wang'}, {'name': 'Shuai Zhang'}, {'name': 'Xinrui Yan'}, {'name': 'Le Xu'}, {'name': 'Zhengqi Wen'}, {'name': 'Haizhou Li'}, {'name': 'Zheng Lian'}, {'name': 'Bin Liu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Bin Liu'}\n",
      "\n",
      "\n",
      "author\n",
      "Bin Liu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted by ICASSP 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.08433v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.08433v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.06228v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.06228v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-13T06:53:39Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=13, tm_hour=6, tm_min=53, tm_sec=39, tm_wday=6, tm_yday=44, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-13T06:53:39Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=13, tm_hour=6, tm_min=53, tm_sec=39, tm_wday=6, tm_yday=44, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Robust Deepfake On Unrestricted Media: Generation And Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Robust Deepfake On Unrestricted Media: Generation And Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent advances in deep learning have led to substantial improvements in\n",
      "deepfake generation, resulting in fake media with a more realistic appearance.\n",
      "Although deepfake media have potential application in a wide range of areas and\n",
      "are drawing much attention from both the academic and industrial communities,\n",
      "it also leads to serious social and criminal concerns. This chapter explores\n",
      "the evolution of and challenges in deepfake generation and detection. It also\n",
      "discusses possible ways to improve the robustness of deepfake detection for a\n",
      "wide variety of media (e.g., in-the-wild images and videos). Finally, it\n",
      "suggests a focus for future fake media research.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Recent advances in deep learning have led to substantial improvements in\\ndeepfake generation, resulting in fake media with a more realistic appearance.\\nAlthough deepfake media have potential application in a wide range of areas and\\nare drawing much attention from both the academic and industrial communities,\\nit also leads to serious social and criminal concerns. This chapter explores\\nthe evolution of and challenges in deepfake generation and detection. It also\\ndiscusses possible ways to improve the robustness of deepfake detection for a\\nwide variety of media (e.g., in-the-wild images and videos). Finally, it\\nsuggests a focus for future fake media research.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Trung-Nghia Le'}, {'name': 'Huy H Nguyen'}, {'name': 'Junichi Yamagishi'}, {'name': 'Isao Echizen'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Isao Echizen'}\n",
      "\n",
      "\n",
      "author\n",
      "Isao Echizen\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "This article will appear as one chapter for a new book called\n",
      "  Frontiers in Fake Media Generation and Detection, edited by Mahdi Khosravy,\n",
      "  Isao Echizen, and Noboru Babaguchi\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.06228v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.06228v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.06095v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.06095v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-12T16:22:46Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=12, tm_hour=16, tm_min=22, tm_sec=46, tm_wday=5, tm_yday=43, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-12T16:22:46Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=12, tm_hour=16, tm_min=22, tm_sec=46, tm_wday=5, tm_yday=43, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "A Review of Deep Learning-based Approaches for Deepfake Content\n",
      "  Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'A Review of Deep Learning-based Approaches for Deepfake Content\\n  Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "The fast-spreading information over the internet is essential to support the\n",
      "rapid supply of numerous public utility services and entertainment to users.\n",
      "Social networks and online media paved the way for modern,\n",
      "timely-communication-fashion and convenient access to all types of information.\n",
      "However, it also provides new chances for ill use of the massive amount of\n",
      "available data, such as spreading fake content to manipulate public opinion.\n",
      "Detection of counterfeit content has raised attention in the last few years for\n",
      "the advances in deepfake generation. The rapid growth of machine learning\n",
      "techniques, particularly deep learning, can predict fake content in several\n",
      "application domains, including fake image and video manipulation. This paper\n",
      "presents a comprehensive review of recent studies for deepfake content\n",
      "detection using deep learning-based approaches. We aim to broaden the\n",
      "state-of-the-art research by systematically reviewing the different categories\n",
      "of fake content detection. Furthermore, we report the advantages and drawbacks\n",
      "of the examined works and future directions towards the issues and shortcomings\n",
      "still unsolved on deepfake detection.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The fast-spreading information over the internet is essential to support the\\nrapid supply of numerous public utility services and entertainment to users.\\nSocial networks and online media paved the way for modern,\\ntimely-communication-fashion and convenient access to all types of information.\\nHowever, it also provides new chances for ill use of the massive amount of\\navailable data, such as spreading fake content to manipulate public opinion.\\nDetection of counterfeit content has raised attention in the last few years for\\nthe advances in deepfake generation. The rapid growth of machine learning\\ntechniques, particularly deep learning, can predict fake content in several\\napplication domains, including fake image and video manipulation. This paper\\npresents a comprehensive review of recent studies for deepfake content\\ndetection using deep learning-based approaches. We aim to broaden the\\nstate-of-the-art research by systematically reviewing the different categories\\nof fake content detection. Furthermore, we report the advantages and drawbacks\\nof the examined works and future directions towards the issues and shortcomings\\nstill unsolved on deepfake detection.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Leandro A. Passos'}, {'name': 'Danilo Jodas'}, {'name': 'Kelton A. P. da Costa'}, {'name': 'Luis A. Souza Júnior'}, {'name': 'Danilo Colombo'}, {'name': 'João Paulo Papa'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'João Paulo Papa'}\n",
      "\n",
      "\n",
      "author\n",
      "João Paulo Papa\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.06095v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.06095v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.05687v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.05687v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-11T15:21:11Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=11, tm_hour=15, tm_min=21, tm_sec=11, tm_wday=4, tm_yday=42, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-11T15:21:11Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=11, tm_hour=15, tm_min=21, tm_sec=11, tm_wday=4, tm_yday=42, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Towards Adversarially Robust Deepfake Detection: An Ensemble Approach\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Towards Adversarially Robust Deepfake Detection: An Ensemble Approach'}\n",
      "\n",
      "\n",
      "summary\n",
      "Detecting deepfakes is an important problem, but recent work has shown that\n",
      "DNN-based deepfake detectors are brittle against adversarial deepfakes, in\n",
      "which an adversary adds imperceptible perturbations to a deepfake to evade\n",
      "detection. In this work, we show that a modification to the detection strategy\n",
      "in which we replace a single classifier with a carefully chosen ensemble, in\n",
      "which input transformations for each model in the ensemble induces pairwise\n",
      "orthogonal gradients, can significantly improve robustness beyond the de facto\n",
      "solution of adversarial training. We present theoretical results to show that\n",
      "such orthogonal gradients can help thwart a first-order adversary by reducing\n",
      "the dimensionality of the input subspace in which adversarial deepfakes lie. We\n",
      "validate the results empirically by instantiating and evaluating a randomized\n",
      "version of such \"orthogonal\" ensembles for adversarial deepfake detection and\n",
      "find that these randomized ensembles exhibit significantly higher robustness as\n",
      "deepfake detectors compared to state-of-the-art deepfake detectors against\n",
      "adversarial deepfakes, even those created using strong PGD-500 attacks.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Detecting deepfakes is an important problem, but recent work has shown that\\nDNN-based deepfake detectors are brittle against adversarial deepfakes, in\\nwhich an adversary adds imperceptible perturbations to a deepfake to evade\\ndetection. In this work, we show that a modification to the detection strategy\\nin which we replace a single classifier with a carefully chosen ensemble, in\\nwhich input transformations for each model in the ensemble induces pairwise\\northogonal gradients, can significantly improve robustness beyond the de facto\\nsolution of adversarial training. We present theoretical results to show that\\nsuch orthogonal gradients can help thwart a first-order adversary by reducing\\nthe dimensionality of the input subspace in which adversarial deepfakes lie. We\\nvalidate the results empirically by instantiating and evaluating a randomized\\nversion of such \"orthogonal\" ensembles for adversarial deepfake detection and\\nfind that these randomized ensembles exhibit significantly higher robustness as\\ndeepfake detectors compared to state-of-the-art deepfake detectors against\\nadversarial deepfakes, even those created using strong PGD-500 attacks.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ashish Hooda'}, {'name': 'Neal Mangaokar'}, {'name': 'Ryan Feng'}, {'name': 'Kassem Fawaz'}, {'name': 'Somesh Jha'}, {'name': 'Atul Prakash'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Atul Prakash'}\n",
      "\n",
      "\n",
      "author\n",
      "Atul Prakash\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.05687v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.05687v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.04069v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.04069v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-08T01:14:30Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=8, tm_hour=1, tm_min=14, tm_sec=30, tm_wday=1, tm_yday=39, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-08T01:14:30Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=8, tm_hour=1, tm_min=14, tm_sec=30, tm_wday=1, tm_yday=39, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Detecting and Localizing Copy-Move and Image-Splicing Forgery\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Detecting and Localizing Copy-Move and Image-Splicing Forgery'}\n",
      "\n",
      "\n",
      "summary\n",
      "In the world of fake news and deepfakes, there have been an alarmingly large\n",
      "number of cases of images being tampered with and published in newspapers, used\n",
      "in court, and posted on social media for defamation purposes. Detecting these\n",
      "tampered images is an important task and one we try to tackle. In this paper,\n",
      "we focus on the methods to detect if an image has been tampered with using both\n",
      "Deep Learning and Image transformation methods and comparing the performances\n",
      "and robustness of each method. We then attempt to identify the tampered area of\n",
      "the image and predict the corresponding mask. Based on the results, suggestions\n",
      "and approaches are provided to achieve a more robust framework to detect and\n",
      "identify the forgeries.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'In the world of fake news and deepfakes, there have been an alarmingly large\\nnumber of cases of images being tampered with and published in newspapers, used\\nin court, and posted on social media for defamation purposes. Detecting these\\ntampered images is an important task and one we try to tackle. In this paper,\\nwe focus on the methods to detect if an image has been tampered with using both\\nDeep Learning and Image transformation methods and comparing the performances\\nand robustness of each method. We then attempt to identify the tampered area of\\nthe image and predict the corresponding mask. Based on the results, suggestions\\nand approaches are provided to achieve a more robust framework to detect and\\nidentify the forgeries.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Aditya Pandey'}, {'name': 'Anshuman Mitra'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Anshuman Mitra'}\n",
      "\n",
      "\n",
      "author\n",
      "Anshuman Mitra\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.04069v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.04069v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.03347v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.03347v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-07T16:45:11Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=7, tm_hour=16, tm_min=45, tm_sec=11, tm_wday=0, tm_yday=38, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-07T16:45:11Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=7, tm_hour=16, tm_min=45, tm_sec=11, tm_wday=0, tm_yday=38, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FrePGAN: Robust Deepfake Detection Using Frequency-level Perturbations\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'FrePGAN: Robust Deepfake Detection Using Frequency-level Perturbations'}\n",
      "\n",
      "\n",
      "summary\n",
      "Various deepfake detectors have been proposed, but challenges still exist to\n",
      "detect images of unknown categories or GAN models outside of the training\n",
      "settings. Such issues arise from the overfitting issue, which we discover from\n",
      "our own analysis and the previous studies to originate from the frequency-level\n",
      "artifacts in generated images. We find that ignoring the frequency-level\n",
      "artifacts can improve the detector's generalization across various GAN models,\n",
      "but it can reduce the model's performance for the trained GAN models. Thus, we\n",
      "design a framework to generalize the deepfake detector for both the known and\n",
      "unseen GAN models. Our framework generates the frequency-level perturbation\n",
      "maps to make the generated images indistinguishable from the real images. By\n",
      "updating the deepfake detector along with the training of the perturbation\n",
      "generator, our model is trained to detect the frequency-level artifacts at the\n",
      "initial iterations and consider the image-level irregularities at the last\n",
      "iterations. For experiments, we design new test scenarios varying from the\n",
      "training settings in GAN models, color manipulations, and object categories.\n",
      "Numerous experiments validate the state-of-the-art performance of our deepfake\n",
      "detector.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Various deepfake detectors have been proposed, but challenges still exist to\\ndetect images of unknown categories or GAN models outside of the training\\nsettings. Such issues arise from the overfitting issue, which we discover from\\nour own analysis and the previous studies to originate from the frequency-level\\nartifacts in generated images. We find that ignoring the frequency-level\\nartifacts can improve the detector's generalization across various GAN models,\\nbut it can reduce the model's performance for the trained GAN models. Thus, we\\ndesign a framework to generalize the deepfake detector for both the known and\\nunseen GAN models. Our framework generates the frequency-level perturbation\\nmaps to make the generated images indistinguishable from the real images. By\\nupdating the deepfake detector along with the training of the perturbation\\ngenerator, our model is trained to detect the frequency-level artifacts at the\\ninitial iterations and consider the image-level irregularities at the last\\niterations. For experiments, we design new test scenarios varying from the\\ntraining settings in GAN models, color manipulations, and object categories.\\nNumerous experiments validate the state-of-the-art performance of our deepfake\\ndetector.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yonghyun Jeong'}, {'name': 'Doyeon Kim'}, {'name': 'Youngmin Ro'}, {'name': 'Jongwon Choi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jongwon Choi'}\n",
      "\n",
      "\n",
      "author\n",
      "Jongwon Choi\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.03347v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.03347v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.02819v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.02819v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-06T17:16:46Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=6, tm_hour=17, tm_min=16, tm_sec=46, tm_wday=6, tm_yday=37, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-06T17:16:46Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=6, tm_hour=17, tm_min=16, tm_sec=46, tm_wday=6, tm_yday=37, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Block shuffling learning for Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Block shuffling learning for Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Although the deepfake detection based on convolutional neural network has\n",
      "achieved good results, the detection results show that these detectors show\n",
      "obvious performance degradation when the input images undergo some common\n",
      "transformations (like resizing, blurring), which indicates that the\n",
      "generalization ability of the detector is insufficient. In this paper, we\n",
      "propose a novel block shuffling learning method to solve this problem.\n",
      "Specifically, we divide the images into blocks and then introduce the random\n",
      "shuffling to intra-block and inter-block. Intra-block shuffling increases the\n",
      "robustness of the detector and we also propose an adversarial loss algorithm to\n",
      "overcome the over-fitting problem brought by the noise introduced by shuffling.\n",
      "Moreover, we encourage the detector to focus on finding differences among the\n",
      "local features through inter-block shuffling, and reconstruct the spatial\n",
      "layout of the blocks to model the semantic associations between them.\n",
      "Especially, our method can be easily integrated with various CNN models.\n",
      "Extensive experiments show that our proposed method achieves state-of-the-art\n",
      "performance in forgery face detection, including good generalization ability in\n",
      "the face of common image transformations.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Although the deepfake detection based on convolutional neural network has\\nachieved good results, the detection results show that these detectors show\\nobvious performance degradation when the input images undergo some common\\ntransformations (like resizing, blurring), which indicates that the\\ngeneralization ability of the detector is insufficient. In this paper, we\\npropose a novel block shuffling learning method to solve this problem.\\nSpecifically, we divide the images into blocks and then introduce the random\\nshuffling to intra-block and inter-block. Intra-block shuffling increases the\\nrobustness of the detector and we also propose an adversarial loss algorithm to\\novercome the over-fitting problem brought by the noise introduced by shuffling.\\nMoreover, we encourage the detector to focus on finding differences among the\\nlocal features through inter-block shuffling, and reconstruct the spatial\\nlayout of the blocks to model the semantic associations between them.\\nEspecially, our method can be easily integrated with various CNN models.\\nExtensive experiments show that our proposed method achieves state-of-the-art\\nperformance in forgery face detection, including good generalization ability in\\nthe face of common image transformations.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sitong Liu'}, {'name': 'Zhichao Lian'}, {'name': 'Siqi Gu'}, {'name': 'Liang Xiao'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Liang Xiao'}\n",
      "\n",
      "\n",
      "author\n",
      "Liang Xiao\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.02819v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.02819v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2202.00374v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2202.00374v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-01T12:30:27Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=1, tm_hour=12, tm_min=30, tm_sec=27, tm_wday=1, tm_yday=32, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-02-01T12:30:27Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=1, tm_hour=12, tm_min=30, tm_sec=27, tm_wday=1, tm_yday=32, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake pornography as a male gaze on fan culture\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfake pornography as a male gaze on fan culture'}\n",
      "\n",
      "\n",
      "summary\n",
      "This essay shows the impact of deepfake technology on fan culture. The\n",
      "innovative technology provided the male audience with an instrument to express\n",
      "its ideas and plots. Which subsequently led to the rise of deepfake\n",
      "pornography. It is often seen as a part of celebrity studies; however, the\n",
      "essay shows that it could also be considered a type of fanfic and a product of\n",
      "participatory culture, sharing community origin, exploitation by commercial\n",
      "companies and deep sexualisation. These two branches of fanfic evolution can be\n",
      "connected via the genre of machinima pornography. Textual fanfics are mainly\n",
      "created by females for females, depicting males; otherwise, deepfake\n",
      "pornography and machinima are made by males and for males targeting females.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'This essay shows the impact of deepfake technology on fan culture. The\\ninnovative technology provided the male audience with an instrument to express\\nits ideas and plots. Which subsequently led to the rise of deepfake\\npornography. It is often seen as a part of celebrity studies; however, the\\nessay shows that it could also be considered a type of fanfic and a product of\\nparticipatory culture, sharing community origin, exploitation by commercial\\ncompanies and deep sexualisation. These two branches of fanfic evolution can be\\nconnected via the genre of machinima pornography. Textual fanfics are mainly\\ncreated by females for females, depicting males; otherwise, deepfake\\npornography and machinima are made by males and for males targeting females.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Inna Suvorova'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Inna Suvorova'}\n",
      "\n",
      "\n",
      "author\n",
      "Inna Suvorova\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2202.00374v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.00374v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2201.12051v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2201.12051v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-01-28T11:29:07Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=1, tm_mday=28, tm_hour=11, tm_min=29, tm_sec=7, tm_wday=4, tm_yday=28, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-01-28T11:29:07Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=1, tm_mday=28, tm_hour=11, tm_min=29, tm_sec=7, tm_wday=4, tm_yday=28, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Detection of fake faces in videos\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Detection of fake faces in videos'}\n",
      "\n",
      "\n",
      "summary\n",
      ": Deep learning methodologies have been used to create applications that can\n",
      "cause threats to privacy, democracy and national security and could be used to\n",
      "further amplify malicious activities. One of those deep learning-powered\n",
      "applications in recent times is synthesized videos of famous personalities.\n",
      "According to Forbes, Generative Adversarial Networks(GANs) generated fake\n",
      "videos growing exponentially every year and the organization known as Deeptrace\n",
      "had estimated an increase of deepfakes by 84% from the year 2018 to 2019. They\n",
      "are used to generate and modify human faces, where most of the existing fake\n",
      "videos are of prurient non-consensual nature, of which its estimates to be\n",
      "around 96% and some carried out impersonating personalities for cyber crime. In\n",
      "this paper, available video datasets are identified and a pretrained model\n",
      "BlazeFace is used to detect faces, and a ResNet and Xception ensembled\n",
      "architectured neural network trained on the dataset to achieve the goal of\n",
      "detection of fake faces in videos. The model is optimized over a loss value and\n",
      "log loss values and evaluated over its F1 score. Over a sample of data, it is\n",
      "observed that focal loss provides better accuracy, F1 score and loss as the\n",
      "gamma of the focal loss becomes a hyper parameter. This provides a k-folded\n",
      "accuracy of around 91% at its peak in a training cycle with the real world\n",
      "accuracy subjected to change over time as the model decays.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': ': Deep learning methodologies have been used to create applications that can\\ncause threats to privacy, democracy and national security and could be used to\\nfurther amplify malicious activities. One of those deep learning-powered\\napplications in recent times is synthesized videos of famous personalities.\\nAccording to Forbes, Generative Adversarial Networks(GANs) generated fake\\nvideos growing exponentially every year and the organization known as Deeptrace\\nhad estimated an increase of deepfakes by 84% from the year 2018 to 2019. They\\nare used to generate and modify human faces, where most of the existing fake\\nvideos are of prurient non-consensual nature, of which its estimates to be\\naround 96% and some carried out impersonating personalities for cyber crime. In\\nthis paper, available video datasets are identified and a pretrained model\\nBlazeFace is used to detect faces, and a ResNet and Xception ensembled\\narchitectured neural network trained on the dataset to achieve the goal of\\ndetection of fake faces in videos. The model is optimized over a loss value and\\nlog loss values and evaluated over its F1 score. Over a sample of data, it is\\nobserved that focal loss provides better accuracy, F1 score and loss as the\\ngamma of the focal loss becomes a hyper parameter. This provides a k-folded\\naccuracy of around 91% at its peak in a training cycle with the real world\\naccuracy subjected to change over time as the model decays.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'M. Shamanth'}, {'name': 'Russel Mathias'}, {'name': 'Dr Vijayalakshmi MN'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Dr Vijayalakshmi MN'}\n",
      "\n",
      "\n",
      "author\n",
      "Dr Vijayalakshmi MN\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "5 pages, 11 figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2201.12051v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2201.12051v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2201.05889v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2201.05889v1\n",
      "\n",
      "\n",
      "updated\n",
      "2022-01-15T17:04:38Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=1, tm_mday=15, tm_hour=17, tm_min=4, tm_sec=38, tm_wday=5, tm_yday=15, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2022-01-15T17:04:38Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=1, tm_mday=15, tm_hour=17, tm_min=4, tm_sec=38, tm_wday=5, tm_yday=15, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "StolenEncoder: Stealing Pre-trained Encoders\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'StolenEncoder: Stealing Pre-trained Encoders'}\n",
      "\n",
      "\n",
      "summary\n",
      "Pre-trained encoders are general-purpose feature extractors that can be used\n",
      "for many downstream tasks. Recent progress in self-supervised learning can\n",
      "pre-train highly effective encoders using a large volume of unlabeled data,\n",
      "leading to the emerging encoder as a service (EaaS). A pre-trained encoder may\n",
      "be deemed confidential because its training often requires lots of data and\n",
      "computation resources as well as its public release may facilitate misuse of\n",
      "AI, e.g., for deepfakes generation. In this paper, we propose the first attack\n",
      "called StolenEncoder to steal pre-trained image encoders. We evaluate\n",
      "StolenEncoder on multiple target encoders pre-trained by ourselves and three\n",
      "real-world target encoders including the ImageNet encoder pre-trained by\n",
      "Google, CLIP encoder pre-trained by OpenAI, and Clarifai's General Embedding\n",
      "encoder deployed as a paid EaaS. Our results show that the encoders stolen by\n",
      "StolenEncoder have similar functionality with the target encoders. In\n",
      "particular, the downstream classifiers built upon a target encoder and a stolen\n",
      "encoder have similar accuracy. Moreover, stealing a target encoder using\n",
      "StolenEncoder requires much less data and computation resources than\n",
      "pre-training it from scratch. We also explore three defenses that perturb\n",
      "feature vectors produced by a target encoder. Our evaluation shows that these\n",
      "defenses are not enough to mitigate StolenEncoder.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Pre-trained encoders are general-purpose feature extractors that can be used\\nfor many downstream tasks. Recent progress in self-supervised learning can\\npre-train highly effective encoders using a large volume of unlabeled data,\\nleading to the emerging encoder as a service (EaaS). A pre-trained encoder may\\nbe deemed confidential because its training often requires lots of data and\\ncomputation resources as well as its public release may facilitate misuse of\\nAI, e.g., for deepfakes generation. In this paper, we propose the first attack\\ncalled StolenEncoder to steal pre-trained image encoders. We evaluate\\nStolenEncoder on multiple target encoders pre-trained by ourselves and three\\nreal-world target encoders including the ImageNet encoder pre-trained by\\nGoogle, CLIP encoder pre-trained by OpenAI, and Clarifai's General Embedding\\nencoder deployed as a paid EaaS. Our results show that the encoders stolen by\\nStolenEncoder have similar functionality with the target encoders. In\\nparticular, the downstream classifiers built upon a target encoder and a stolen\\nencoder have similar accuracy. Moreover, stealing a target encoder using\\nStolenEncoder requires much less data and computation resources than\\npre-training it from scratch. We also explore three defenses that perturb\\nfeature vectors produced by a target encoder. Our evaluation shows that these\\ndefenses are not enough to mitigate StolenEncoder.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yupei Liu'}, {'name': 'Jinyuan Jia'}, {'name': 'Hongbin Liu'}, {'name': 'Neil Zhenqiang Gong'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Neil Zhenqiang Gong'}\n",
      "\n",
      "\n",
      "author\n",
      "Neil Zhenqiang Gong\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2201.05889v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2201.05889v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2112.12001v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2112.12001v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-12-22T16:25:24Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=22, tm_hour=16, tm_min=25, tm_sec=24, tm_wday=2, tm_yday=356, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-12-22T16:25:24Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=22, tm_hour=16, tm_min=25, tm_sec=24, tm_wday=2, tm_yday=356, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DA-FDFtNet: Dual Attention Fake Detection Fine-tuning Network to Detect\n",
      "  Various AI-Generated Fake Images\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'DA-FDFtNet: Dual Attention Fake Detection Fine-tuning Network to Detect\\n  Various AI-Generated Fake Images'}\n",
      "\n",
      "\n",
      "summary\n",
      "Due to the advancement of Generative Adversarial Networks (GAN),\n",
      "Autoencoders, and other AI technologies, it has been much easier to create fake\n",
      "images such as \"Deepfakes\". More recent research has introduced few-shot\n",
      "learning, which uses a small amount of training data to produce fake images and\n",
      "videos more effectively. Therefore, the ease of generating manipulated images\n",
      "and the difficulty of distinguishing those images can cause a serious threat to\n",
      "our society, such as propagating fake information. However, detecting realistic\n",
      "fake images generated by the latest AI technology is challenging due to the\n",
      "reasons mentioned above. In this work, we propose Dual Attention Fake Detection\n",
      "Fine-tuning Network (DA-FDFtNet) to detect the manipulated fake face images\n",
      "from the real face data. Our DA-FDFtNet integrates the pre-trained model with\n",
      "Fine-Tune Transformer, MBblockV3, and a channel attention module to improve the\n",
      "performance and robustness across different types of fake images. In\n",
      "particular, Fine-Tune Transformer consists of multiple numbers of an\n",
      "image-based self-attention module and a down-sampling layer. The channel\n",
      "attention module is also connected with the pre-trained model to capture the\n",
      "fake images feature space. We experiment with our DA-FDFtNet with the\n",
      "FaceForensics++ dataset and various GAN-generated datasets, and we show that\n",
      "our approach outperforms the previous baseline models.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Due to the advancement of Generative Adversarial Networks (GAN),\\nAutoencoders, and other AI technologies, it has been much easier to create fake\\nimages such as \"Deepfakes\". More recent research has introduced few-shot\\nlearning, which uses a small amount of training data to produce fake images and\\nvideos more effectively. Therefore, the ease of generating manipulated images\\nand the difficulty of distinguishing those images can cause a serious threat to\\nour society, such as propagating fake information. However, detecting realistic\\nfake images generated by the latest AI technology is challenging due to the\\nreasons mentioned above. In this work, we propose Dual Attention Fake Detection\\nFine-tuning Network (DA-FDFtNet) to detect the manipulated fake face images\\nfrom the real face data. Our DA-FDFtNet integrates the pre-trained model with\\nFine-Tune Transformer, MBblockV3, and a channel attention module to improve the\\nperformance and robustness across different types of fake images. In\\nparticular, Fine-Tune Transformer consists of multiple numbers of an\\nimage-based self-attention module and a down-sampling layer. The channel\\nattention module is also connected with the pre-trained model to capture the\\nfake images feature space. We experiment with our DA-FDFtNet with the\\nFaceForensics++ dataset and various GAN-generated datasets, and we show that\\nour approach outperforms the previous baseline models.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Young Oh Bang'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2112.12001v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2112.12001v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2112.10936v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2112.10936v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-12-21T01:57:04Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=21, tm_hour=1, tm_min=57, tm_sec=4, tm_wday=1, tm_yday=355, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-12-21T01:57:04Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=21, tm_hour=1, tm_min=57, tm_sec=4, tm_wday=1, tm_yday=355, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Watch Those Words: Video Falsification Detection Using Word-Conditioned\n",
      "  Facial Motion\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Watch Those Words: Video Falsification Detection Using Word-Conditioned\\n  Facial Motion'}\n",
      "\n",
      "\n",
      "summary\n",
      "In today's era of digital misinformation, we are increasingly faced with new\n",
      "threats posed by video falsification techniques. Such falsifications range from\n",
      "cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,\n",
      "sophisticated AI media synthesis methods), which are becoming perceptually\n",
      "indistinguishable from real videos. To tackle this challenge, we propose a\n",
      "multi-modal semantic forensic approach to discover clues that go beyond\n",
      "detecting discrepancies in visual quality, thereby handling both simpler\n",
      "cheapfakes and visually persuasive deepfakes. In this work, our goal is to\n",
      "verify that the purported person seen in the video is indeed themselves by\n",
      "detecting anomalous correspondences between their facial movements and the\n",
      "words they are saying. We leverage the idea of attribution to learn\n",
      "person-specific biometric patterns that distinguish a given speaker from\n",
      "others. We use interpretable Action Units (AUs) to capture a persons' face and\n",
      "head movement as opposed to deep CNN visual features, and we are the first to\n",
      "use word-conditioned facial motion analysis. Unlike existing person-specific\n",
      "approaches, our method is also effective against attacks that focus on lip\n",
      "manipulation. We further demonstrate our method's effectiveness on a range of\n",
      "fakes not seen in training including those without video manipulation, that\n",
      "were not addressed in prior work.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"In today's era of digital misinformation, we are increasingly faced with new\\nthreats posed by video falsification techniques. Such falsifications range from\\ncheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,\\nsophisticated AI media synthesis methods), which are becoming perceptually\\nindistinguishable from real videos. To tackle this challenge, we propose a\\nmulti-modal semantic forensic approach to discover clues that go beyond\\ndetecting discrepancies in visual quality, thereby handling both simpler\\ncheapfakes and visually persuasive deepfakes. In this work, our goal is to\\nverify that the purported person seen in the video is indeed themselves by\\ndetecting anomalous correspondences between their facial movements and the\\nwords they are saying. We leverage the idea of attribution to learn\\nperson-specific biometric patterns that distinguish a given speaker from\\nothers. We use interpretable Action Units (AUs) to capture a persons' face and\\nhead movement as opposed to deep CNN visual features, and we are the first to\\nuse word-conditioned facial motion analysis. Unlike existing person-specific\\napproaches, our method is also effective against attacks that focus on lip\\nmanipulation. We further demonstrate our method's effectiveness on a range of\\nfakes not seen in training including those without video manipulation, that\\nwere not addressed in prior work.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shruti Agarwal'}, {'name': 'Liwen Hu'}, {'name': 'Evonne Ng'}, {'name': 'Trevor Darrell'}, {'name': 'Hao Li'}, {'name': 'Anna Rohrbach'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Anna Rohrbach'}\n",
      "\n",
      "\n",
      "author\n",
      "Anna Rohrbach\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2112.10936v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2112.10936v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2112.03553v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2112.03553v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-12-07T07:58:28Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=7, tm_hour=7, tm_min=58, tm_sec=28, tm_wday=1, tm_yday=341, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-12-07T07:58:28Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=7, tm_hour=7, tm_min=58, tm_sec=28, tm_wday=1, tm_yday=341, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "ADD: Frequency Attention and Multi-View based Knowledge Distillation to\n",
      "  Detect Low-Quality Compressed Deepfake Images\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'ADD: Frequency Attention and Multi-View based Knowledge Distillation to\\n  Detect Low-Quality Compressed Deepfake Images'}\n",
      "\n",
      "\n",
      "summary\n",
      "Despite significant advancements of deep learning-based forgery detectors for\n",
      "distinguishing manipulated deepfake images, most detection approaches suffer\n",
      "from moderate to significant performance degradation with low-quality\n",
      "compressed deepfake images. Because of the limited information in low-quality\n",
      "images, detecting low-quality deepfake remains an important challenge. In this\n",
      "work, we apply frequency domain learning and optimal transport theory in\n",
      "knowledge distillation (KD) to specifically improve the detection of\n",
      "low-quality compressed deepfake images. We explore transfer learning capability\n",
      "in KD to enable a student network to learn discriminative features from\n",
      "low-quality images effectively. In particular, we propose the Attention-based\n",
      "Deepfake detection Distiller (ADD), which consists of two novel distillations:\n",
      "1) frequency attention distillation that effectively retrieves the removed\n",
      "high-frequency components in the student network, and 2) multi-view attention\n",
      "distillation that creates multiple attention vectors by slicing the teacher's\n",
      "and student's tensors under different views to transfer the teacher tensor's\n",
      "distribution to the student more efficiently. Our extensive experimental\n",
      "results demonstrate that our approach outperforms state-of-the-art baselines in\n",
      "detecting low-quality compressed deepfake images.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Despite significant advancements of deep learning-based forgery detectors for\\ndistinguishing manipulated deepfake images, most detection approaches suffer\\nfrom moderate to significant performance degradation with low-quality\\ncompressed deepfake images. Because of the limited information in low-quality\\nimages, detecting low-quality deepfake remains an important challenge. In this\\nwork, we apply frequency domain learning and optimal transport theory in\\nknowledge distillation (KD) to specifically improve the detection of\\nlow-quality compressed deepfake images. We explore transfer learning capability\\nin KD to enable a student network to learn discriminative features from\\nlow-quality images effectively. In particular, we propose the Attention-based\\nDeepfake detection Distiller (ADD), which consists of two novel distillations:\\n1) frequency attention distillation that effectively retrieves the removed\\nhigh-frequency components in the student network, and 2) multi-view attention\\ndistillation that creates multiple attention vectors by slicing the teacher's\\nand student's tensors under different views to transfer the teacher tensor's\\ndistribution to the student more efficiently. Our extensive experimental\\nresults demonstrate that our approach outperforms state-of-the-art baselines in\\ndetecting low-quality compressed deepfake images.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Binh M. Le'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "Thirty-Sixth AAAI Conference on Artificial Intelligence, 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2112.03553v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2112.03553v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2112.03528v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2112.03528v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-12-07T06:54:48Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=7, tm_hour=6, tm_min=54, tm_sec=48, tm_wday=1, tm_yday=341, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-12-07T06:54:48Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=7, tm_hour=6, tm_min=54, tm_sec=48, tm_wday=1, tm_yday=341, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Physics guided deep learning generative models for crystal materials\n",
      "  discovery\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Physics guided deep learning generative models for crystal materials\\n  discovery'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deep learning based generative models such as deepfake have been able to\n",
      "generate amazing images and videos. However, these models may need significant\n",
      "transformation when applied to generate crystal materials structures in which\n",
      "the building blocks, the physical atoms are very different from the pixels.\n",
      "Naively transferred generative models tend to generate a large portion of\n",
      "physically infeasible crystal structures that are not stable or synthesizable.\n",
      "Herein we show that by exploiting and adding physically oriented data\n",
      "augmentation, loss function terms, and post processing, our deep adversarial\n",
      "network (GAN) based generative models can now generate crystal structures with\n",
      "higher physical feasibility and expand our previous models which can only\n",
      "create cubic structures.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deep learning based generative models such as deepfake have been able to\\ngenerate amazing images and videos. However, these models may need significant\\ntransformation when applied to generate crystal materials structures in which\\nthe building blocks, the physical atoms are very different from the pixels.\\nNaively transferred generative models tend to generate a large portion of\\nphysically infeasible crystal structures that are not stable or synthesizable.\\nHerein we show that by exploiting and adding physically oriented data\\naugmentation, loss function terms, and post processing, our deep adversarial\\nnetwork (GAN) based generative models can now generate crystal structures with\\nhigher physical feasibility and expand our previous models which can only\\ncreate cubic structures.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yong Zhao'}, {'name': 'Edirisuriya MD Siriwardane'}, {'name': 'Jianjun Hu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jianjun Hu'}\n",
      "\n",
      "\n",
      "author\n",
      "Jianjun Hu\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "AAAI Fall Symposium Series (FSS) 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2112.03528v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2112.03528v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cond-mat.mtrl-sci', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cond-mat.mtrl-sci', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2112.03351v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2112.03351v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-12-06T20:53:41Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=6, tm_hour=20, tm_min=53, tm_sec=41, tm_wday=0, tm_yday=340, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-12-06T20:53:41Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=6, tm_hour=20, tm_min=53, tm_sec=41, tm_wday=0, tm_yday=340, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Audio Deepfake Perceptions in College Going Populations\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Audio Deepfake Perceptions in College Going Populations'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake is content or material that is generated or manipulated using AI\n",
      "methods, to pass off as real. There are four different deepfake types: audio,\n",
      "video, image and text. In this research we focus on audio deepfakes and how\n",
      "people perceive it. There are several audio deepfake generation frameworks, but\n",
      "we chose MelGAN which is a non-autoregressive and fast audio deepfake\n",
      "generating framework, requiring fewer parameters. This study tries to assess\n",
      "audio deepfake perceptions among college students from different majors. This\n",
      "study also answers the question of how their background and major can affect\n",
      "their perception towards AI generated deepfakes. We also analyzed the results\n",
      "based on different aspects of: grade level, complexity of the grammar used in\n",
      "the audio clips, length of the audio clips, those who knew the term deepfakes\n",
      "and those who did not, as well as the political angle. It is interesting that\n",
      "the results show when an audio clip has a political connotation, it can affect\n",
      "what people think about whether it is real or fake, even if the content is\n",
      "fairly similar. This study also explores the question of how background and\n",
      "major can affect perception towards deepfakes.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfake is content or material that is generated or manipulated using AI\\nmethods, to pass off as real. There are four different deepfake types: audio,\\nvideo, image and text. In this research we focus on audio deepfakes and how\\npeople perceive it. There are several audio deepfake generation frameworks, but\\nwe chose MelGAN which is a non-autoregressive and fast audio deepfake\\ngenerating framework, requiring fewer parameters. This study tries to assess\\naudio deepfake perceptions among college students from different majors. This\\nstudy also answers the question of how their background and major can affect\\ntheir perception towards AI generated deepfakes. We also analyzed the results\\nbased on different aspects of: grade level, complexity of the grammar used in\\nthe audio clips, length of the audio clips, those who knew the term deepfakes\\nand those who did not, as well as the political angle. It is interesting that\\nthe results show when an audio clip has a political connotation, it can affect\\nwhat people think about whether it is real or fake, even if the content is\\nfairly similar. This study also explores the question of how background and\\nmajor can affect perception towards deepfakes.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Gabrielle Watson'}, {'name': 'Zahra Khanjani'}, {'name': 'Vandana P. Janeja'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Vandana P. Janeja'}\n",
      "\n",
      "\n",
      "author\n",
      "Vandana P. Janeja\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Summary of study findings\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2112.03351v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2112.03351v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2111.14203v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2111.14203v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-11-28T18:28:30Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=28, tm_hour=18, tm_min=28, tm_sec=30, tm_wday=6, tm_yday=332, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-11-28T18:28:30Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=28, tm_hour=18, tm_min=28, tm_sec=30, tm_wday=6, tm_yday=332, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake is content or material that is synthetically generated or\n",
      "manipulated using artificial intelligence (AI) methods, to be passed off as\n",
      "real and can include audio, video, image, and text synthesis. This survey has\n",
      "been conducted with a different perspective compared to existing survey papers,\n",
      "that mostly focus on just video and image deepfakes. This survey not only\n",
      "evaluates generation and detection methods in the different deepfake\n",
      "categories, but mainly focuses on audio deepfakes that are overlooked in most\n",
      "of the existing surveys. This paper critically analyzes and provides a unique\n",
      "source of audio deepfake research, mostly ranging from 2016 to 2020. To the\n",
      "best of our knowledge, this is the first survey focusing on audio deepfakes in\n",
      "English. This survey provides readers with a summary of 1) different deepfake\n",
      "categories 2) how they could be created and detected 3) the most recent trends\n",
      "in this domain and shortcomings in detection methods 4) audio deepfakes, how\n",
      "they are created and detected in more detail which is the main focus of this\n",
      "paper. We found that Generative Adversarial Networks(GAN), Convolutional Neural\n",
      "Networks (CNN), and Deep Neural Networks (DNN) are common ways of creating and\n",
      "detecting deepfakes. In our evaluation of over 140 methods we found that the\n",
      "majority of the focus is on video deepfakes and in particular in the generation\n",
      "of video deepfakes. We found that for text deepfakes there are more generation\n",
      "methods but very few robust methods for detection, including fake news\n",
      "detection, which has become a controversial area of research because of the\n",
      "potential of heavy overlaps with human generation of fake content. This paper\n",
      "is an abbreviated version of the full survey and reveals a clear need to\n",
      "research audio deepfakes and particularly detection of audio deepfakes.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfake is content or material that is synthetically generated or\\nmanipulated using artificial intelligence (AI) methods, to be passed off as\\nreal and can include audio, video, image, and text synthesis. This survey has\\nbeen conducted with a different perspective compared to existing survey papers,\\nthat mostly focus on just video and image deepfakes. This survey not only\\nevaluates generation and detection methods in the different deepfake\\ncategories, but mainly focuses on audio deepfakes that are overlooked in most\\nof the existing surveys. This paper critically analyzes and provides a unique\\nsource of audio deepfake research, mostly ranging from 2016 to 2020. To the\\nbest of our knowledge, this is the first survey focusing on audio deepfakes in\\nEnglish. This survey provides readers with a summary of 1) different deepfake\\ncategories 2) how they could be created and detected 3) the most recent trends\\nin this domain and shortcomings in detection methods 4) audio deepfakes, how\\nthey are created and detected in more detail which is the main focus of this\\npaper. We found that Generative Adversarial Networks(GAN), Convolutional Neural\\nNetworks (CNN), and Deep Neural Networks (DNN) are common ways of creating and\\ndetecting deepfakes. In our evaluation of over 140 methods we found that the\\nmajority of the focus is on video deepfakes and in particular in the generation\\nof video deepfakes. We found that for text deepfakes there are more generation\\nmethods but very few robust methods for detection, including fake news\\ndetection, which has become a controversial area of research because of the\\npotential of heavy overlaps with human generation of fake content. This paper\\nis an abbreviated version of the full survey and reveals a clear need to\\nresearch audio deepfakes and particularly detection of audio deepfakes.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Zahra Khanjani'}, {'name': 'Gabrielle Watson'}, {'name': 'Vandana P. Janeja'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Vandana P. Janeja'}\n",
      "\n",
      "\n",
      "author\n",
      "Vandana P. Janeja\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Abbreviated version of a longer survey under review\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2111.14203v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2111.14203v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2111.07725v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2111.07725v3\n",
      "\n",
      "\n",
      "updated\n",
      "2022-02-04T13:25:23Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=2, tm_mday=4, tm_hour=13, tm_min=25, tm_sec=23, tm_wday=4, tm_yday=35, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-11-15T12:52:50Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=15, tm_hour=12, tm_min=52, tm_sec=50, tm_wday=0, tm_yday=319, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Investigating self-supervised front ends for speech spoofing\n",
      "  countermeasures\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Investigating self-supervised front ends for speech spoofing\\n  countermeasures'}\n",
      "\n",
      "\n",
      "summary\n",
      "Self-supervised speech model is a rapid progressing research topic, and many\n",
      "pre-trained models have been released and used in various down stream tasks.\n",
      "For speech anti-spoofing, most countermeasures (CMs) use signal processing\n",
      "algorithms to extract acoustic features for classification. In this study, we\n",
      "use pre-trained self-supervised speech models as the front end of spoofing CMs.\n",
      "We investigated different back end architectures to be combined with the\n",
      "self-supervised front end, the effectiveness of fine-tuning the front end, and\n",
      "the performance of using different pre-trained self-supervised models. Our\n",
      "findings showed that, when a good pre-trained front end was fine-tuned with\n",
      "either a shallow or a deep neural network-based back end on the ASVspoof 2019\n",
      "logical access (LA) training set, the resulting CM not only achieved a low EER\n",
      "score on the 2019 LA test set but also significantly outperformed the baseline\n",
      "on the ASVspoof 2015, 2021 LA, and 2021 deepfake test sets. A sub-band analysis\n",
      "further demonstrated that the CM mainly used the information in a specific\n",
      "frequency band to discriminate the bona fide and spoofed trials across the test\n",
      "sets.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Self-supervised speech model is a rapid progressing research topic, and many\\npre-trained models have been released and used in various down stream tasks.\\nFor speech anti-spoofing, most countermeasures (CMs) use signal processing\\nalgorithms to extract acoustic features for classification. In this study, we\\nuse pre-trained self-supervised speech models as the front end of spoofing CMs.\\nWe investigated different back end architectures to be combined with the\\nself-supervised front end, the effectiveness of fine-tuning the front end, and\\nthe performance of using different pre-trained self-supervised models. Our\\nfindings showed that, when a good pre-trained front end was fine-tuned with\\neither a shallow or a deep neural network-based back end on the ASVspoof 2019\\nlogical access (LA) training set, the resulting CM not only achieved a low EER\\nscore on the 2019 LA test set but also significantly outperformed the baseline\\non the ASVspoof 2015, 2021 LA, and 2021 deepfake test sets. A sub-band analysis\\nfurther demonstrated that the CM mainly used the information in a specific\\nfrequency band to discriminate the bona fide and spoofed trials across the test\\nsets.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Xin Wang'}, {'name': 'Junichi Yamagishi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Junichi Yamagishi'}\n",
      "\n",
      "\n",
      "author\n",
      "Junichi Yamagishi\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "V3: added sub-band analysis, submitted to ISCA Odyssey2022; V2: added\n",
      "  min tDCF results on 2019 and 2021 LA. EERs on LA 2021 were slightly updated\n",
      "  to fix one glitch in the score file. EERs and min tDCFs on 2021 LA and DF can\n",
      "  be computed using the latest official code\n",
      "  https://github.com/asvspoof-challenge/2021. Work in progress. Feedback is\n",
      "  welcome!\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2111.07725v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2111.07725v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2111.07601v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2111.07601v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-11-15T08:44:52Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=15, tm_hour=8, tm_min=44, tm_sec=52, tm_wday=0, tm_yday=319, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-11-15T08:44:52Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=15, tm_hour=8, tm_min=44, tm_sec=52, tm_wday=0, tm_yday=319, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FakeTransformer: Exposing Face Forgery From Spatial-Temporal\n",
      "  Representation Modeled By Facial Pixel Variations\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'FakeTransformer: Exposing Face Forgery From Spatial-Temporal\\n  Representation Modeled By Facial Pixel Variations'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the rapid development of generation model, AI-based face manipulation\n",
      "technology, which called DeepFakes, has become more and more realistic. This\n",
      "means of face forgery can attack any target, which poses a new threat to\n",
      "personal privacy and property security. Moreover, the misuse of synthetic video\n",
      "shows potential dangers in many areas, such as identity harassment, pornography\n",
      "and news rumors. Inspired by the fact that the spatial coherence and temporal\n",
      "consistency of physiological signal are destroyed in the generated content, we\n",
      "attempt to find inconsistent patterns that can distinguish between real videos\n",
      "and synthetic videos from the variations of facial pixels, which are highly\n",
      "related to physiological information. Our approach first applies Eulerian Video\n",
      "Magnification (EVM) at multiple Gaussian scales to the original video to\n",
      "enlarge the physiological variations caused by the change of facial blood\n",
      "volume, and then transform the original video and magnified videos into a\n",
      "Multi-Scale Eulerian Magnified Spatial-Temporal map (MEMSTmap), which can\n",
      "represent time-varying physiological enhancement sequences on different\n",
      "octaves. Then, these maps are reshaped into frame patches in column units and\n",
      "sent to the vision Transformer to learn the spatio-time descriptors of frame\n",
      "levels. Finally, we sort out the feature embedding and output the probability\n",
      "of judging whether the video is real or fake. We validate our method on the\n",
      "FaceForensics++ and DeepFake Detection datasets. The results show that our\n",
      "model achieves excellent performance in forgery detection, and also show\n",
      "outstanding generalization capability in cross-data domain.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'With the rapid development of generation model, AI-based face manipulation\\ntechnology, which called DeepFakes, has become more and more realistic. This\\nmeans of face forgery can attack any target, which poses a new threat to\\npersonal privacy and property security. Moreover, the misuse of synthetic video\\nshows potential dangers in many areas, such as identity harassment, pornography\\nand news rumors. Inspired by the fact that the spatial coherence and temporal\\nconsistency of physiological signal are destroyed in the generated content, we\\nattempt to find inconsistent patterns that can distinguish between real videos\\nand synthetic videos from the variations of facial pixels, which are highly\\nrelated to physiological information. Our approach first applies Eulerian Video\\nMagnification (EVM) at multiple Gaussian scales to the original video to\\nenlarge the physiological variations caused by the change of facial blood\\nvolume, and then transform the original video and magnified videos into a\\nMulti-Scale Eulerian Magnified Spatial-Temporal map (MEMSTmap), which can\\nrepresent time-varying physiological enhancement sequences on different\\noctaves. Then, these maps are reshaped into frame patches in column units and\\nsent to the vision Transformer to learn the spatio-time descriptors of frame\\nlevels. Finally, we sort out the feature embedding and output the probability\\nof judging whether the video is real or fake. We validate our method on the\\nFaceForensics++ and DeepFake Detection datasets. The results show that our\\nmodel achieves excellent performance in forgery detection, and also show\\noutstanding generalization capability in cross-data domain.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yuyang Sun'}, {'name': 'Zhiyong Zhang'}, {'name': 'Changzhen Qiu'}, {'name': 'Liang Wang'}, {'name': 'Zekai Wang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Zekai Wang'}\n",
      "\n",
      "\n",
      "author\n",
      "Zekai Wang\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2111.07601v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2111.07601v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2111.07468v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2111.07468v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-11-14T22:50:39Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=14, tm_hour=22, tm_min=50, tm_sec=39, tm_wday=6, tm_yday=318, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-11-14T22:50:39Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=14, tm_hour=22, tm_min=50, tm_sec=39, tm_wday=6, tm_yday=318, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Impact of Benign Modifications on Discriminative Performance of Deepfake\n",
      "  Detectors\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Impact of Benign Modifications on Discriminative Performance of Deepfake\\n  Detectors'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes are becoming increasingly popular in both good faith applications\n",
      "such as in entertainment and maliciously intended manipulations such as in\n",
      "image and video forgery. Primarily motivated by the latter, a large number of\n",
      "deepfake detectors have been proposed recently in order to identify such\n",
      "content. While the performance of such detectors still need further\n",
      "improvements, they are often assessed in simple if not trivial scenarios. In\n",
      "particular, the impact of benign processing operations such as transcoding,\n",
      "denoising, resizing and enhancement are not sufficiently studied. This paper\n",
      "proposes a more rigorous and systematic framework to assess the performance of\n",
      "deepfake detectors in more realistic situations. It quantitatively measures how\n",
      "and to which extent each benign processing approach impacts a state-of-the-art\n",
      "deepfake detection method. By illustrating it in a popular deepfake detector,\n",
      "our benchmark proposes a framework to assess robustness of detectors and\n",
      "provides valuable insights to design more efficient deepfake detectors.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfakes are becoming increasingly popular in both good faith applications\\nsuch as in entertainment and maliciously intended manipulations such as in\\nimage and video forgery. Primarily motivated by the latter, a large number of\\ndeepfake detectors have been proposed recently in order to identify such\\ncontent. While the performance of such detectors still need further\\nimprovements, they are often assessed in simple if not trivial scenarios. In\\nparticular, the impact of benign processing operations such as transcoding,\\ndenoising, resizing and enhancement are not sufficiently studied. This paper\\nproposes a more rigorous and systematic framework to assess the performance of\\ndeepfake detectors in more realistic situations. It quantitatively measures how\\nand to which extent each benign processing approach impacts a state-of-the-art\\ndeepfake detection method. By illustrating it in a popular deepfake detector,\\nour benchmark proposes a framework to assess robustness of detectors and\\nprovides valuable insights to design more efficient deepfake detectors.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yuhang Lu'}, {'name': 'Evgeniy Upenik'}, {'name': 'Touradj Ebrahimi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Touradj Ebrahimi'}\n",
      "\n",
      "\n",
      "author\n",
      "Touradj Ebrahimi\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2111.07468v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2111.07468v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2111.02813v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2111.02813v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-11-04T12:26:34Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=4, tm_hour=12, tm_min=26, tm_sec=34, tm_wday=3, tm_yday=308, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-11-04T12:26:34Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=4, tm_hour=12, tm_min=26, tm_sec=34, tm_wday=3, tm_yday=308, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "WaveFake: A Data Set to Facilitate Audio Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'WaveFake: A Data Set to Facilitate Audio Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deep generative modeling has the potential to cause significant harm to\n",
      "society. Recognizing this threat, a magnitude of research into detecting\n",
      "so-called \"Deepfakes\" has emerged. This research most often focuses on the\n",
      "image domain, while studies exploring generated audio signals have, so-far,\n",
      "been neglected. In this paper we make three key contributions to narrow this\n",
      "gap. First, we provide researchers with an introduction to common signal\n",
      "processing techniques used for analyzing audio signals. Second, we present a\n",
      "novel data set, for which we collected nine sample sets from five different\n",
      "network architectures, spanning two languages. Finally, we supply practitioners\n",
      "with two baseline models, adopted from the signal processing community, to\n",
      "facilitate further research in this area.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deep generative modeling has the potential to cause significant harm to\\nsociety. Recognizing this threat, a magnitude of research into detecting\\nso-called \"Deepfakes\" has emerged. This research most often focuses on the\\nimage domain, while studies exploring generated audio signals have, so-far,\\nbeen neglected. In this paper we make three key contributions to narrow this\\ngap. First, we provide researchers with an introduction to common signal\\nprocessing techniques used for analyzing audio signals. Second, we present a\\nnovel data set, for which we collected nine sample sets from five different\\nnetwork architectures, spanning two languages. Finally, we supply practitioners\\nwith two baseline models, adopted from the signal processing community, to\\nfacilitate further research in this area.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Joel Frank'}, {'name': 'Lea Schönherr'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Lea Schönherr'}\n",
      "\n",
      "\n",
      "author\n",
      "Lea Schönherr\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to NeurIPS 2021 (Benchmark and Dataset Track); Code:\n",
      "  https://github.com/RUB-SysSec/WaveFake; Data:\n",
      "  https://zenodo.org/record/5642694\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2111.02813v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2111.02813v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2110.15561v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2110.15561v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-29T06:05:52Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=29, tm_hour=6, tm_min=5, tm_sec=52, tm_wday=4, tm_yday=302, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-10-29T06:05:52Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=29, tm_hour=6, tm_min=5, tm_sec=52, tm_wday=4, tm_yday=302, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Exposing Deepfake with Pixel-wise AR and PPG Correlation from Faint\n",
      "  Signals\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Exposing Deepfake with Pixel-wise AR and PPG Correlation from Faint\\n  Signals'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake poses a serious threat to the reliability of judicial evidence and\n",
      "intellectual property protection. In spite of an urgent need for Deepfake\n",
      "identification, existing pixel-level detection methods are increasingly unable\n",
      "to resist the growing realism of fake videos and lack generalization. In this\n",
      "paper, we propose a scheme to expose Deepfake through faint signals hidden in\n",
      "face videos. This scheme extracts two types of minute information hidden\n",
      "between face pixels-photoplethysmography (PPG) features and auto-regressive\n",
      "(AR) features, which are used as the basis for forensics in the temporal and\n",
      "spatial domains, respectively. According to the principle of PPG, tracking the\n",
      "absorption of light by blood cells allows remote estimation of the temporal\n",
      "domains heart rate (HR) of face video, and irregular HR fluctuations can be\n",
      "seen as traces of tampering. On the other hand, AR coefficients are able to\n",
      "reflect the inter-pixel correlation, and can also reflect the traces of\n",
      "smoothing caused by up-sampling in the process of generating fake faces.\n",
      "Furthermore, the scheme combines asymmetric convolution block (ACBlock)-based\n",
      "improved densely connected networks (DenseNets) to achieve face video\n",
      "authenticity forensics. Its asymmetric convolutional structure enhances the\n",
      "robustness of network to the input feature image upside-down and left-right\n",
      "flipping, so that the sequence of feature stitching does not affect detection\n",
      "results. Simulation results show that our proposed scheme provides more\n",
      "accurate authenticity detection results on multiple deep forgery datasets and\n",
      "has better generalization compared to the benchmark strategy.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfake poses a serious threat to the reliability of judicial evidence and\\nintellectual property protection. In spite of an urgent need for Deepfake\\nidentification, existing pixel-level detection methods are increasingly unable\\nto resist the growing realism of fake videos and lack generalization. In this\\npaper, we propose a scheme to expose Deepfake through faint signals hidden in\\nface videos. This scheme extracts two types of minute information hidden\\nbetween face pixels-photoplethysmography (PPG) features and auto-regressive\\n(AR) features, which are used as the basis for forensics in the temporal and\\nspatial domains, respectively. According to the principle of PPG, tracking the\\nabsorption of light by blood cells allows remote estimation of the temporal\\ndomains heart rate (HR) of face video, and irregular HR fluctuations can be\\nseen as traces of tampering. On the other hand, AR coefficients are able to\\nreflect the inter-pixel correlation, and can also reflect the traces of\\nsmoothing caused by up-sampling in the process of generating fake faces.\\nFurthermore, the scheme combines asymmetric convolution block (ACBlock)-based\\nimproved densely connected networks (DenseNets) to achieve face video\\nauthenticity forensics. Its asymmetric convolutional structure enhances the\\nrobustness of network to the input feature image upside-down and left-right\\nflipping, so that the sequence of feature stitching does not affect detection\\nresults. Simulation results show that our proposed scheme provides more\\naccurate authenticity detection results on multiple deep forgery datasets and\\nhas better generalization compared to the benchmark strategy.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Maoyu Mao'}, {'name': 'Jun Yang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jun Yang'}\n",
      "\n",
      "\n",
      "author\n",
      "Jun Yang\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2110.15561v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2110.15561v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2110.03309v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2110.03309v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-07T10:00:04Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=7, tm_hour=10, tm_min=0, tm_sec=4, tm_wday=3, tm_yday=280, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-10-07T10:00:04Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=7, tm_hour=10, tm_min=0, tm_sec=4, tm_wday=3, tm_yday=280, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Explaining deep learning models for spoofing and deepfake detection with\n",
      "  SHapley Additive exPlanations\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Explaining deep learning models for spoofing and deepfake detection with\\n  SHapley Additive exPlanations'}\n",
      "\n",
      "\n",
      "summary\n",
      "Substantial progress in spoofing and deepfake detection has been made in\n",
      "recent years. Nonetheless, the community has yet to make notable inroads in\n",
      "providing an explanation for how a classifier produces its output. The\n",
      "dominance of black box spoofing detection solutions is at further odds with the\n",
      "drive toward trustworthy, explainable artificial intelligence. This paper\n",
      "describes our use of SHapley Additive exPlanations (SHAP) to gain new insights\n",
      "in spoofing detection. We demonstrate use of the tool in revealing unexpected\n",
      "classifier behaviour, the artefacts that contribute most to classifier outputs\n",
      "and differences in the behaviour of competing spoofing detection models. The\n",
      "tool is both efficient and flexible, being readily applicable to a host of\n",
      "different architecture models in addition to related, different applications.\n",
      "All results reported in the paper are reproducible using open-source software.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Substantial progress in spoofing and deepfake detection has been made in\\nrecent years. Nonetheless, the community has yet to make notable inroads in\\nproviding an explanation for how a classifier produces its output. The\\ndominance of black box spoofing detection solutions is at further odds with the\\ndrive toward trustworthy, explainable artificial intelligence. This paper\\ndescribes our use of SHapley Additive exPlanations (SHAP) to gain new insights\\nin spoofing detection. We demonstrate use of the tool in revealing unexpected\\nclassifier behaviour, the artefacts that contribute most to classifier outputs\\nand differences in the behaviour of competing spoofing detection models. The\\ntool is both efficient and flexible, being readily applicable to a host of\\ndifferent architecture models in addition to related, different applications.\\nAll results reported in the paper are reproducible using open-source software.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Wanying Ge'}, {'name': 'Jose Patino'}, {'name': 'Massimiliano Todisco'}, {'name': 'Nicholas Evans'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Nicholas Evans'}\n",
      "\n",
      "\n",
      "author\n",
      "Nicholas Evans\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Submitted to ICASSP 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2110.03309v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2110.03309v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2110.02516v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2110.02516v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-06T05:46:31Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=6, tm_hour=5, tm_min=46, tm_sec=31, tm_wday=2, tm_yday=279, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-10-06T05:46:31Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=6, tm_hour=5, tm_min=46, tm_sec=31, tm_wday=2, tm_yday=279, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Attack as the Best Defense: Nullifying Image-to-image Translation GANs\n",
      "  via Limit-aware Adversarial Attack\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Attack as the Best Defense: Nullifying Image-to-image Translation GANs\\n  via Limit-aware Adversarial Attack'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the successful creation of high-quality image-to-image (Img2Img)\n",
      "translation GANs comes the non-ethical applications of DeepFake and DeepNude.\n",
      "Such misuses of img2img techniques present a challenging problem for society.\n",
      "In this work, we tackle the problem by introducing the Limit-Aware Self-Guiding\n",
      "Gradient Sliding Attack (LaS-GSA). LaS-GSA follows the Nullifying Attack to\n",
      "cancel the img2img translation process under a black-box setting. In other\n",
      "words, by processing input images with the proposed LaS-GSA before publishing,\n",
      "any targeted img2img GANs can be nullified, preventing the model from\n",
      "maliciously manipulating the images. To improve efficiency, we introduce the\n",
      "limit-aware random gradient-free estimation and the gradient sliding mechanism\n",
      "to estimate the gradient that adheres to the adversarial limit, i.e., the pixel\n",
      "value limitations of the adversarial example. Theoretical justifications\n",
      "validate how the above techniques prevent inefficiency caused by the\n",
      "adversarial limit in both the direction and the step length. Furthermore, an\n",
      "effective self-guiding prior is extracted solely from the threat model and the\n",
      "target image to efficiently leverage the prior information and guide the\n",
      "gradient estimation process. Extensive experiments demonstrate that LaS-GSA\n",
      "requires fewer queries to nullify the image translation process with higher\n",
      "success rates than 4 state-of-the-art black-box methods.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'With the successful creation of high-quality image-to-image (Img2Img)\\ntranslation GANs comes the non-ethical applications of DeepFake and DeepNude.\\nSuch misuses of img2img techniques present a challenging problem for society.\\nIn this work, we tackle the problem by introducing the Limit-Aware Self-Guiding\\nGradient Sliding Attack (LaS-GSA). LaS-GSA follows the Nullifying Attack to\\ncancel the img2img translation process under a black-box setting. In other\\nwords, by processing input images with the proposed LaS-GSA before publishing,\\nany targeted img2img GANs can be nullified, preventing the model from\\nmaliciously manipulating the images. To improve efficiency, we introduce the\\nlimit-aware random gradient-free estimation and the gradient sliding mechanism\\nto estimate the gradient that adheres to the adversarial limit, i.e., the pixel\\nvalue limitations of the adversarial example. Theoretical justifications\\nvalidate how the above techniques prevent inefficiency caused by the\\nadversarial limit in both the direction and the step length. Furthermore, an\\neffective self-guiding prior is extracted solely from the threat model and the\\ntarget image to efficiently leverage the prior information and guide the\\ngradient estimation process. Extensive experiments demonstrate that LaS-GSA\\nrequires fewer queries to nullify the image translation process with higher\\nsuccess rates than 4 state-of-the-art black-box methods.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Chin-Yuan Yeh'}, {'name': 'Hsi-Wen Chen'}, {'name': 'Hong-Han Shuai'}, {'name': 'De-Nian Yang'}, {'name': 'Ming-Syan Chen'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ming-Syan Chen'}\n",
      "\n",
      "\n",
      "author\n",
      "Ming-Syan Chen\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2110.02516v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2110.02516v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2110.01640v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2110.01640v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-04T18:02:56Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=4, tm_hour=18, tm_min=2, tm_sec=56, tm_wday=0, tm_yday=277, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-10-04T18:02:56Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=4, tm_hour=18, tm_min=2, tm_sec=56, tm_wday=0, tm_yday=277, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "An Experimental Evaluation on Deepfake Detection using Deep Face\n",
      "  Recognition\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'An Experimental Evaluation on Deepfake Detection using Deep Face\\n  Recognition'}\n",
      "\n",
      "\n",
      "summary\n",
      "Significant advances in deep learning have obtained hallmark accuracy rates\n",
      "for various computer vision applications. However, advances in deep generative\n",
      "models have also led to the generation of very realistic fake content, also\n",
      "known as deepfakes, causing a threat to privacy, democracy, and national\n",
      "security. Most of the current deepfake detection methods are deemed as a binary\n",
      "classification problem in distinguishing authentic images or videos from fake\n",
      "ones using two-class convolutional neural networks (CNNs). These methods are\n",
      "based on detecting visual artifacts, temporal or color inconsistencies produced\n",
      "by deep generative models. However, these methods require a large amount of\n",
      "real and fake data for model training and their performance drops significantly\n",
      "in cross dataset evaluation with samples generated using advanced deepfake\n",
      "generation techniques. In this paper, we thoroughly evaluate the efficacy of\n",
      "deep face recognition in identifying deepfakes, using different loss functions\n",
      "and deepfake generation techniques. Experimental investigations on challenging\n",
      "Celeb-DF and FaceForensics++ deepfake datasets suggest the efficacy of deep\n",
      "face recognition in identifying deepfakes over two-class CNNs and the ocular\n",
      "modality. Reported results suggest a maximum Area Under Curve (AUC) of 0.98 and\n",
      "an Equal Error Rate (EER) of 7.1% in detecting deepfakes using face recognition\n",
      "on the Celeb-DF dataset. This EER is lower by 16.6% compared to the EER\n",
      "obtained for the two-class CNN and the ocular modality on the Celeb-DF dataset.\n",
      "Further on the FaceForensics++ dataset, an AUC of 0.99 and EER of 2.04% were\n",
      "obtained. The use of biometric facial recognition technology has the advantage\n",
      "of bypassing the need for a large amount of fake data for model training and\n",
      "obtaining better generalizability to evolving deepfake creation techniques.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Significant advances in deep learning have obtained hallmark accuracy rates\\nfor various computer vision applications. However, advances in deep generative\\nmodels have also led to the generation of very realistic fake content, also\\nknown as deepfakes, causing a threat to privacy, democracy, and national\\nsecurity. Most of the current deepfake detection methods are deemed as a binary\\nclassification problem in distinguishing authentic images or videos from fake\\nones using two-class convolutional neural networks (CNNs). These methods are\\nbased on detecting visual artifacts, temporal or color inconsistencies produced\\nby deep generative models. However, these methods require a large amount of\\nreal and fake data for model training and their performance drops significantly\\nin cross dataset evaluation with samples generated using advanced deepfake\\ngeneration techniques. In this paper, we thoroughly evaluate the efficacy of\\ndeep face recognition in identifying deepfakes, using different loss functions\\nand deepfake generation techniques. Experimental investigations on challenging\\nCeleb-DF and FaceForensics++ deepfake datasets suggest the efficacy of deep\\nface recognition in identifying deepfakes over two-class CNNs and the ocular\\nmodality. Reported results suggest a maximum Area Under Curve (AUC) of 0.98 and\\nan Equal Error Rate (EER) of 7.1% in detecting deepfakes using face recognition\\non the Celeb-DF dataset. This EER is lower by 16.6% compared to the EER\\nobtained for the two-class CNN and the ocular modality on the Celeb-DF dataset.\\nFurther on the FaceForensics++ dataset, an AUC of 0.99 and EER of 2.04% were\\nobtained. The use of biometric facial recognition technology has the advantage\\nof bypassing the need for a large amount of fake data for model training and\\nobtaining better generalizability to evolving deepfake creation techniques.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sreeraj Ramachandran'}, {'name': 'Aakash Varma Nadimpalli'}, {'name': 'Ajita Rattani'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ajita Rattani'}\n",
      "\n",
      "\n",
      "author\n",
      "Ajita Rattani\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "6 pages, 3 figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2110.01640v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2110.01640v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.14136v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.14136v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-29T01:54:13Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=29, tm_hour=1, tm_min=54, tm_sec=13, tm_wday=2, tm_yday=272, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-29T01:54:13Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=29, tm_hour=1, tm_min=54, tm_sec=13, tm_wday=2, tm_yday=272, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Improved Xception with Dual Attention Mechanism and Feature Fusion for\n",
      "  Face Forgery Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Improved Xception with Dual Attention Mechanism and Feature Fusion for\\n  Face Forgery Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the rapid development of deep learning technology, more and more face\n",
      "forgeries by deepfake are widely spread on social media, causing serious social\n",
      "concern. Face forgery detection has become a research hotspot in recent years,\n",
      "and many related methods have been proposed until now. For those images with\n",
      "low quality and/or diverse sources, however, the detection performances of\n",
      "existing methods are still far from satisfactory. In this paper, we propose an\n",
      "improved Xception with dual attention mechanism and feature fusion for face\n",
      "forgery detection. Different from the middle flow in original Xception model,\n",
      "we try to catch different high-semantic features of the face images using\n",
      "different levels of convolution, and introduce the convolutional block\n",
      "attention module and feature fusion to refine and reorganize those\n",
      "high-semantic features. In the exit flow, we employ the self-attention\n",
      "mechanism and depthwise separable convolution to learn the global information\n",
      "and local information of the fused features separately to improve the\n",
      "classification the ability of the proposed model. Experimental results\n",
      "evaluated on three Deepfake datasets demonstrate that the proposed method\n",
      "outperforms Xception as well as other related methods both in effectiveness and\n",
      "generalization ability.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'With the rapid development of deep learning technology, more and more face\\nforgeries by deepfake are widely spread on social media, causing serious social\\nconcern. Face forgery detection has become a research hotspot in recent years,\\nand many related methods have been proposed until now. For those images with\\nlow quality and/or diverse sources, however, the detection performances of\\nexisting methods are still far from satisfactory. In this paper, we propose an\\nimproved Xception with dual attention mechanism and feature fusion for face\\nforgery detection. Different from the middle flow in original Xception model,\\nwe try to catch different high-semantic features of the face images using\\ndifferent levels of convolution, and introduce the convolutional block\\nattention module and feature fusion to refine and reorganize those\\nhigh-semantic features. In the exit flow, we employ the self-attention\\nmechanism and depthwise separable convolution to learn the global information\\nand local information of the fused features separately to improve the\\nclassification the ability of the proposed model. Experimental results\\nevaluated on three Deepfake datasets demonstrate that the proposed method\\noutperforms Xception as well as other related methods both in effectiveness and\\ngeneralization ability.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hao Lin'}, {'name': 'Weiqi Luo'}, {'name': 'Kangkang Wei'}, {'name': 'Minglin Liu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Minglin Liu'}\n",
      "\n",
      "\n",
      "author\n",
      "Minglin Liu\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.14136v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.14136v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.12800v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.12800v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-27T05:10:55Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=27, tm_hour=5, tm_min=10, tm_sec=55, tm_wday=0, tm_yday=270, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-27T05:10:55Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=27, tm_hour=5, tm_min=10, tm_sec=55, tm_wday=0, tm_yday=270, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Machine Learning based Medical Image Deepfake Detection: A Comparative\n",
      "  Study\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Machine Learning based Medical Image Deepfake Detection: A Comparative\\n  Study'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deep generative networks in recent years have reinforced the need for caution\n",
      "while consuming various modalities of digital information. One avenue of\n",
      "deepfake creation is aligned with injection and removal of tumors from medical\n",
      "scans. Failure to detect medical deepfakes can lead to large setbacks on\n",
      "hospital resources or even loss of life. This paper attempts to address the\n",
      "detection of such attacks with a structured case study. We evaluate different\n",
      "machine learning algorithms and pretrained convolutional neural networks on\n",
      "distinguishing between tampered and untampered data. The findings of this work\n",
      "show near perfect accuracy in detecting instances of tumor injections and\n",
      "removals.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deep generative networks in recent years have reinforced the need for caution\\nwhile consuming various modalities of digital information. One avenue of\\ndeepfake creation is aligned with injection and removal of tumors from medical\\nscans. Failure to detect medical deepfakes can lead to large setbacks on\\nhospital resources or even loss of life. This paper attempts to address the\\ndetection of such attacks with a structured case study. We evaluate different\\nmachine learning algorithms and pretrained convolutional neural networks on\\ndistinguishing between tampered and untampered data. The findings of this work\\nshow near perfect accuracy in detecting instances of tumor injections and\\nremovals.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Siddharth Solaiyappan'}, {'name': 'Yuxin Wen'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yuxin Wen'}\n",
      "\n",
      "\n",
      "author\n",
      "Yuxin Wen\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.12800v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.12800v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.10688v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.10688v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-21T16:18:45Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=21, tm_hour=16, tm_min=18, tm_sec=45, tm_wday=1, tm_yday=264, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-21T16:18:45Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=21, tm_hour=16, tm_min=18, tm_sec=45, tm_wday=1, tm_yday=264, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Finding Facial Forgery Artifacts with Parts-Based Detectors\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Finding Facial Forgery Artifacts with Parts-Based Detectors'}\n",
      "\n",
      "\n",
      "summary\n",
      "Manipulated videos, especially those where the identity of an individual has\n",
      "been modified using deep neural networks, are becoming an increasingly relevant\n",
      "threat in the modern day. In this paper, we seek to develop a generalizable,\n",
      "explainable solution to detecting these manipulated videos. To achieve this, we\n",
      "design a series of forgery detection systems that each focus on one individual\n",
      "part of the face. These parts-based detection systems, which can be combined\n",
      "and used together in a single architecture, meet all of our desired criteria -\n",
      "they generalize effectively between datasets and give us valuable insights into\n",
      "what the network is looking at when making its decision. We thus use these\n",
      "detectors to perform detailed empirical analysis on the FaceForensics++,\n",
      "Celeb-DF, and Facebook Deepfake Detection Challenge datasets, examining not\n",
      "just what the detectors find but also collecting and analyzing useful related\n",
      "statistics on the datasets themselves.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Manipulated videos, especially those where the identity of an individual has\\nbeen modified using deep neural networks, are becoming an increasingly relevant\\nthreat in the modern day. In this paper, we seek to develop a generalizable,\\nexplainable solution to detecting these manipulated videos. To achieve this, we\\ndesign a series of forgery detection systems that each focus on one individual\\npart of the face. These parts-based detection systems, which can be combined\\nand used together in a single architecture, meet all of our desired criteria -\\nthey generalize effectively between datasets and give us valuable insights into\\nwhat the network is looking at when making its decision. We thus use these\\ndetectors to perform detailed empirical analysis on the FaceForensics++,\\nCeleb-DF, and Facebook Deepfake Detection Challenge datasets, examining not\\njust what the detectors find but also collecting and analyzing useful related\\nstatistics on the datasets themselves.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Steven Schwarcz'}, {'name': 'Rama Chellappa'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Rama Chellappa'}\n",
      "\n",
      "\n",
      "author\n",
      "Rama Chellappa\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted into the CVPR Workshop on Media Forensics 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.10688v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.10688v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.07311v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.07311v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-15T14:11:53Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=15, tm_hour=14, tm_min=11, tm_sec=53, tm_wday=2, tm_yday=258, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-15T14:11:53Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=15, tm_hour=14, tm_min=11, tm_sec=53, tm_wday=2, tm_yday=258, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "MD-CSDNetwork: Multi-Domain Cross Stitched Network for Deepfake\n",
      "  Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'MD-CSDNetwork: Multi-Domain Cross Stitched Network for Deepfake\\n  Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "The rapid progress in the ease of creating and spreading ultra-realistic\n",
      "media over social platforms calls for an urgent need to develop a generalizable\n",
      "deepfake detection technique. It has been observed that current deepfake\n",
      "generation methods leave discriminative artifacts in the frequency spectrum of\n",
      "fake images and videos. Inspired by this observation, in this paper, we present\n",
      "a novel approach, termed as MD-CSDNetwork, for combining the features in the\n",
      "spatial and frequency domains to mine a shared discriminative representation\n",
      "for classifying \\textit{deepfakes}. MD-CSDNetwork is a novel cross-stitched\n",
      "network with two parallel branches carrying the spatial and frequency\n",
      "information, respectively. We hypothesize that these multi-domain input data\n",
      "streams can be considered as related supervisory signals. The supervision from\n",
      "both branches ensures better performance and generalization. Further, the\n",
      "concept of cross-stitch connections is utilized where they are inserted between\n",
      "the two branches to learn an optimal combination of domain-specific and shared\n",
      "representations from other domains automatically. Extensive experiments are\n",
      "conducted on the popular benchmark dataset namely FaceForeniscs++ for forgery\n",
      "classification. We report improvements over all the manipulation types in\n",
      "FaceForensics++ dataset and comparable results with state-of-the-art methods\n",
      "for cross-database evaluation on the Celeb-DF dataset and the Deepfake\n",
      "Detection Dataset.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The rapid progress in the ease of creating and spreading ultra-realistic\\nmedia over social platforms calls for an urgent need to develop a generalizable\\ndeepfake detection technique. It has been observed that current deepfake\\ngeneration methods leave discriminative artifacts in the frequency spectrum of\\nfake images and videos. Inspired by this observation, in this paper, we present\\na novel approach, termed as MD-CSDNetwork, for combining the features in the\\nspatial and frequency domains to mine a shared discriminative representation\\nfor classifying \\\\textit{deepfakes}. MD-CSDNetwork is a novel cross-stitched\\nnetwork with two parallel branches carrying the spatial and frequency\\ninformation, respectively. We hypothesize that these multi-domain input data\\nstreams can be considered as related supervisory signals. The supervision from\\nboth branches ensures better performance and generalization. Further, the\\nconcept of cross-stitch connections is utilized where they are inserted between\\nthe two branches to learn an optimal combination of domain-specific and shared\\nrepresentations from other domains automatically. Extensive experiments are\\nconducted on the popular benchmark dataset namely FaceForeniscs++ for forgery\\nclassification. We report improvements over all the manipulation types in\\nFaceForensics++ dataset and comparable results with state-of-the-art methods\\nfor cross-database evaluation on the Celeb-DF dataset and the Deepfake\\nDetection Dataset.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Aayushi Agarwal'}, {'name': 'Akshay Agarwal'}, {'name': 'Sayan Sinha'}, {'name': 'Mayank Vatsa'}, {'name': 'Richa Singh'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Richa Singh'}\n",
      "\n",
      "\n",
      "author\n",
      "Richa Singh\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "8 pages\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.07311v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.07311v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.05673v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.05673v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-13T02:36:25Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=13, tm_hour=2, tm_min=36, tm_sec=25, tm_wday=0, tm_yday=256, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-13T02:36:25Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=13, tm_hour=2, tm_min=36, tm_sec=25, tm_wday=0, tm_yday=256, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FaceGuard: Proactive Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'FaceGuard: Proactive Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Existing deepfake-detection methods focus on passive detection, i.e., they\n",
      "detect fake face images via exploiting the artifacts produced during deepfake\n",
      "manipulation. A key limitation of passive detection is that it cannot detect\n",
      "fake faces that are generated by new deepfake generation methods. In this work,\n",
      "we propose FaceGuard, a proactive deepfake-detection framework. FaceGuard\n",
      "embeds a watermark into a real face image before it is published on social\n",
      "media. Given a face image that claims to be an individual (e.g., Nicolas Cage),\n",
      "FaceGuard extracts a watermark from it and predicts the face image to be fake\n",
      "if the extracted watermark does not match well with the individual's ground\n",
      "truth one. A key component of FaceGuard is a new deep-learning-based\n",
      "watermarking method, which is 1) robust to normal image post-processing such as\n",
      "JPEG compression, Gaussian blurring, cropping, and resizing, but 2) fragile to\n",
      "deepfake manipulation. Our evaluation on multiple datasets shows that FaceGuard\n",
      "can detect deepfakes accurately and outperforms existing methods.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Existing deepfake-detection methods focus on passive detection, i.e., they\\ndetect fake face images via exploiting the artifacts produced during deepfake\\nmanipulation. A key limitation of passive detection is that it cannot detect\\nfake faces that are generated by new deepfake generation methods. In this work,\\nwe propose FaceGuard, a proactive deepfake-detection framework. FaceGuard\\nembeds a watermark into a real face image before it is published on social\\nmedia. Given a face image that claims to be an individual (e.g., Nicolas Cage),\\nFaceGuard extracts a watermark from it and predicts the face image to be fake\\nif the extracted watermark does not match well with the individual's ground\\ntruth one. A key component of FaceGuard is a new deep-learning-based\\nwatermarking method, which is 1) robust to normal image post-processing such as\\nJPEG compression, Gaussian blurring, cropping, and resizing, but 2) fragile to\\ndeepfake manipulation. Our evaluation on multiple datasets shows that FaceGuard\\ncan detect deepfakes accurately and outperforms existing methods.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yuankun Yang'}, {'name': 'Chenyue Liang'}, {'name': 'Hongyu He'}, {'name': 'Xiaoyu Cao'}, {'name': 'Neil Zhenqiang Gong'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Neil Zhenqiang Gong'}\n",
      "\n",
      "\n",
      "author\n",
      "Neil Zhenqiang Gong\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.05673v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.05673v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.05397v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.05397v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-26T06:50:13Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=26, tm_hour=6, tm_min=50, tm_sec=13, tm_wday=6, tm_yday=269, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-12T01:22:12Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=12, tm_hour=1, tm_min=22, tm_sec=12, tm_wday=6, tm_yday=255, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Challenges and Solutions in DeepFakes\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Challenges and Solutions in DeepFakes'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deep learning has been successfully appertained to solve various complex\n",
      "problems in the area of big data analytics to computer vision. A deep\n",
      "learning-powered application recently emerged is Deep Fake. It helps to create\n",
      "fake images and videos that human cannot distinguish them from the real ones\n",
      "and are recent off-shelf manipulation technique that allows swapping two\n",
      "identities in a single video. Technology is a controversial technology with\n",
      "many wide-reaching issues impacting society. So, to counter this emerging\n",
      "problem, we introduce a dataset of 140k real and fake faces which contain 70k\n",
      "real faces from the Flickr dataset collected by Nvidia, as well as 70k fake\n",
      "faces sampled from 1 million fake faces generated by style GAN. We will train\n",
      "our model in the dataset so that our model can identify real or fake faces.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deep learning has been successfully appertained to solve various complex\\nproblems in the area of big data analytics to computer vision. A deep\\nlearning-powered application recently emerged is Deep Fake. It helps to create\\nfake images and videos that human cannot distinguish them from the real ones\\nand are recent off-shelf manipulation technique that allows swapping two\\nidentities in a single video. Technology is a controversial technology with\\nmany wide-reaching issues impacting society. So, to counter this emerging\\nproblem, we introduce a dataset of 140k real and fake faces which contain 70k\\nreal faces from the Flickr dataset collected by Nvidia, as well as 70k fake\\nfaces sampled from 1 million fake faces generated by style GAN. We will train\\nour model in the dataset so that our model can identify real or fake faces.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Jatin Sharma'}, {'name': 'Sahil Sharma'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Sahil Sharma'}\n",
      "\n",
      "\n",
      "author\n",
      "Sahil Sharma\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Paper has a lot of mistakes and is not good enough according to the\n",
      "  technical standards a research paper should have\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.05397v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.05397v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.04991v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.04991v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-17T10:13:49Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=17, tm_hour=10, tm_min=13, tm_sec=49, tm_wday=4, tm_yday=260, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-10T16:59:15Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=10, tm_hour=16, tm_min=59, tm_sec=15, tm_wday=4, tm_yday=253, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Detection of GAN-synthesized street videos\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Detection of GAN-synthesized street videos'}\n",
      "\n",
      "\n",
      "summary\n",
      "Research on the detection of AI-generated videos has focused almost\n",
      "exclusively on face videos, usually referred to as deepfakes. Manipulations\n",
      "like face swapping, face reenactment and expression manipulation have been the\n",
      "subject of an intense research with the development of a number of efficient\n",
      "tools to distinguish artificial videos from genuine ones. Much less attention\n",
      "has been paid to the detection of artificial non-facial videos. Yet, new tools\n",
      "for the generation of such kind of videos are being developed at a fast pace\n",
      "and will soon reach the quality level of deepfake videos. The goal of this\n",
      "paper is to investigate the detectability of a new kind of AI-generated videos\n",
      "framing driving street sequences (here referred to as DeepStreets videos),\n",
      "which, by their nature, can not be analysed with the same tools used for facial\n",
      "deepfakes. Specifically, we present a simple frame-based detector, achieving\n",
      "very good performance on state-of-the-art DeepStreets videos generated by the\n",
      "Vid2vid architecture. Noticeably, the detector retains very good performance on\n",
      "compressed videos, even when the compression level used during training does\n",
      "not match that used for the test videos.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Research on the detection of AI-generated videos has focused almost\\nexclusively on face videos, usually referred to as deepfakes. Manipulations\\nlike face swapping, face reenactment and expression manipulation have been the\\nsubject of an intense research with the development of a number of efficient\\ntools to distinguish artificial videos from genuine ones. Much less attention\\nhas been paid to the detection of artificial non-facial videos. Yet, new tools\\nfor the generation of such kind of videos are being developed at a fast pace\\nand will soon reach the quality level of deepfake videos. The goal of this\\npaper is to investigate the detectability of a new kind of AI-generated videos\\nframing driving street sequences (here referred to as DeepStreets videos),\\nwhich, by their nature, can not be analysed with the same tools used for facial\\ndeepfakes. Specifically, we present a simple frame-based detector, achieving\\nvery good performance on state-of-the-art DeepStreets videos generated by the\\nVid2vid architecture. Noticeably, the detector retains very good performance on\\ncompressed videos, even when the compression level used during training does\\nnot match that used for the test videos.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Omran Alamayreh'}, {'name': 'Mauro Barni'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Mauro Barni'}\n",
      "\n",
      "\n",
      "author\n",
      "Mauro Barni\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted in the 29th European Signal Processing Conference (EUSIPCO),\n",
      "  Dublin, Ireland, August 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.04991v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.04991v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.02993v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.02993v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-07T11:00:20Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=7, tm_hour=11, tm_min=0, tm_sec=20, tm_wday=1, tm_yday=250, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-07T11:00:20Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=7, tm_hour=11, tm_min=0, tm_sec=20, tm_wday=1, tm_yday=250, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal\n",
      "  and Multimodal Detectors\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal\\n  and Multimodal Detectors'}\n",
      "\n",
      "\n",
      "summary\n",
      "Significant advancements made in the generation of deepfakes have caused\n",
      "security and privacy issues. Attackers can easily impersonate a person's\n",
      "identity in an image by replacing his face with the target person's face.\n",
      "Moreover, a new domain of cloning human voices using deep-learning technologies\n",
      "is also emerging. Now, an attacker can generate realistic cloned voices of\n",
      "humans using only a few seconds of audio of the target person. With the\n",
      "emerging threat of potential harm deepfakes can cause, researchers have\n",
      "proposed deepfake detection methods. However, they only focus on detecting a\n",
      "single modality, i.e., either video or audio. On the other hand, to develop a\n",
      "good deepfake detector that can cope with the recent advancements in deepfake\n",
      "generation, we need to have a detector that can detect deepfakes of multiple\n",
      "modalities, i.e., videos and audios. To build such a detector, we need a\n",
      "dataset that contains video and respective audio deepfakes. We were able to\n",
      "find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection\n",
      "Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized\n",
      "fake audios as well. We used this multimodal deepfake dataset and performed\n",
      "detailed baseline experiments using state-of-the-art unimodal, ensemble-based,\n",
      "and multimodal detection methods to evaluate it. We conclude through detailed\n",
      "experimentation that unimodals, addressing only a single modality, video or\n",
      "audio, do not perform well compared to ensemble-based methods. Whereas purely\n",
      "multimodal-based baselines provide the worst performance.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Significant advancements made in the generation of deepfakes have caused\\nsecurity and privacy issues. Attackers can easily impersonate a person's\\nidentity in an image by replacing his face with the target person's face.\\nMoreover, a new domain of cloning human voices using deep-learning technologies\\nis also emerging. Now, an attacker can generate realistic cloned voices of\\nhumans using only a few seconds of audio of the target person. With the\\nemerging threat of potential harm deepfakes can cause, researchers have\\nproposed deepfake detection methods. However, they only focus on detecting a\\nsingle modality, i.e., either video or audio. On the other hand, to develop a\\ngood deepfake detector that can cope with the recent advancements in deepfake\\ngeneration, we need to have a detector that can detect deepfakes of multiple\\nmodalities, i.e., videos and audios. To build such a detector, we need a\\ndataset that contains video and respective audio deepfakes. We were able to\\nfind a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection\\nDataset (FakeAVCeleb), that contains not only deepfake videos but synthesized\\nfake audios as well. We used this multimodal deepfake dataset and performed\\ndetailed baseline experiments using state-of-the-art unimodal, ensemble-based,\\nand multimodal detection methods to evaluate it. We conclude through detailed\\nexperimentation that unimodals, addressing only a single modality, video or\\naudio, do not perform well compared to ensemble-based methods. Whereas purely\\nmultimodal-based baselines provide the worst performance.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hasam Khalid'}, {'name': 'Minha Kim'}, {'name': 'Shahroz Tariq'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1145/3476099.3484315\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3476099.3484315', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2109.02993v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.02993v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on\n",
      "  Synthetic Multimedia - Audiovisual Deepfake Generation and Detection (ADGD\n",
      "  '21) at ACM MM 2021\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.4.9; I.5.4', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.02874v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.02874v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-07T05:19:36Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=7, tm_hour=5, tm_min=19, tm_sec=36, tm_wday=1, tm_yday=250, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-07T05:19:36Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=7, tm_hour=5, tm_min=19, tm_sec=36, tm_wday=1, tm_yday=250, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFakes: Detecting Forged and Synthetic Media Content Using Machine\n",
      "  Learning\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'DeepFakes: Detecting Forged and Synthetic Media Content Using Machine\\n  Learning'}\n",
      "\n",
      "\n",
      "summary\n",
      "The rapid advancement in deep learning makes the differentiation of authentic\n",
      "and manipulated facial images and video clips unprecedentedly harder. The\n",
      "underlying technology of manipulating facial appearances through deep\n",
      "generative approaches, enunciated as DeepFake that have emerged recently by\n",
      "promoting a vast number of malicious face manipulation applications.\n",
      "Subsequently, the need of other sort of techniques that can assess the\n",
      "integrity of digital visual content is indisputable to reduce the impact of the\n",
      "creations of DeepFake. A large body of research that are performed on DeepFake\n",
      "creation and detection create a scope of pushing each other beyond the current\n",
      "status. This study presents challenges, research trends, and directions related\n",
      "to DeepFake creation and detection techniques by reviewing the notable research\n",
      "in the DeepFake domain to facilitate the development of more robust approaches\n",
      "that could deal with the more advance DeepFake in the future.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The rapid advancement in deep learning makes the differentiation of authentic\\nand manipulated facial images and video clips unprecedentedly harder. The\\nunderlying technology of manipulating facial appearances through deep\\ngenerative approaches, enunciated as DeepFake that have emerged recently by\\npromoting a vast number of malicious face manipulation applications.\\nSubsequently, the need of other sort of techniques that can assess the\\nintegrity of digital visual content is indisputable to reduce the impact of the\\ncreations of DeepFake. A large body of research that are performed on DeepFake\\ncreation and detection create a scope of pushing each other beyond the current\\nstatus. This study presents challenges, research trends, and directions related\\nto DeepFake creation and detection techniques by reviewing the notable research\\nin the DeepFake domain to facilitate the development of more robust approaches\\nthat could deal with the more advance DeepFake in the future.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sm Zobaed'}, {'name': 'Md Fazle Rabby'}, {'name': 'Md Istiaq Hossain'}, {'name': 'Ekram Hossain'}, {'name': 'Sazib Hasan'}, {'name': 'Asif Karim'}, {'name': 'Khan Md. Hasib'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Khan Md. Hasib'}\n",
      "\n",
      "\n",
      "author\n",
      "Khan Md. Hasib\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "A preprint version\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.02874v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.02874v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.01860v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.01860v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-11T05:15:08Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=11, tm_hour=5, tm_min=15, tm_sec=8, tm_wday=0, tm_yday=284, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-04T13:05:37Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=4, tm_hour=13, tm_min=5, tm_sec=37, tm_wday=5, tm_yday=247, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Spatiotemporal Inconsistency Learning for DeepFake Video Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Spatiotemporal Inconsistency Learning for DeepFake Video Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "The rapid development of facial manipulation techniques has aroused public\n",
      "concerns in recent years. Following the success of deep learning, existing\n",
      "methods always formulate DeepFake video detection as a binary classification\n",
      "problem and develop frame-based and video-based solutions. However, little\n",
      "attention has been paid to capturing the spatial-temporal inconsistency in\n",
      "forged videos. To address this issue, we term this task as a Spatial-Temporal\n",
      "Inconsistency Learning (STIL) process and instantiate it into a novel STIL\n",
      "block, which consists of a Spatial Inconsistency Module (SIM), a Temporal\n",
      "Inconsistency Module (TIM), and an Information Supplement Module (ISM).\n",
      "Specifically, we present a novel temporal modeling paradigm in TIM by\n",
      "exploiting the temporal difference over adjacent frames along with both\n",
      "horizontal and vertical directions. And the ISM simultaneously utilizes the\n",
      "spatial information from SIM and temporal information from TIM to establish a\n",
      "more comprehensive spatial-temporal representation. Moreover, our STIL block is\n",
      "flexible and could be plugged into existing 2D CNNs. Extensive experiments and\n",
      "visualizations are presented to demonstrate the effectiveness of our method\n",
      "against the state-of-the-art competitors.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The rapid development of facial manipulation techniques has aroused public\\nconcerns in recent years. Following the success of deep learning, existing\\nmethods always formulate DeepFake video detection as a binary classification\\nproblem and develop frame-based and video-based solutions. However, little\\nattention has been paid to capturing the spatial-temporal inconsistency in\\nforged videos. To address this issue, we term this task as a Spatial-Temporal\\nInconsistency Learning (STIL) process and instantiate it into a novel STIL\\nblock, which consists of a Spatial Inconsistency Module (SIM), a Temporal\\nInconsistency Module (TIM), and an Information Supplement Module (ISM).\\nSpecifically, we present a novel temporal modeling paradigm in TIM by\\nexploiting the temporal difference over adjacent frames along with both\\nhorizontal and vertical directions. And the ISM simultaneously utilizes the\\nspatial information from SIM and temporal information from TIM to establish a\\nmore comprehensive spatial-temporal representation. Moreover, our STIL block is\\nflexible and could be plugged into existing 2D CNNs. Extensive experiments and\\nvisualizations are presented to demonstrate the effectiveness of our method\\nagainst the state-of-the-art competitors.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Zhihao Gu'}, {'name': 'Yang Chen'}, {'name': 'Taiping Yao'}, {'name': 'Shouhong Ding'}, {'name': 'Jilin Li'}, {'name': 'Feiyue Huang'}, {'name': 'Lizhuang Ma'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Lizhuang Ma'}\n",
      "\n",
      "\n",
      "author\n",
      "Lizhuang Ma\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To appear in ACM MM 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.01860v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.01860v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.00537v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.00537v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-01T16:17:31Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=1, tm_hour=16, tm_min=17, tm_sec=31, tm_wday=2, tm_yday=244, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-01T16:17:31Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=1, tm_hour=16, tm_min=17, tm_sec=31, tm_wday=2, tm_yday=244, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "ASVspoof 2021: accelerating progress in spoofed and deepfake speech\n",
      "  detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'ASVspoof 2021: accelerating progress in spoofed and deepfake speech\\n  detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "ASVspoof 2021 is the forth edition in the series of bi-annual challenges\n",
      "which aim to promote the study of spoofing and the design of countermeasures to\n",
      "protect automatic speaker verification systems from manipulation. In addition\n",
      "to a continued focus upon logical and physical access tasks in which there are\n",
      "a number of advances compared to previous editions, ASVspoof 2021 introduces a\n",
      "new task involving deepfake speech detection. This paper describes all three\n",
      "tasks, the new databases for each of them, the evaluation metrics, four\n",
      "challenge baselines, the evaluation platform and a summary of challenge\n",
      "results. Despite the introduction of channel and compression variability which\n",
      "compound the difficulty, results for the logical access and deepfake tasks are\n",
      "close to those from previous ASVspoof editions. Results for the physical access\n",
      "task show the difficulty in detecting attacks in real, variable physical\n",
      "spaces. With ASVspoof 2021 being the first edition for which participants were\n",
      "not provided with any matched training or development data and with this\n",
      "reflecting real conditions in which the nature of spoofed and deepfake speech\n",
      "can never be predicated with confidence, the results are extremely encouraging\n",
      "and demonstrate the substantial progress made in the field in recent years.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'ASVspoof 2021 is the forth edition in the series of bi-annual challenges\\nwhich aim to promote the study of spoofing and the design of countermeasures to\\nprotect automatic speaker verification systems from manipulation. In addition\\nto a continued focus upon logical and physical access tasks in which there are\\na number of advances compared to previous editions, ASVspoof 2021 introduces a\\nnew task involving deepfake speech detection. This paper describes all three\\ntasks, the new databases for each of them, the evaluation metrics, four\\nchallenge baselines, the evaluation platform and a summary of challenge\\nresults. Despite the introduction of channel and compression variability which\\ncompound the difficulty, results for the logical access and deepfake tasks are\\nclose to those from previous ASVspoof editions. Results for the physical access\\ntask show the difficulty in detecting attacks in real, variable physical\\nspaces. With ASVspoof 2021 being the first edition for which participants were\\nnot provided with any matched training or development data and with this\\nreflecting real conditions in which the nature of spoofed and deepfake speech\\ncan never be predicated with confidence, the results are extremely encouraging\\nand demonstrate the substantial progress made in the field in recent years.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Junichi Yamagishi'}, {'name': 'Xin Wang'}, {'name': 'Massimiliano Todisco'}, {'name': 'Md Sahidullah'}, {'name': 'Jose Patino'}, {'name': 'Andreas Nautsch'}, {'name': 'Xuechen Liu'}, {'name': 'Kong Aik Lee'}, {'name': 'Tomi Kinnunen'}, {'name': 'Nicholas Evans'}, {'name': 'Héctor Delgado'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Héctor Delgado'}\n",
      "\n",
      "\n",
      "author\n",
      "Héctor Delgado\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to the ASVspoof 2021 Workshop\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.00537v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.00537v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.00535v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.00535v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-01T15:32:28Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=1, tm_hour=15, tm_min=32, tm_sec=28, tm_wday=2, tm_yday=244, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-09-01T15:32:28Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=1, tm_hour=15, tm_min=32, tm_sec=28, tm_wday=2, tm_yday=244, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "ASVspoof 2021: Automatic Speaker Verification Spoofing and\n",
      "  Countermeasures Challenge Evaluation Plan\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'ASVspoof 2021: Automatic Speaker Verification Spoofing and\\n  Countermeasures Challenge Evaluation Plan'}\n",
      "\n",
      "\n",
      "summary\n",
      "The automatic speaker verification spoofing and countermeasures (ASVspoof)\n",
      "challenge series is a community-led initiative which aims to promote the\n",
      "consideration of spoofing and the development of countermeasures. ASVspoof 2021\n",
      "is the 4th in a series of bi-annual, competitive challenges where the goal is\n",
      "to develop countermeasures capable of discriminating between bona fide and\n",
      "spoofed or deepfake speech. This document provides a technical description of\n",
      "the ASVspoof 2021 challenge, including details of training, development and\n",
      "evaluation data, metrics, baselines, evaluation rules, submission procedures\n",
      "and the schedule.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The automatic speaker verification spoofing and countermeasures (ASVspoof)\\nchallenge series is a community-led initiative which aims to promote the\\nconsideration of spoofing and the development of countermeasures. ASVspoof 2021\\nis the 4th in a series of bi-annual, competitive challenges where the goal is\\nto develop countermeasures capable of discriminating between bona fide and\\nspoofed or deepfake speech. This document provides a technical description of\\nthe ASVspoof 2021 challenge, including details of training, development and\\nevaluation data, metrics, baselines, evaluation rules, submission procedures\\nand the schedule.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Héctor Delgado'}, {'name': 'Nicholas Evans'}, {'name': 'Tomi Kinnunen'}, {'name': 'Kong Aik Lee'}, {'name': 'Xuechen Liu'}, {'name': 'Andreas Nautsch'}, {'name': 'Jose Patino'}, {'name': 'Md Sahidullah'}, {'name': 'Massimiliano Todisco'}, {'name': 'Xin Wang'}, {'name': 'Junichi Yamagishi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Junichi Yamagishi'}\n",
      "\n",
      "\n",
      "author\n",
      "Junichi Yamagishi\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "http://www.asvspoof.org\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.00535v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.00535v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2108.12715v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2108.12715v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-28T22:56:09Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=28, tm_hour=22, tm_min=56, tm_sec=9, tm_wday=5, tm_yday=240, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-08-28T22:56:09Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=28, tm_hour=22, tm_min=56, tm_sec=9, tm_wday=5, tm_yday=240, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFake Detection with Inconsistent Head Poses: Reproducibility and\n",
      "  Analysis\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'DeepFake Detection with Inconsistent Head Poses: Reproducibility and\\n  Analysis'}\n",
      "\n",
      "\n",
      "summary\n",
      "Applications of deep learning to synthetic media generation allow the\n",
      "creation of convincing forgeries, called DeepFakes, with limited technical\n",
      "expertise. DeepFake detection is an increasingly active research area. In this\n",
      "paper, we analyze an existing DeepFake detection technique based on head pose\n",
      "estimation, which can be applied when fake images are generated with an\n",
      "autoencoder-based face swap. Existing literature suggests that this method is\n",
      "an effective DeepFake detector, and its motivating principles are attractively\n",
      "simple. With an eye towards using these principles to develop new DeepFake\n",
      "detectors, we conduct a reproducibility study of the existing method. We\n",
      "conclude that its merits are dramatically overstated, despite its celebrated\n",
      "status. By investigating this discrepancy we uncover a number of important and\n",
      "generalizable insights related to facial landmark detection, identity-agnostic\n",
      "head pose estimation, and algorithmic bias in DeepFake detectors. Our results\n",
      "correct the current literature's perception of state of the art performance for\n",
      "DeepFake detection.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Applications of deep learning to synthetic media generation allow the\\ncreation of convincing forgeries, called DeepFakes, with limited technical\\nexpertise. DeepFake detection is an increasingly active research area. In this\\npaper, we analyze an existing DeepFake detection technique based on head pose\\nestimation, which can be applied when fake images are generated with an\\nautoencoder-based face swap. Existing literature suggests that this method is\\nan effective DeepFake detector, and its motivating principles are attractively\\nsimple. With an eye towards using these principles to develop new DeepFake\\ndetectors, we conduct a reproducibility study of the existing method. We\\nconclude that its merits are dramatically overstated, despite its celebrated\\nstatus. By investigating this discrepancy we uncover a number of important and\\ngeneralizable insights related to facial landmark detection, identity-agnostic\\nhead pose estimation, and algorithmic bias in DeepFake detectors. Our results\\ncorrect the current literature's perception of state of the art performance for\\nDeepFake detection.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Kevin Lutz'}, {'name': 'Robert Bassett'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Robert Bassett'}\n",
      "\n",
      "\n",
      "author\n",
      "Robert Bassett\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2108.12715v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2108.12715v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2108.07949v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2108.07949v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-18T02:37:17Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=18, tm_hour=2, tm_min=37, tm_sec=17, tm_wday=2, tm_yday=230, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-08-18T02:37:17Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=18, tm_hour=2, tm_min=37, tm_sec=17, tm_wday=2, tm_yday=230, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFake MNIST+: A DeepFake Facial Animation Dataset\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'DeepFake MNIST+: A DeepFake Facial Animation Dataset'}\n",
      "\n",
      "\n",
      "summary\n",
      "The DeepFakes, which are the facial manipulation techniques, is the emerging\n",
      "threat to digital society. Various DeepFake detection methods and datasets are\n",
      "proposed for detecting such data, especially for face-swapping. However, recent\n",
      "researches less consider facial animation, which is also important in the\n",
      "DeepFake attack side. It tries to animate a face image with actions provided by\n",
      "a driving video, which also leads to a concern about the security of recent\n",
      "payment systems that reply on liveness detection to authenticate real users via\n",
      "recognising a sequence of user facial actions. However, our experiments show\n",
      "that the existed datasets are not sufficient to develop reliable detection\n",
      "methods. While the current liveness detector cannot defend such videos as the\n",
      "attack. As a response, we propose a new human face animation dataset, called\n",
      "DeepFake MNIST+, generated by a SOTA image animation generator. It includes\n",
      "10,000 facial animation videos in ten different actions, which can spoof the\n",
      "recent liveness detectors. A baseline detection method and a comprehensive\n",
      "analysis of the method is also included in this paper. In addition, we analyze\n",
      "the proposed dataset's properties and reveal the difficulty and importance of\n",
      "detecting animation datasets under different types of motion and compression\n",
      "quality.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"The DeepFakes, which are the facial manipulation techniques, is the emerging\\nthreat to digital society. Various DeepFake detection methods and datasets are\\nproposed for detecting such data, especially for face-swapping. However, recent\\nresearches less consider facial animation, which is also important in the\\nDeepFake attack side. It tries to animate a face image with actions provided by\\na driving video, which also leads to a concern about the security of recent\\npayment systems that reply on liveness detection to authenticate real users via\\nrecognising a sequence of user facial actions. However, our experiments show\\nthat the existed datasets are not sufficient to develop reliable detection\\nmethods. While the current liveness detector cannot defend such videos as the\\nattack. As a response, we propose a new human face animation dataset, called\\nDeepFake MNIST+, generated by a SOTA image animation generator. It includes\\n10,000 facial animation videos in ten different actions, which can spoof the\\nrecent liveness detectors. A baseline detection method and a comprehensive\\nanalysis of the method is also included in this paper. In addition, we analyze\\nthe proposed dataset's properties and reveal the difficulty and importance of\\ndetecting animation datasets under different types of motion and compression\\nquality.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Jiajun Huang'}, {'name': 'Xueyu Wang'}, {'name': 'Bo Du'}, {'name': 'Pei Du'}, {'name': 'Chang Xu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Chang Xu'}\n",
      "\n",
      "\n",
      "author\n",
      "Chang Xu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "14 pages\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2108.07949v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2108.07949v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2109.00911v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2109.00911v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-16T07:56:45Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=16, tm_hour=7, tm_min=56, tm_sec=45, tm_wday=0, tm_yday=228, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-08-16T07:56:45Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=16, tm_hour=7, tm_min=56, tm_sec=45, tm_wday=0, tm_yday=228, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "BiHPF: Bilateral High-Pass Filters for Robust Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'BiHPF: Bilateral High-Pass Filters for Robust Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "The advancement in numerous generative models has a two-fold effect: a simple\n",
      "and easy generation of realistic synthesized images, but also an increased risk\n",
      "of malicious abuse of those images. Thus, it is important to develop a\n",
      "generalized detector for synthesized images of any GAN model or object\n",
      "category, including those unseen during the training phase. However, the\n",
      "conventional methods heavily depend on the training settings, which cause a\n",
      "dramatic decline in performance when tested with unknown domains. To resolve\n",
      "the issue and obtain a generalized detection ability, we propose Bilateral\n",
      "High-Pass Filters (BiHPF), which amplify the effect of the frequency-level\n",
      "artifacts that are known to be found in the synthesized images of generative\n",
      "models. Numerous experimental results validate that our method outperforms\n",
      "other state-of-the-art methods, even when tested with unseen domains.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The advancement in numerous generative models has a two-fold effect: a simple\\nand easy generation of realistic synthesized images, but also an increased risk\\nof malicious abuse of those images. Thus, it is important to develop a\\ngeneralized detector for synthesized images of any GAN model or object\\ncategory, including those unseen during the training phase. However, the\\nconventional methods heavily depend on the training settings, which cause a\\ndramatic decline in performance when tested with unknown domains. To resolve\\nthe issue and obtain a generalized detection ability, we propose Bilateral\\nHigh-Pass Filters (BiHPF), which amplify the effect of the frequency-level\\nartifacts that are known to be found in the synthesized images of generative\\nmodels. Numerous experimental results validate that our method outperforms\\nother state-of-the-art methods, even when tested with unseen domains.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yonghyun Jeong'}, {'name': 'Doyeon Kim'}, {'name': 'Seungjai Min'}, {'name': 'Seongho Joe'}, {'name': 'Youngjune Gwon'}, {'name': 'Jongwon Choi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jongwon Choi'}\n",
      "\n",
      "\n",
      "author\n",
      "Jongwon Choi\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2109.00911v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.00911v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2108.06702v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2108.06702v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-15T09:37:38Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=15, tm_hour=9, tm_min=37, tm_sec=38, tm_wday=6, tm_yday=227, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-08-15T09:37:38Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=15, tm_hour=9, tm_min=37, tm_sec=38, tm_wday=6, tm_yday=227, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Representation with Multilinear Regression\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfake Representation with Multilinear Regression'}\n",
      "\n",
      "\n",
      "summary\n",
      "Generative neural network architectures such as GANs, may be used to generate\n",
      "synthetic instances to compensate for the lack of real data. However, they may\n",
      "be employed to create media that may cause social, political or economical\n",
      "upheaval. One emerging media is \"Deepfake\".Techniques that can discriminate\n",
      "between such media is indispensable. In this paper, we propose a modified\n",
      "multilinear (tensor) method, a combination of linear and multilinear\n",
      "regressions for representing fake and real data. We test our approach by\n",
      "representing Deepfakes with our modified multilinear (tensor) approach and\n",
      "perform SVM classification with encouraging results.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Generative neural network architectures such as GANs, may be used to generate\\nsynthetic instances to compensate for the lack of real data. However, they may\\nbe employed to create media that may cause social, political or economical\\nupheaval. One emerging media is \"Deepfake\".Techniques that can discriminate\\nbetween such media is indispensable. In this paper, we propose a modified\\nmultilinear (tensor) method, a combination of linear and multilinear\\nregressions for representing fake and real data. We test our approach by\\nrepresenting Deepfakes with our modified multilinear (tensor) approach and\\nperform SVM classification with encouraging results.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sara Abdali'}, {'name': 'M. Alex O. Vasilescu'}, {'name': 'Evangelos E. Papalexakis'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Evangelos E. Papalexakis'}\n",
      "\n",
      "\n",
      "author\n",
      "Evangelos E. Papalexakis\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2108.06702v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2108.06702v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2108.05307v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2108.05307v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-11T16:22:56Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=11, tm_hour=16, tm_min=22, tm_sec=56, tm_wday=2, tm_yday=223, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-08-11T16:22:56Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=11, tm_hour=16, tm_min=22, tm_sec=56, tm_wday=2, tm_yday=223, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Video Transformer for Deepfake Detection with Incremental Learning\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Video Transformer for Deepfake Detection with Incremental Learning'}\n",
      "\n",
      "\n",
      "summary\n",
      "Face forgery by deepfake is widely spread over the internet and this raises\n",
      "severe societal concerns. In this paper, we propose a novel video transformer\n",
      "with incremental learning for detecting deepfake videos. To better align the\n",
      "input face images, we use a 3D face reconstruction method to generate UV\n",
      "texture from a single input face image. The aligned face image can also provide\n",
      "pose, eyes blink and mouth movement information that cannot be perceived in the\n",
      "UV texture image, so we use both face images and their UV texture maps to\n",
      "extract the image features. We present an incremental learning strategy to\n",
      "fine-tune the proposed model on a smaller amount of data and achieve better\n",
      "deepfake detection performance. The comprehensive experiments on various public\n",
      "deepfake datasets demonstrate that the proposed video transformer model with\n",
      "incremental learning achieves state-of-the-art performance in the deepfake\n",
      "video detection task with enhanced feature learning from the sequenced data.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Face forgery by deepfake is widely spread over the internet and this raises\\nsevere societal concerns. In this paper, we propose a novel video transformer\\nwith incremental learning for detecting deepfake videos. To better align the\\ninput face images, we use a 3D face reconstruction method to generate UV\\ntexture from a single input face image. The aligned face image can also provide\\npose, eyes blink and mouth movement information that cannot be perceived in the\\nUV texture image, so we use both face images and their UV texture maps to\\nextract the image features. We present an incremental learning strategy to\\nfine-tune the proposed model on a smaller amount of data and achieve better\\ndeepfake detection performance. The comprehensive experiments on various public\\ndeepfake datasets demonstrate that the proposed video transformer model with\\nincremental learning achieves state-of-the-art performance in the deepfake\\nvideo detection task with enhanced feature learning from the sequenced data.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sohail A. Khan'}, {'name': 'Hang Dai'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Hang Dai'}\n",
      "\n",
      "\n",
      "author\n",
      "Hang Dai\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted at ACM International Conference on Multimedia, October 20 to\n",
      "  24, 2021, Virtual Event, China\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2108.05307v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2108.05307v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2108.05080v4\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2108.05080v4\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-01T10:38:07Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=1, tm_hour=10, tm_min=38, tm_sec=7, tm_wday=1, tm_yday=60, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-08-11T07:49:36Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=11, tm_hour=7, tm_min=49, tm_sec=36, tm_wday=2, tm_yday=223, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset'}\n",
      "\n",
      "\n",
      "summary\n",
      "While the significant advancements have made in the generation of deepfakes\n",
      "using deep learning technologies, its misuse is a well-known issue now.\n",
      "Deepfakes can cause severe security and privacy issues as they can be used to\n",
      "impersonate a person's identity in a video by replacing his/her face with\n",
      "another person's face. Recently, a new problem of generating synthesized human\n",
      "voice of a person is emerging, where AI-based deep learning models can\n",
      "synthesize any person's voice requiring just a few seconds of audio. With the\n",
      "emerging threat of impersonation attacks using deepfake audios and videos, a\n",
      "new generation of deepfake detectors is needed to focus on both video and audio\n",
      "collectively. To develop a competent deepfake detector, a large amount of\n",
      "high-quality data is typically required to capture real-world (or practical)\n",
      "scenarios. Existing deepfake datasets either contain deepfake videos or audios,\n",
      "which are racially biased as well. As a result, it is critical to develop a\n",
      "high-quality video and audio deepfake dataset that can be used to detect both\n",
      "audio and video deepfakes simultaneously. To fill this gap, we propose a novel\n",
      "Audio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake\n",
      "videos but also respective synthesized lip-synced fake audios. We generate this\n",
      "dataset using the most popular deepfake generation methods. We selected real\n",
      "YouTube videos of celebrities with four ethnic backgrounds to develop a more\n",
      "realistic multimodal dataset that addresses racial bias, and further help\n",
      "develop multimodal deepfake detectors. We performed several experiments using\n",
      "state-of-the-art detection methods to evaluate our deepfake dataset and\n",
      "demonstrate the challenges and usefulness of our multimodal Audio-Video\n",
      "deepfake dataset.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"While the significant advancements have made in the generation of deepfakes\\nusing deep learning technologies, its misuse is a well-known issue now.\\nDeepfakes can cause severe security and privacy issues as they can be used to\\nimpersonate a person's identity in a video by replacing his/her face with\\nanother person's face. Recently, a new problem of generating synthesized human\\nvoice of a person is emerging, where AI-based deep learning models can\\nsynthesize any person's voice requiring just a few seconds of audio. With the\\nemerging threat of impersonation attacks using deepfake audios and videos, a\\nnew generation of deepfake detectors is needed to focus on both video and audio\\ncollectively. To develop a competent deepfake detector, a large amount of\\nhigh-quality data is typically required to capture real-world (or practical)\\nscenarios. Existing deepfake datasets either contain deepfake videos or audios,\\nwhich are racially biased as well. As a result, it is critical to develop a\\nhigh-quality video and audio deepfake dataset that can be used to detect both\\naudio and video deepfakes simultaneously. To fill this gap, we propose a novel\\nAudio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake\\nvideos but also respective synthesized lip-synced fake audios. We generate this\\ndataset using the most popular deepfake generation methods. We selected real\\nYouTube videos of celebrities with four ethnic backgrounds to develop a more\\nrealistic multimodal dataset that addresses racial bias, and further help\\ndevelop multimodal deepfake detectors. We performed several experiments using\\nstate-of-the-art detection methods to evaluate our deepfake dataset and\\ndemonstrate the challenges and usefulness of our multimodal Audio-Video\\ndeepfake dataset.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hasam Khalid'}, {'name': 'Shahroz Tariq'}, {'name': 'Minha Kim'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Part of Proceedings of the Neural Information Processing Systems\n",
      "  Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2108.05080v4', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2108.05080v4', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.4.9; I.5.4', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2108.01469v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2108.01469v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-02T06:17:25Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=2, tm_hour=6, tm_min=17, tm_sec=25, tm_wday=0, tm_yday=214, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-08-02T06:17:25Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=2, tm_hour=6, tm_min=17, tm_sec=25, tm_wday=0, tm_yday=214, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Creation and Detection of German Voice Deepfakes\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Creation and Detection of German Voice Deepfakes'}\n",
      "\n",
      "\n",
      "summary\n",
      "Synthesizing voice with the help of machine learning techniques has made\n",
      "rapid progress over the last years [1] and first high profile fraud cases have\n",
      "been recently reported [2]. Given the current increase in using conferencing\n",
      "tools for online teaching, we question just how easy (i.e. needed data,\n",
      "hardware, skill set) it would be to create a convincing voice fake. We analyse\n",
      "how much training data a participant (e.g. a student) would actually need to\n",
      "fake another participants voice (e.g. a professor). We provide an analysis of\n",
      "the existing state of the art in creating voice deep fakes, as well as offer\n",
      "detailed technical guidance and evidence of just how much effort is needed to\n",
      "copy a voice. A user study with more than 100 participants shows how difficult\n",
      "it is to identify real and fake voice (on avg. only 37 percent can distinguish\n",
      "between real and fake voice of a professor). With a focus on German language\n",
      "and an online teaching environment we discuss the societal implications as well\n",
      "as demonstrate how to use machine learning techniques to possibly detect such\n",
      "fakes.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Synthesizing voice with the help of machine learning techniques has made\\nrapid progress over the last years [1] and first high profile fraud cases have\\nbeen recently reported [2]. Given the current increase in using conferencing\\ntools for online teaching, we question just how easy (i.e. needed data,\\nhardware, skill set) it would be to create a convincing voice fake. We analyse\\nhow much training data a participant (e.g. a student) would actually need to\\nfake another participants voice (e.g. a professor). We provide an analysis of\\nthe existing state of the art in creating voice deep fakes, as well as offer\\ndetailed technical guidance and evidence of just how much effort is needed to\\ncopy a voice. A user study with more than 100 participants shows how difficult\\nit is to identify real and fake voice (on avg. only 37 percent can distinguish\\nbetween real and fake voice of a professor). With a focus on German language\\nand an online teaching environment we discuss the societal implications as well\\nas demonstrate how to use machine learning techniques to possibly detect such\\nfakes.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Vanessa Barnekow'}, {'name': 'Dominik Binder'}, {'name': 'Niclas Kromrey'}, {'name': 'Pascal Munaretto'}, {'name': 'Andreas Schaad'}, {'name': 'Felix Schmieder'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Felix Schmieder'}\n",
      "\n",
      "\n",
      "author\n",
      "Felix Schmieder\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2108.01469v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2108.01469v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.14480v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.14480v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-07-30T08:15:41Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=30, tm_hour=8, tm_min=15, tm_sec=41, tm_wday=4, tm_yday=211, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-30T08:15:41Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=30, tm_hour=8, tm_min=15, tm_sec=41, tm_wday=4, tm_yday=211, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery\n",
      "  Detection And Segmentation In-The-Wild\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery\\n  Detection And Segmentation In-The-Wild'}\n",
      "\n",
      "\n",
      "summary\n",
      "The proliferation of deepfake media is raising concerns among the public and\n",
      "relevant authorities. It has become essential to develop countermeasures\n",
      "against forged faces in social media. This paper presents a comprehensive study\n",
      "on two new countermeasure tasks: multi-face forgery detection and segmentation\n",
      "in-the-wild. Localizing forged faces among multiple human faces in unrestricted\n",
      "natural scenes is far more challenging than the traditional deepfake\n",
      "recognition task. To promote these new tasks, we have created the first\n",
      "large-scale dataset posing a high level of challenges that is designed with\n",
      "face-wise rich annotations explicitly for face forgery detection and\n",
      "segmentation, namely OpenForensics. With its rich annotations, our\n",
      "OpenForensics dataset has great potentials for research in both deepfake\n",
      "prevention and general human face detection. We have also developed a suite of\n",
      "benchmarks for these tasks by conducting an extensive evaluation of\n",
      "state-of-the-art instance detection and segmentation methods on our newly\n",
      "constructed dataset in various scenarios. The dataset, benchmark results,\n",
      "codes, and supplementary materials will be publicly available on our project\n",
      "page: https://sites.google.com/view/ltnghia/research/openforensics\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The proliferation of deepfake media is raising concerns among the public and\\nrelevant authorities. It has become essential to develop countermeasures\\nagainst forged faces in social media. This paper presents a comprehensive study\\non two new countermeasure tasks: multi-face forgery detection and segmentation\\nin-the-wild. Localizing forged faces among multiple human faces in unrestricted\\nnatural scenes is far more challenging than the traditional deepfake\\nrecognition task. To promote these new tasks, we have created the first\\nlarge-scale dataset posing a high level of challenges that is designed with\\nface-wise rich annotations explicitly for face forgery detection and\\nsegmentation, namely OpenForensics. With its rich annotations, our\\nOpenForensics dataset has great potentials for research in both deepfake\\nprevention and general human face detection. We have also developed a suite of\\nbenchmarks for these tasks by conducting an extensive evaluation of\\nstate-of-the-art instance detection and segmentation methods on our newly\\nconstructed dataset in various scenarios. The dataset, benchmark results,\\ncodes, and supplementary materials will be publicly available on our project\\npage: https://sites.google.com/view/ltnghia/research/openforensics'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Trung-Nghia Le'}, {'name': 'Huy H. Nguyen'}, {'name': 'Junichi Yamagishi'}, {'name': 'Isao Echizen'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Isao Echizen'}\n",
      "\n",
      "\n",
      "author\n",
      "Isao Echizen\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to ICCV 2021. Project page:\n",
      "  https://sites.google.com/view/ltnghia/research/openforensics\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.14480v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.14480v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.12710v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.12710v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-23T17:06:53Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=23, tm_hour=17, tm_min=6, tm_sec=53, tm_wday=0, tm_yday=235, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-27T10:11:41Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=27, tm_hour=10, tm_min=11, tm_sec=41, tm_wday=1, tm_yday=208, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "End-to-End Spectro-Temporal Graph Attention Networks for Speaker\n",
      "  Verification Anti-Spoofing and Speech Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'End-to-End Spectro-Temporal Graph Attention Networks for Speaker\\n  Verification Anti-Spoofing and Speech Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Artefacts that serve to distinguish bona fide speech from spoofed or deepfake\n",
      "speech are known to reside in specific subbands and temporal segments. Various\n",
      "approaches can be used to capture and model such artefacts, however, none works\n",
      "well across a spectrum of diverse spoofing attacks. Reliable detection then\n",
      "often depends upon the fusion of multiple detection systems, each tuned to\n",
      "detect different forms of attack. In this paper we show that better performance\n",
      "can be achieved when the fusion is performed within the model itself and when\n",
      "the representation is learned automatically from raw waveform inputs. The\n",
      "principal contribution is a spectro-temporal graph attention network (GAT)\n",
      "which learns the relationship between cues spanning different sub-bands and\n",
      "temporal intervals. Using a model-level graph fusion of spectral (S) and\n",
      "temporal (T) sub-graphs and a graph pooling strategy to improve discrimination,\n",
      "the proposed RawGAT-ST model achieves an equal error rate of 1.06 % for the\n",
      "ASVspoof 2019 logical access database. This is one of the best results reported\n",
      "to date and is reproducible using an open source implementation.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Artefacts that serve to distinguish bona fide speech from spoofed or deepfake\\nspeech are known to reside in specific subbands and temporal segments. Various\\napproaches can be used to capture and model such artefacts, however, none works\\nwell across a spectrum of diverse spoofing attacks. Reliable detection then\\noften depends upon the fusion of multiple detection systems, each tuned to\\ndetect different forms of attack. In this paper we show that better performance\\ncan be achieved when the fusion is performed within the model itself and when\\nthe representation is learned automatically from raw waveform inputs. The\\nprincipal contribution is a spectro-temporal graph attention network (GAT)\\nwhich learns the relationship between cues spanning different sub-bands and\\ntemporal intervals. Using a model-level graph fusion of spectral (S) and\\ntemporal (T) sub-graphs and a graph pooling strategy to improve discrimination,\\nthe proposed RawGAT-ST model achieves an equal error rate of 1.06 % for the\\nASVspoof 2019 logical access database. This is one of the best results reported\\nto date and is reproducible using an open source implementation.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hemlata Tak'}, {'name': 'Jee-weon Jung'}, {'name': 'Jose Patino'}, {'name': 'Madhu Kamble'}, {'name': 'Massimiliano Todisco'}, {'name': 'Nicholas Evans'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Nicholas Evans'}\n",
      "\n",
      "\n",
      "author\n",
      "Nicholas Evans\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted in ASVspoof 2021 Workshop\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.12710v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.12710v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.12212v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.12212v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-06T14:07:22Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=6, tm_hour=14, tm_min=7, tm_sec=22, tm_wday=2, tm_yday=279, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-26T13:36:14Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=26, tm_hour=13, tm_min=36, tm_sec=14, tm_wday=0, tm_yday=207, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Raw Differentiable Architecture Search for Speech Deepfake and Spoofing\n",
      "  Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Raw Differentiable Architecture Search for Speech Deepfake and Spoofing\\n  Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "End-to-end approaches to anti-spoofing, especially those which operate\n",
      "directly upon the raw signal, are starting to be competitive with their more\n",
      "traditional counterparts. Until recently, all such approaches consider only the\n",
      "learning of network parameters; the network architecture is still hand crafted.\n",
      "This too, however, can also be learned. Described in this paper is our attempt\n",
      "to learn automatically the network architecture of a speech deepfake and\n",
      "spoofing detection solution, while jointly optimising other network components\n",
      "and parameters, such as the first convolutional layer which operates on raw\n",
      "signal inputs. The resulting raw differentiable architecture search system\n",
      "delivers a tandem detection cost function score of 0.0517 for the ASVspoof 2019\n",
      "logical access database, a result which is among the best single-system results\n",
      "reported to date.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'End-to-end approaches to anti-spoofing, especially those which operate\\ndirectly upon the raw signal, are starting to be competitive with their more\\ntraditional counterparts. Until recently, all such approaches consider only the\\nlearning of network parameters; the network architecture is still hand crafted.\\nThis too, however, can also be learned. Described in this paper is our attempt\\nto learn automatically the network architecture of a speech deepfake and\\nspoofing detection solution, while jointly optimising other network components\\nand parameters, such as the first convolutional layer which operates on raw\\nsignal inputs. The resulting raw differentiable architecture search system\\ndelivers a tandem detection cost function score of 0.0517 for the ASVspoof 2019\\nlogical access database, a result which is among the best single-system results\\nreported to date.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Wanying Ge'}, {'name': 'Jose Patino'}, {'name': 'Massimiliano Todisco'}, {'name': 'Nicholas Evans'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Nicholas Evans'}\n",
      "\n",
      "\n",
      "author\n",
      "Nicholas Evans\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to ASVspoof 2021 Workshop\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.12212v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.12212v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.12018v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.12018v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-23T19:42:53Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=23, tm_hour=19, tm_min=42, tm_sec=53, tm_wday=0, tm_yday=235, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-26T08:15:24Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=26, tm_hour=8, tm_min=15, tm_sec=24, tm_wday=0, tm_yday=207, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "UR Channel-Robust Synthetic Speech Detection System for ASVspoof 2021\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'UR Channel-Robust Synthetic Speech Detection System for ASVspoof 2021'}\n",
      "\n",
      "\n",
      "summary\n",
      "In this paper, we present UR-AIR system submission to the logical access (LA)\n",
      "and the speech deepfake (DF) tracks of the ASVspoof 2021 Challenge. The LA and\n",
      "DF tasks focus on synthetic speech detection (SSD), i.e. detecting\n",
      "text-to-speech and voice conversion as spoofing attacks. Different from\n",
      "previous ASVspoof challenges, the LA task this year presents codec and\n",
      "transmission channel variability, while the new task DF presents general audio\n",
      "compression. Built upon our previous research work on improving the robustness\n",
      "of the SSD systems to channel effects, we propose a channel-robust synthetic\n",
      "speech detection system for the challenge. To mitigate the channel variability\n",
      "issue, we use an acoustic simulator to apply transmission codec, compression\n",
      "codec, and convolutional impulse responses to augmenting the original datasets.\n",
      "For the neural network backbone, we propose to use Emphasized Channel\n",
      "Attention, Propagation and Aggregation Time Delay Neural Networks (ECAPA-TDNN)\n",
      "as our primary model. We also incorporate one-class learning with\n",
      "channel-robust training strategies to further learn a channel-invariant speech\n",
      "representation. Our submission achieved EER 20.33% in the DF task; EER 5.46%\n",
      "and min-tDCF 0.3094 in the LA task.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'In this paper, we present UR-AIR system submission to the logical access (LA)\\nand the speech deepfake (DF) tracks of the ASVspoof 2021 Challenge. The LA and\\nDF tasks focus on synthetic speech detection (SSD), i.e. detecting\\ntext-to-speech and voice conversion as spoofing attacks. Different from\\nprevious ASVspoof challenges, the LA task this year presents codec and\\ntransmission channel variability, while the new task DF presents general audio\\ncompression. Built upon our previous research work on improving the robustness\\nof the SSD systems to channel effects, we propose a channel-robust synthetic\\nspeech detection system for the challenge. To mitigate the channel variability\\nissue, we use an acoustic simulator to apply transmission codec, compression\\ncodec, and convolutional impulse responses to augmenting the original datasets.\\nFor the neural network backbone, we propose to use Emphasized Channel\\nAttention, Propagation and Aggregation Time Delay Neural Networks (ECAPA-TDNN)\\nas our primary model. We also incorporate one-class learning with\\nchannel-robust training strategies to further learn a channel-invariant speech\\nrepresentation. Our submission achieved EER 20.33% in the DF task; EER 5.46%\\nand min-tDCF 0.3094 in the LA task.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Xinhui Chen'}, {'name': 'You Zhang'}, {'name': 'Ge Zhu'}, {'name': 'Zhiyao Duan'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Zhiyao Duan'}\n",
      "\n",
      "\n",
      "author\n",
      "Zhiyao Duan\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To appear in Proc. ASVspoof 2021 Workshop\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.12018v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.12018v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.10139v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.10139v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-07-29T05:28:52Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=29, tm_hour=5, tm_min=28, tm_sec=52, tm_wday=3, tm_yday=210, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-21T15:16:10Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=21, tm_hour=15, tm_min=16, tm_sec=10, tm_wday=2, tm_yday=202, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Generative Models for Security: Attacks, Defenses, and Opportunities\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Generative Models for Security: Attacks, Defenses, and Opportunities'}\n",
      "\n",
      "\n",
      "summary\n",
      "Generative models learn the distribution of data from a sample dataset and\n",
      "can then generate new data instances. Recent advances in deep learning has\n",
      "brought forth improvements in generative model architectures, and some\n",
      "state-of-the-art models can (in some cases) produce outputs realistic enough to\n",
      "fool humans.\n",
      "  We survey recent research at the intersection of security and privacy and\n",
      "generative models. In particular, we discuss the use of generative models in\n",
      "adversarial machine learning, in helping automate or enhance existing attacks,\n",
      "and as building blocks for defenses in contexts such as intrusion detection,\n",
      "biometrics spoofing, and malware obfuscation. We also describe the use of\n",
      "generative models in diverse applications such as fairness in machine learning,\n",
      "privacy-preserving data synthesis, and steganography. Finally, we discuss new\n",
      "threats due to generative models: the creation of synthetic media such as\n",
      "deepfakes that can be used for disinformation.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Generative models learn the distribution of data from a sample dataset and\\ncan then generate new data instances. Recent advances in deep learning has\\nbrought forth improvements in generative model architectures, and some\\nstate-of-the-art models can (in some cases) produce outputs realistic enough to\\nfool humans.\\n  We survey recent research at the intersection of security and privacy and\\ngenerative models. In particular, we discuss the use of generative models in\\nadversarial machine learning, in helping automate or enhance existing attacks,\\nand as building blocks for defenses in contexts such as intrusion detection,\\nbiometrics spoofing, and malware obfuscation. We also describe the use of\\ngenerative models in diverse applications such as fairness in machine learning,\\nprivacy-preserving data synthesis, and steganography. Finally, we discuss new\\nthreats due to generative models: the creation of synthetic media such as\\ndeepfakes that can be used for disinformation.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Luke A. Bauer'}, {'name': 'Vincent Bindschaedler'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Vincent Bindschaedler'}\n",
      "\n",
      "\n",
      "author\n",
      "Vincent Bindschaedler\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.10139v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.10139v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.09667v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.09667v3\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-28T12:32:45Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=28, tm_hour=12, tm_min=32, tm_sec=45, tm_wday=0, tm_yday=87, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-20T09:19:42Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=20, tm_hour=9, tm_min=19, tm_sec=42, tm_wday=1, tm_yday=201, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Human Perception of Audio Deepfakes\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Human Perception of Audio Deepfakes'}\n",
      "\n",
      "\n",
      "summary\n",
      "The recent emergence of deepfakes has brought manipulated and generated\n",
      "content to the forefront of machine learning research. Automatic detection of\n",
      "deepfakes has seen many new machine learning techniques, however, human\n",
      "detection capabilities are far less explored. In this paper, we present results\n",
      "from comparing the abilities of humans and machines for detecting audio\n",
      "deepfakes used to imitate someone's voice. For this, we use a web-based\n",
      "application framework formulated as a game. Participants were asked to\n",
      "distinguish between real and fake audio samples. In our experiment, 240 unique\n",
      "users competed against a state-of-the-art AI deepfake detection algorithm for\n",
      "8407 total of rounds of the game. We find that humans and deepfake detection\n",
      "algorithms share similar strengths and weaknesses, both struggling to detect\n",
      "certain types of attacks. This is in contrast to the superhuman performance of\n",
      "AI in many application areas such as object detection or face recognition.\n",
      "Concerning human success factors, we find that IT professionals have no\n",
      "advantage over non-professionals but native speakers have an advantage over\n",
      "non-native speakers. This insight will be helpful when designing future\n",
      "cybersecurity training for humans as well as developing better detection\n",
      "algorithms.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"The recent emergence of deepfakes has brought manipulated and generated\\ncontent to the forefront of machine learning research. Automatic detection of\\ndeepfakes has seen many new machine learning techniques, however, human\\ndetection capabilities are far less explored. In this paper, we present results\\nfrom comparing the abilities of humans and machines for detecting audio\\ndeepfakes used to imitate someone's voice. For this, we use a web-based\\napplication framework formulated as a game. Participants were asked to\\ndistinguish between real and fake audio samples. In our experiment, 240 unique\\nusers competed against a state-of-the-art AI deepfake detection algorithm for\\n8407 total of rounds of the game. We find that humans and deepfake detection\\nalgorithms share similar strengths and weaknesses, both struggling to detect\\ncertain types of attacks. This is in contrast to the superhuman performance of\\nAI in many application areas such as object detection or face recognition.\\nConcerning human success factors, we find that IT professionals have no\\nadvantage over non-professionals but native speakers have an advantage over\\nnon-native speakers. This insight will be helpful when designing future\\ncybersecurity training for humans as well as developing better detection\\nalgorithms.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Nicolas M. Müller'}, {'name': 'Karla Markert'}, {'name': 'Jennifer Williams'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jennifer Williams'}\n",
      "\n",
      "\n",
      "author\n",
      "Jennifer Williams\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Submitted to Interspeech 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.09667v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.09667v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.05297v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.05297v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-07-12T10:14:45Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=12, tm_hour=10, tm_min=14, tm_sec=45, tm_wday=0, tm_yday=193, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-12T10:14:45Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=12, tm_hour=10, tm_min=14, tm_sec=45, tm_wday=0, tm_yday=193, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "MMSys'21 Grand Challenge on Detecting Cheapfakes\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"MMSys'21 Grand Challenge on Detecting Cheapfakes\"}\n",
      "\n",
      "\n",
      "summary\n",
      "Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\n",
      "manipulations of multimedia content. Cheapfakes are known to be more prevalent\n",
      "than deepfakes. Cheapfake media can be created using editing software for\n",
      "image/video manipulations, or even without using any software, by simply\n",
      "altering the context of an image/video by sharing the media alongside\n",
      "misleading claims. This alteration of context is referred to as out-of-context\n",
      "(OOC) misuse} of media. OOC media is much harder to detect than fake media,\n",
      "since the images and videos are not tampered. In this challenge, we focus on\n",
      "detecting OOC images, and more specifically the misuse of real photographs with\n",
      "conflicting image captions in news items. The aim of this challenge is to\n",
      "develop and benchmark models that can be used to detect whether given samples\n",
      "(news image and associated captions) are OOC, based on the recently compiled\n",
      "COSMOS dataset.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\\nmanipulations of multimedia content. Cheapfakes are known to be more prevalent\\nthan deepfakes. Cheapfake media can be created using editing software for\\nimage/video manipulations, or even without using any software, by simply\\naltering the context of an image/video by sharing the media alongside\\nmisleading claims. This alteration of context is referred to as out-of-context\\n(OOC) misuse} of media. OOC media is much harder to detect than fake media,\\nsince the images and videos are not tampered. In this challenge, we focus on\\ndetecting OOC images, and more specifically the misuse of real photographs with\\nconflicting image captions in news items. The aim of this challenge is to\\ndevelop and benchmark models that can be used to detect whether given samples\\n(news image and associated captions) are OOC, based on the recently compiled\\nCOSMOS dataset.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shivangi Aneja'}, {'name': 'Cise Midoglu'}, {'name': 'Duc-Tien Dang-Nguyen'}, {'name': 'Michael Alexander Riegler'}, {'name': 'Paal Halvorsen'}, {'name': 'Matthias Niessner'}, {'name': 'Balu Adsumilli'}, {'name': 'Chris Bregler'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Chris Bregler'}\n",
      "\n",
      "\n",
      "author\n",
      "Chris Bregler\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.05297v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.05297v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.02612v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.02612v2\n",
      "\n",
      "\n",
      "updated\n",
      "2022-01-20T14:35:11Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=1, tm_mday=20, tm_hour=14, tm_min=35, tm_sec=11, tm_wday=3, tm_yday=20, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-06T13:35:11Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=6, tm_hour=13, tm_min=35, tm_sec=11, tm_wday=1, tm_yday=187, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Combining EfficientNet and Vision Transformers for Video Deepfake\n",
      "  Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Combining EfficientNet and Vision Transformers for Video Deepfake\\n  Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes are the result of digital manipulation to forge realistic yet fake\n",
      "imagery. With the astonishing advances in deep generative models, fake images\n",
      "or videos are nowadays obtained using variational autoencoders (VAEs) or\n",
      "Generative Adversarial Networks (GANs). These technologies are becoming more\n",
      "accessible and accurate, resulting in fake videos that are very difficult to be\n",
      "detected. Traditionally, Convolutional Neural Networks (CNNs) have been used to\n",
      "perform video deepfake detection, with the best results obtained using methods\n",
      "based on EfficientNet B7. In this study, we focus on video deep fake detection\n",
      "on faces, given that most methods are becoming extremely accurate in the\n",
      "generation of realistic human faces. Specifically, we combine various types of\n",
      "Vision Transformers with a convolutional EfficientNet B0 used as a feature\n",
      "extractor, obtaining comparable results with some very recent methods that use\n",
      "Vision Transformers. Differently from the state-of-the-art approaches, we use\n",
      "neither distillation nor ensemble methods. Furthermore, we present a\n",
      "straightforward inference procedure based on a simple voting scheme for\n",
      "handling multiple faces in the same video shot. The best model achieved an AUC\n",
      "of 0.951 and an F1 score of 88.0%, very close to the state-of-the-art on the\n",
      "DeepFake Detection Challenge (DFDC).\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfakes are the result of digital manipulation to forge realistic yet fake\\nimagery. With the astonishing advances in deep generative models, fake images\\nor videos are nowadays obtained using variational autoencoders (VAEs) or\\nGenerative Adversarial Networks (GANs). These technologies are becoming more\\naccessible and accurate, resulting in fake videos that are very difficult to be\\ndetected. Traditionally, Convolutional Neural Networks (CNNs) have been used to\\nperform video deepfake detection, with the best results obtained using methods\\nbased on EfficientNet B7. In this study, we focus on video deep fake detection\\non faces, given that most methods are becoming extremely accurate in the\\ngeneration of realistic human faces. Specifically, we combine various types of\\nVision Transformers with a convolutional EfficientNet B0 used as a feature\\nextractor, obtaining comparable results with some very recent methods that use\\nVision Transformers. Differently from the state-of-the-art approaches, we use\\nneither distillation nor ensemble methods. Furthermore, we present a\\nstraightforward inference procedure based on a simple voting scheme for\\nhandling multiple faces in the same video shot. The best model achieved an AUC\\nof 0.951 and an F1 score of 88.0%, very close to the state-of-the-art on the\\nDeepFake Detection Challenge (DFDC).'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Davide Coccomini'}, {'name': 'Nicola Messina'}, {'name': 'Claudio Gennaro'}, {'name': 'Fabrizio Falchi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Fabrizio Falchi'}\n",
      "\n",
      "\n",
      "author\n",
      "Fabrizio Falchi\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.02612v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.02612v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.02408v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.02408v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-05T05:53:57Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=5, tm_hour=5, tm_min=53, tm_sec=57, tm_wday=3, tm_yday=217, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-06T06:07:17Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=6, tm_hour=6, tm_min=7, tm_sec=17, tm_wday=1, tm_yday=187, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "CoReD: Generalizing Fake Media Detection with Continual Representation\n",
      "  using Distillation\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'CoReD: Generalizing Fake Media Detection with Continual Representation\\n  using Distillation'}\n",
      "\n",
      "\n",
      "summary\n",
      "Over the last few decades, artificial intelligence research has made\n",
      "tremendous strides, but it still heavily relies on fixed datasets in stationary\n",
      "environments. Continual learning is a growing field of research that examines\n",
      "how AI systems can learn sequentially from a continuous stream of linked data\n",
      "in the same way that biological systems do. Simultaneously, fake media such as\n",
      "deepfakes and synthetic face images have emerged as significant to current\n",
      "multimedia technologies. Recently, numerous method has been proposed which can\n",
      "detect deepfakes with high accuracy. However, they suffer significantly due to\n",
      "their reliance on fixed datasets in limited evaluation settings. Therefore, in\n",
      "this work, we apply continuous learning to neural networks' learning dynamics,\n",
      "emphasizing its potential to increase data efficiency significantly. We propose\n",
      "Continual Representation using Distillation (CoReD) method that employs the\n",
      "concept of Continual Learning (CL), Representation Learning (RL), and Knowledge\n",
      "Distillation (KD). We design CoReD to perform sequential domain adaptation\n",
      "tasks on new deepfake and GAN-generated synthetic face datasets, while\n",
      "effectively minimizing the catastrophic forgetting in a teacher-student model\n",
      "setting. Our extensive experimental results demonstrate that our method is\n",
      "efficient at domain adaptation to detect low-quality deepfakes videos and\n",
      "GAN-generated images from several datasets, outperforming the-state-of-art\n",
      "baseline methods.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Over the last few decades, artificial intelligence research has made\\ntremendous strides, but it still heavily relies on fixed datasets in stationary\\nenvironments. Continual learning is a growing field of research that examines\\nhow AI systems can learn sequentially from a continuous stream of linked data\\nin the same way that biological systems do. Simultaneously, fake media such as\\ndeepfakes and synthetic face images have emerged as significant to current\\nmultimedia technologies. Recently, numerous method has been proposed which can\\ndetect deepfakes with high accuracy. However, they suffer significantly due to\\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\\nthis work, we apply continuous learning to neural networks' learning dynamics,\\nemphasizing its potential to increase data efficiency significantly. We propose\\nContinual Representation using Distillation (CoReD) method that employs the\\nconcept of Continual Learning (CL), Representation Learning (RL), and Knowledge\\nDistillation (KD). We design CoReD to perform sequential domain adaptation\\ntasks on new deepfake and GAN-generated synthetic face datasets, while\\neffectively minimizing the catastrophic forgetting in a teacher-student model\\nsetting. Our extensive experimental results demonstrate that our method is\\nefficient at domain adaptation to detect low-quality deepfakes videos and\\nGAN-generated images from several datasets, outperforming the-state-of-art\\nbaseline methods.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Minha Kim'}, {'name': 'Shahroz Tariq'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1145/3474085.3475535\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3474085.3475535', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2107.02408v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.02408v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "13 pages, 7 Figures, 13 Tables, Accepted for publication in the 29th\n",
      "  ACM International Conference on Multimedia (ACMMM '21)\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.4.9; I.5.4', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.02045v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.02045v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-06T18:46:22Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=6, tm_hour=18, tm_min=46, tm_sec=22, tm_wday=2, tm_yday=279, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-05T14:18:21Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=5, tm_hour=14, tm_min=18, tm_sec=21, tm_wday=0, tm_yday=186, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Understanding the Security of Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Understanding the Security of Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes pose growing challenges to the trust of information on the\n",
      "Internet. Thus, detecting deepfakes has attracted increasing attentions from\n",
      "both academia and industry. State-of-the-art deepfake detection methods consist\n",
      "of two key components, i.e., face extractor and face classifier, which extract\n",
      "the face region in an image and classify it to be real/fake, respectively.\n",
      "Existing studies mainly focused on improving the detection performance in\n",
      "non-adversarial settings, leaving security of deepfake detection in adversarial\n",
      "settings largely unexplored. In this work, we aim to bridge the gap. In\n",
      "particular, we perform a systematic measurement study to understand the\n",
      "security of the state-of-the-art deepfake detection methods in adversarial\n",
      "settings. We use two large-scale public deepfakes data sources including\n",
      "FaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes\n",
      "are fake face images; and we train state-of-the-art deepfake detection methods.\n",
      "These detection methods can achieve 0.94--0.99 accuracies in non-adversarial\n",
      "settings on these datasets. However, our measurement results uncover multiple\n",
      "security limitations of the deepfake detection methods in adversarial settings.\n",
      "First, we find that an attacker can evade a face extractor, i.e., the face\n",
      "extractor fails to extract the correct face regions, via adding small Gaussian\n",
      "noise to its deepfake images. Second, we find that a face classifier trained\n",
      "using deepfakes generated by one method cannot detect deepfakes generated by\n",
      "another method, i.e., an attacker can evade detection via generating deepfakes\n",
      "using a new method. Third, we find that an attacker can leverage backdoor\n",
      "attacks developed by the adversarial machine learning community to evade a face\n",
      "classifier. Our results highlight that deepfake detection should consider the\n",
      "adversarial nature of the problem.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfakes pose growing challenges to the trust of information on the\\nInternet. Thus, detecting deepfakes has attracted increasing attentions from\\nboth academia and industry. State-of-the-art deepfake detection methods consist\\nof two key components, i.e., face extractor and face classifier, which extract\\nthe face region in an image and classify it to be real/fake, respectively.\\nExisting studies mainly focused on improving the detection performance in\\nnon-adversarial settings, leaving security of deepfake detection in adversarial\\nsettings largely unexplored. In this work, we aim to bridge the gap. In\\nparticular, we perform a systematic measurement study to understand the\\nsecurity of the state-of-the-art deepfake detection methods in adversarial\\nsettings. We use two large-scale public deepfakes data sources including\\nFaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes\\nare fake face images; and we train state-of-the-art deepfake detection methods.\\nThese detection methods can achieve 0.94--0.99 accuracies in non-adversarial\\nsettings on these datasets. However, our measurement results uncover multiple\\nsecurity limitations of the deepfake detection methods in adversarial settings.\\nFirst, we find that an attacker can evade a face extractor, i.e., the face\\nextractor fails to extract the correct face regions, via adding small Gaussian\\nnoise to its deepfake images. Second, we find that a face classifier trained\\nusing deepfakes generated by one method cannot detect deepfakes generated by\\nanother method, i.e., an attacker can evade detection via generating deepfakes\\nusing a new method. Third, we find that an attacker can leverage backdoor\\nattacks developed by the adversarial machine learning community to evade a face\\nclassifier. Our results highlight that deepfake detection should consider the\\nadversarial nature of the problem.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Xiaoyu Cao'}, {'name': 'Neil Zhenqiang Gong'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Neil Zhenqiang Gong'}\n",
      "\n",
      "\n",
      "author\n",
      "Neil Zhenqiang Gong\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To appear in ICDF2C 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.02045v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.02045v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2107.02016v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2107.02016v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-26T07:42:41Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=26, tm_hour=7, tm_min=42, tm_sec=41, tm_wday=3, tm_yday=238, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-07-05T13:35:39Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=5, tm_hour=13, tm_min=35, tm_sec=39, tm_wday=0, tm_yday=186, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FFR_FD: Effective and Fast Detection of DeepFakes Based on Feature Point\n",
      "  Defects\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'FFR_FD: Effective and Fast Detection of DeepFakes Based on Feature Point\\n  Defects'}\n",
      "\n",
      "\n",
      "summary\n",
      "The internet is filled with fake face images and videos synthesized by deep\n",
      "generative models. These realistic DeepFakes pose a challenge to determine the\n",
      "authenticity of multimedia content. As countermeasures, artifact-based\n",
      "detection methods suffer from insufficiently fine-grained features that lead to\n",
      "limited detection performance. DNN-based detection methods are not efficient\n",
      "enough, given that a DeepFake can be created easily by mobile apps and\n",
      "DNN-based models require high computational resources. For the first time, we\n",
      "show that DeepFake faces have fewer feature points than real ones, especially\n",
      "in certain facial regions. Inspired by feature point detector-descriptors to\n",
      "extract discriminative features at the pixel level, we propose the Fused Facial\n",
      "Region_Feature Descriptor (FFR_FD) for effective and fast DeepFake detection.\n",
      "FFR_FD is only a vector extracted from the face, and it can be constructed from\n",
      "any feature point detector-descriptors. We train a random forest classifier\n",
      "with FFR_FD and conduct extensive experiments on six large-scale DeepFake\n",
      "datasets, whose results demonstrate that our method is superior to most state\n",
      "of the art DNN-based models.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The internet is filled with fake face images and videos synthesized by deep\\ngenerative models. These realistic DeepFakes pose a challenge to determine the\\nauthenticity of multimedia content. As countermeasures, artifact-based\\ndetection methods suffer from insufficiently fine-grained features that lead to\\nlimited detection performance. DNN-based detection methods are not efficient\\nenough, given that a DeepFake can be created easily by mobile apps and\\nDNN-based models require high computational resources. For the first time, we\\nshow that DeepFake faces have fewer feature points than real ones, especially\\nin certain facial regions. Inspired by feature point detector-descriptors to\\nextract discriminative features at the pixel level, we propose the Fused Facial\\nRegion_Feature Descriptor (FFR_FD) for effective and fast DeepFake detection.\\nFFR_FD is only a vector extracted from the face, and it can be constructed from\\nany feature point detector-descriptors. We train a random forest classifier\\nwith FFR_FD and conduct extensive experiments on six large-scale DeepFake\\ndatasets, whose results demonstrate that our method is superior to most state\\nof the art DNN-based models.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Gaojian Wang'}, {'name': 'Qian Jiang'}, {'name': 'Xin Jin'}, {'name': 'Xiaohui Cui'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Xiaohui Cui'}\n",
      "\n",
      "\n",
      "author\n",
      "Xiaohui Cui\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "14 pages, 11 figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2107.02016v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.02016v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.4; I.5', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2106.12832v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2106.12832v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-06-24T08:33:32Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=24, tm_hour=8, tm_min=33, tm_sec=32, tm_wday=3, tm_yday=175, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-06-24T08:33:32Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=24, tm_hour=8, tm_min=33, tm_sec=32, tm_wday=3, tm_yday=175, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Detection of Deepfake Videos Using Long Distance Attention\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Detection of Deepfake Videos Using Long Distance Attention'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the rapid progress of deepfake techniques in recent years, facial video\n",
      "forgery can generate highly deceptive video contents and bring severe security\n",
      "threats. And detection of such forgery videos is much more urgent and\n",
      "challenging. Most existing detection methods treat the problem as a vanilla\n",
      "binary classification problem. In this paper, the problem is treated as a\n",
      "special fine-grained classification problem since the differences between fake\n",
      "and real faces are very subtle. It is observed that most existing face forgery\n",
      "methods left some common artifacts in the spatial domain and time domain,\n",
      "including generative defects in the spatial domain and inter-frame\n",
      "inconsistencies in the time domain. And a spatial-temporal model is proposed\n",
      "which has two components for capturing spatial and temporal forgery traces in\n",
      "global perspective respectively. The two components are designed using a novel\n",
      "long distance attention mechanism. The one component of the spatial domain is\n",
      "used to capture artifacts in a single frame, and the other component of the\n",
      "time domain is used to capture artifacts in consecutive frames. They generate\n",
      "attention maps in the form of patches. The attention method has a broader\n",
      "vision which contributes to better assembling global information and extracting\n",
      "local statistic information. Finally, the attention maps are used to guide the\n",
      "network to focus on pivotal parts of the face, just like other fine-grained\n",
      "classification methods. The experimental results on different public datasets\n",
      "demonstrate that the proposed method achieves the state-of-the-art performance,\n",
      "and the proposed long distance attention method can effectively capture pivotal\n",
      "parts for face forgery.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'With the rapid progress of deepfake techniques in recent years, facial video\\nforgery can generate highly deceptive video contents and bring severe security\\nthreats. And detection of such forgery videos is much more urgent and\\nchallenging. Most existing detection methods treat the problem as a vanilla\\nbinary classification problem. In this paper, the problem is treated as a\\nspecial fine-grained classification problem since the differences between fake\\nand real faces are very subtle. It is observed that most existing face forgery\\nmethods left some common artifacts in the spatial domain and time domain,\\nincluding generative defects in the spatial domain and inter-frame\\ninconsistencies in the time domain. And a spatial-temporal model is proposed\\nwhich has two components for capturing spatial and temporal forgery traces in\\nglobal perspective respectively. The two components are designed using a novel\\nlong distance attention mechanism. The one component of the spatial domain is\\nused to capture artifacts in a single frame, and the other component of the\\ntime domain is used to capture artifacts in consecutive frames. They generate\\nattention maps in the form of patches. The attention method has a broader\\nvision which contributes to better assembling global information and extracting\\nlocal statistic information. Finally, the attention maps are used to guide the\\nnetwork to focus on pivotal parts of the face, just like other fine-grained\\nclassification methods. The experimental results on different public datasets\\ndemonstrate that the proposed method achieves the state-of-the-art performance,\\nand the proposed long distance attention method can effectively capture pivotal\\nparts for face forgery.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Wei Lu'}, {'name': 'Lingyi Liu'}, {'name': 'Junwei Luo'}, {'name': 'Xianfeng Zhao'}, {'name': 'Yicong Zhou'}, {'name': 'Jiwu Huang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jiwu Huang'}\n",
      "\n",
      "\n",
      "author\n",
      "Jiwu Huang\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2106.12832v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2106.12832v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2106.12223v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2106.12223v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-06-23T08:08:59Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=23, tm_hour=8, tm_min=8, tm_sec=59, tm_wday=2, tm_yday=174, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-06-23T08:08:59Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=23, tm_hour=8, tm_min=8, tm_sec=59, tm_wday=2, tm_yday=174, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Reporting Revenge Porn: a Preliminary Expert Analysis\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Reporting Revenge Porn: a Preliminary Expert Analysis'}\n",
      "\n",
      "\n",
      "summary\n",
      "In our research, we focus on the response to the non-consensual distribution\n",
      "of intimate or sexually explicit digital images of adults, also referred as\n",
      "revenge porn, from the point of view of the victims. In this paper, we present\n",
      "a preliminary expert analysis of the process for reporting revenge porn abuses\n",
      "in selected content sharing platforms. Among these, we included social\n",
      "networks, image hosting websites, video hosting platforms, forums, and\n",
      "pornographic sites. We looked at the way to report abuse, concerning both the\n",
      "non-consensual online distribution of private sexual image or video (revenge\n",
      "pornography), as well as the use of deepfake techniques, where the face of a\n",
      "person can be replaced on original visual content with the aim of portraying\n",
      "the victim in the context of sexual behaviours. This preliminary analysis is\n",
      "directed to understand the current practices and potential issues in the\n",
      "procedures designed by the providers for reporting these abuses.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'In our research, we focus on the response to the non-consensual distribution\\nof intimate or sexually explicit digital images of adults, also referred as\\nrevenge porn, from the point of view of the victims. In this paper, we present\\na preliminary expert analysis of the process for reporting revenge porn abuses\\nin selected content sharing platforms. Among these, we included social\\nnetworks, image hosting websites, video hosting platforms, forums, and\\npornographic sites. We looked at the way to report abuse, concerning both the\\nnon-consensual online distribution of private sexual image or video (revenge\\npornography), as well as the use of deepfake techniques, where the face of a\\nperson can be replaced on original visual content with the aim of portraying\\nthe victim in the context of sexual behaviours. This preliminary analysis is\\ndirected to understand the current practices and potential issues in the\\nprocedures designed by the providers for reporting these abuses.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'A. De Angeli'}, {'name': 'M. Falduti'}, {'name': 'M. Menendez Blanco'}, {'name': 'S. Tessaris'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'S. Tessaris'}\n",
      "\n",
      "\n",
      "author\n",
      "S. Tessaris\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1145/3464385.3464739\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3464385.3464739', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2106.12223v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2106.12223v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2106.10705v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2106.10705v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-12T13:21:30Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=12, tm_hour=13, tm_min=21, tm_sec=30, tm_wday=3, tm_yday=224, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-06-20T14:48:50Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=20, tm_hour=14, tm_min=48, tm_sec=50, tm_wday=6, tm_yday=171, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Automated Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Automated Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "In this paper, we propose to utilize Automated Machine Learning to adaptively\n",
      "search a neural architecture for deepfake detection. This is the first time to\n",
      "employ automated machine learning for deepfake detection. Based on our explored\n",
      "search space, our proposed method achieves competitive prediction accuracy\n",
      "compared to previous methods. To improve the generalizability of our method,\n",
      "especially when training data and testing data are manipulated by different\n",
      "methods, we propose a simple yet effective strategy in our network learning\n",
      "process: making it to estimate potential manipulation regions besides\n",
      "predicting the real/fake labels. Unlike previous works manually design neural\n",
      "networks, our method can relieve us from the high labor cost in network\n",
      "construction. More than that, compared to previous works, our method depends\n",
      "much less on prior knowledge, e.g., which manipulation method is utilized or\n",
      "where exactly the fake image is manipulated. Extensive experimental results on\n",
      "two benchmark datasets demonstrate the effectiveness of our proposed method for\n",
      "deepfake detection.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'In this paper, we propose to utilize Automated Machine Learning to adaptively\\nsearch a neural architecture for deepfake detection. This is the first time to\\nemploy automated machine learning for deepfake detection. Based on our explored\\nsearch space, our proposed method achieves competitive prediction accuracy\\ncompared to previous methods. To improve the generalizability of our method,\\nespecially when training data and testing data are manipulated by different\\nmethods, we propose a simple yet effective strategy in our network learning\\nprocess: making it to estimate potential manipulation regions besides\\npredicting the real/fake labels. Unlike previous works manually design neural\\nnetworks, our method can relieve us from the high labor cost in network\\nconstruction. More than that, compared to previous works, our method depends\\nmuch less on prior knowledge, e.g., which manipulation method is utilized or\\nwhere exactly the fake image is manipulated. Extensive experimental results on\\ntwo benchmark datasets demonstrate the effectiveness of our proposed method for\\ndeepfake detection.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ping Liu'}, {'name': 'Yuewei Lin'}, {'name': 'Yang He'}, {'name': 'Yunchao Wei'}, {'name': 'Liangli Zhen'}, {'name': 'Joey Tianyi Zhou'}, {'name': 'Rick Siow Mong Goh'}, {'name': 'Jingen Liu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jingen Liu'}\n",
      "\n",
      "\n",
      "author\n",
      "Jingen Liu\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2106.10705v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2106.10705v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2106.09369v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2106.09369v3\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-17T10:11:52Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=10, tm_min=11, tm_sec=52, tm_wday=3, tm_yday=76, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-06-17T10:41:44Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=17, tm_hour=10, tm_min=41, tm_sec=44, tm_wday=3, tm_yday=168, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Wavelet-Packets for Deepfake Image Analysis and Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Wavelet-Packets for Deepfake Image Analysis and Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "As neural networks become able to generate realistic artificial images, they\n",
      "have the potential to improve movies, music, video games and make the internet\n",
      "an even more creative and inspiring place. Yet, the latest technology\n",
      "potentially enables new digital ways to lie. In response, the need for a\n",
      "diverse and reliable method toolbox arises to identify artificial images and\n",
      "other content. Previous work primarily relies on pixel-space CNNs or the\n",
      "Fourier transform. To the best of our knowledge, synthesized fake image\n",
      "analysis and detection methods based on a multi-scale wavelet representation,\n",
      "localized in both space and frequency, have been absent thus far. The wavelet\n",
      "transform conserves spatial information to a degree, which allows us to present\n",
      "a new analysis. Comparing the wavelet coefficients of real and fake images\n",
      "allows interpretation. Significant differences are identified. Additionally,\n",
      "this paper proposes to learn a model for the detection of synthetic images\n",
      "based on the wavelet-packet representation of natural and GAN-generated images.\n",
      "Our lightweight forensic classifiers exhibit competitive or improved\n",
      "performance at comparatively small network sizes, as we demonstrate on the\n",
      "FFHQ, CelebA and LSUN source identification problems. Furthermore, we study the\n",
      "binary FaceForensics++ fake-detection problem.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'As neural networks become able to generate realistic artificial images, they\\nhave the potential to improve movies, music, video games and make the internet\\nan even more creative and inspiring place. Yet, the latest technology\\npotentially enables new digital ways to lie. In response, the need for a\\ndiverse and reliable method toolbox arises to identify artificial images and\\nother content. Previous work primarily relies on pixel-space CNNs or the\\nFourier transform. To the best of our knowledge, synthesized fake image\\nanalysis and detection methods based on a multi-scale wavelet representation,\\nlocalized in both space and frequency, have been absent thus far. The wavelet\\ntransform conserves spatial information to a degree, which allows us to present\\na new analysis. Comparing the wavelet coefficients of real and fake images\\nallows interpretation. Significant differences are identified. Additionally,\\nthis paper proposes to learn a model for the detection of synthetic images\\nbased on the wavelet-packet representation of natural and GAN-generated images.\\nOur lightweight forensic classifiers exhibit competitive or improved\\nperformance at comparatively small network sizes, as we demonstrate on the\\nFFHQ, CelebA and LSUN source identification problems. Furthermore, we study the\\nbinary FaceForensics++ fake-detection problem.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Moritz Wolter'}, {'name': 'Felix Blanke'}, {'name': 'Raoul Heese'}, {'name': 'Jochen Garcke'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jochen Garcke'}\n",
      "\n",
      "\n",
      "author\n",
      "Jochen Garcke\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Source code is available at\n",
      "  https://github.com/gan-police/frequency-forensics and\n",
      "  https://github.com/v0lta/PyTorch-Wavelet-Toolbox\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2106.09369v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2106.09369v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2106.07873v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2106.07873v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-06-15T04:19:26Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=15, tm_hour=4, tm_min=19, tm_sec=26, tm_wday=1, tm_yday=166, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-06-15T04:19:26Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=15, tm_hour=4, tm_min=19, tm_sec=26, tm_wday=1, tm_yday=166, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Reverse Engineering of Generative Models: Inferring Model\n",
      "  Hyperparameters from Generated Images\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Reverse Engineering of Generative Models: Inferring Model\\n  Hyperparameters from Generated Images'}\n",
      "\n",
      "\n",
      "summary\n",
      "State-of-the-art (SOTA) Generative Models (GMs) can synthesize\n",
      "photo-realistic images that are hard for humans to distinguish from genuine\n",
      "photos. We propose to perform reverse engineering of GMs to infer the model\n",
      "hyperparameters from the images generated by these models. We define a novel\n",
      "problem, \"model parsing\", as estimating GM network architectures and training\n",
      "loss functions by examining their generated images -- a task seemingly\n",
      "impossible for human beings. To tackle this problem, we propose a framework\n",
      "with two components: a Fingerprint Estimation Network (FEN), which estimates a\n",
      "GM fingerprint from a generated image by training with four constraints to\n",
      "encourage the fingerprint to have desired properties, and a Parsing Network\n",
      "(PN), which predicts network architecture and loss functions from the estimated\n",
      "fingerprints. To evaluate our approach, we collect a fake image dataset with\n",
      "$100$K images generated by $100$ GMs. Extensive experiments show encouraging\n",
      "results in parsing the hyperparameters of the unseen models. Finally, our\n",
      "fingerprint estimation can be leveraged for deepfake detection and image\n",
      "attribution, as we show by reporting SOTA results on both the recent Celeb-DF\n",
      "and image attribution benchmarks.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'State-of-the-art (SOTA) Generative Models (GMs) can synthesize\\nphoto-realistic images that are hard for humans to distinguish from genuine\\nphotos. We propose to perform reverse engineering of GMs to infer the model\\nhyperparameters from the images generated by these models. We define a novel\\nproblem, \"model parsing\", as estimating GM network architectures and training\\nloss functions by examining their generated images -- a task seemingly\\nimpossible for human beings. To tackle this problem, we propose a framework\\nwith two components: a Fingerprint Estimation Network (FEN), which estimates a\\nGM fingerprint from a generated image by training with four constraints to\\nencourage the fingerprint to have desired properties, and a Parsing Network\\n(PN), which predicts network architecture and loss functions from the estimated\\nfingerprints. To evaluate our approach, we collect a fake image dataset with\\n$100$K images generated by $100$ GMs. Extensive experiments show encouraging\\nresults in parsing the hyperparameters of the unseen models. Finally, our\\nfingerprint estimation can be leveraged for deepfake detection and image\\nattribution, as we show by reporting SOTA results on both the recent Celeb-DF\\nand image attribution benchmarks.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Vishal Asnani'}, {'name': 'Xi Yin'}, {'name': 'Tal Hassner'}, {'name': 'Xiaoming Liu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Xiaoming Liu'}\n",
      "\n",
      "\n",
      "author\n",
      "Xiaoming Liu\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2106.07873v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2106.07873v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2106.01615v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2106.01615v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-06-03T06:25:04Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=3, tm_hour=6, tm_min=25, tm_sec=4, tm_wday=3, tm_yday=154, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-06-03T06:25:04Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=3, tm_hour=6, tm_min=25, tm_sec=4, tm_wday=3, tm_yday=154, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Imperceptible Adversarial Examples for Fake Image Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Imperceptible Adversarial Examples for Fake Image Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Fooling people with highly realistic fake images generated with Deepfake or\n",
      "GANs brings a great social disturbance to our society. Many methods have been\n",
      "proposed to detect fake images, but they are vulnerable to adversarial\n",
      "perturbations -- intentionally designed noises that can lead to the wrong\n",
      "prediction. Existing methods of attacking fake image detectors usually generate\n",
      "adversarial perturbations to perturb almost the entire image. This is redundant\n",
      "and increases the perceptibility of perturbations. In this paper, we propose a\n",
      "novel method to disrupt the fake image detection by determining key pixels to a\n",
      "fake image detector and attacking only the key pixels, which results in the\n",
      "$L_0$ and the $L_2$ norms of adversarial perturbations much less than those of\n",
      "existing works. Experiments on two public datasets with three fake image\n",
      "detectors indicate that our proposed method achieves state-of-the-art\n",
      "performance in both white-box and black-box attacks.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Fooling people with highly realistic fake images generated with Deepfake or\\nGANs brings a great social disturbance to our society. Many methods have been\\nproposed to detect fake images, but they are vulnerable to adversarial\\nperturbations -- intentionally designed noises that can lead to the wrong\\nprediction. Existing methods of attacking fake image detectors usually generate\\nadversarial perturbations to perturb almost the entire image. This is redundant\\nand increases the perceptibility of perturbations. In this paper, we propose a\\nnovel method to disrupt the fake image detection by determining key pixels to a\\nfake image detector and attacking only the key pixels, which results in the\\n$L_0$ and the $L_2$ norms of adversarial perturbations much less than those of\\nexisting works. Experiments on two public datasets with three fake image\\ndetectors indicate that our proposed method achieves state-of-the-art\\nperformance in both white-box and black-box attacks.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Quanyu Liao'}, {'name': 'Yuezun Li'}, {'name': 'Xin Wang'}, {'name': 'Bin Kong'}, {'name': 'Bin Zhu'}, {'name': 'Siwei Lyu'}, {'name': 'Youbing Yin'}, {'name': 'Qi Song'}, {'name': 'Xi Wu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Xi Wu'}\n",
      "\n",
      "\n",
      "author\n",
      "Xi Wu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted by ICIP 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2106.01615v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2106.01615v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2106.01217v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2106.01217v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-06-02T15:10:13Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=2, tm_hour=15, tm_min=10, tm_sec=13, tm_wday=2, tm_yday=153, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-06-02T15:10:13Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=2, tm_hour=15, tm_min=10, tm_sec=13, tm_wday=2, tm_yday=153, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DFGC 2021: A DeepFake Game Competition\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'DFGC 2021: A DeepFake Game Competition'}\n",
      "\n",
      "\n",
      "summary\n",
      "This paper presents a summary of the DFGC 2021 competition. DeepFake\n",
      "technology is developing fast, and realistic face-swaps are increasingly\n",
      "deceiving and hard to detect. At the same time, DeepFake detection methods are\n",
      "also improving. There is a two-party game between DeepFake creators and\n",
      "detectors. This competition provides a common platform for benchmarking the\n",
      "adversarial game between current state-of-the-art DeepFake creation and\n",
      "detection methods. In this paper, we present the organization, results and top\n",
      "solutions of this competition and also share our insights obtained during this\n",
      "event. We also release the DFGC-21 testing dataset collected from our\n",
      "participants to further benefit the research community.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'This paper presents a summary of the DFGC 2021 competition. DeepFake\\ntechnology is developing fast, and realistic face-swaps are increasingly\\ndeceiving and hard to detect. At the same time, DeepFake detection methods are\\nalso improving. There is a two-party game between DeepFake creators and\\ndetectors. This competition provides a common platform for benchmarking the\\nadversarial game between current state-of-the-art DeepFake creation and\\ndetection methods. In this paper, we present the organization, results and top\\nsolutions of this competition and also share our insights obtained during this\\nevent. We also release the DFGC-21 testing dataset collected from our\\nparticipants to further benefit the research community.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Bo Peng'}, {'name': 'Hongxing Fan'}, {'name': 'Wei Wang'}, {'name': 'Jing Dong'}, {'name': 'Yuezun Li'}, {'name': 'Siwei Lyu'}, {'name': 'Qi Li'}, {'name': 'Zhenan Sun'}, {'name': 'Han Chen'}, {'name': 'Baoying Chen'}, {'name': 'Yanjie Hu'}, {'name': 'Shenghai Luo'}, {'name': 'Junrui Huang'}, {'name': 'Yutong Yao'}, {'name': 'Boyuan Liu'}, {'name': 'Hefei Ling'}, {'name': 'Guosheng Zhang'}, {'name': 'Zhiliang Xu'}, {'name': 'Changtao Miao'}, {'name': 'Changlei Lu'}, {'name': 'Shan He'}, {'name': 'Xiaoyan Wu'}, {'name': 'Wanyi Zhuang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Wanyi Zhuang'}\n",
      "\n",
      "\n",
      "author\n",
      "Wanyi Zhuang\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1109/IJCB52358.2021.9484387\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1109/IJCB52358.2021.9484387', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2106.01217v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2106.01217v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "2021 IEEE International Joint Conference on Biometrics (IJCB),\n",
      "  2021, pp. 1-8\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.14376v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.14376v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-29T21:22:24Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=29, tm_hour=21, tm_min=22, tm_sec=24, tm_wday=5, tm_yday=149, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-29T21:22:24Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=29, tm_hour=21, tm_min=22, tm_sec=24, tm_wday=5, tm_yday=149, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis'}\n",
      "\n",
      "\n",
      "summary\n",
      "The rapid advances in deep generative models over the past years have led to\n",
      "highly {realistic media, known as deepfakes,} that are commonly\n",
      "indistinguishable from real to human eyes. These advances make assessing the\n",
      "authenticity of visual data increasingly difficult and pose a misinformation\n",
      "threat to the trustworthiness of visual content in general. Although recent\n",
      "work has shown strong detection accuracy of such deepfakes, the success largely\n",
      "relies on identifying frequency artifacts in the generated images, which will\n",
      "not yield a sustainable detection approach as generative models continue\n",
      "evolving and closing the gap to real images. In order to overcome this issue,\n",
      "we propose a novel fake detection that is designed to re-synthesize testing\n",
      "images and extract visual cues for detection. The re-synthesis procedure is\n",
      "flexible, allowing us to incorporate a series of visual tasks - we adopt\n",
      "super-resolution, denoising and colorization as the re-synthesis. We\n",
      "demonstrate the improved effectiveness, cross-GAN generalization, and\n",
      "robustness against perturbations of our approach in a variety of detection\n",
      "scenarios involving multiple generators over CelebA-HQ, FFHQ, and LSUN\n",
      "datasets. Source code is available at\n",
      "https://github.com/SSAW14/BeyondtheSpectrum.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The rapid advances in deep generative models over the past years have led to\\nhighly {realistic media, known as deepfakes,} that are commonly\\nindistinguishable from real to human eyes. These advances make assessing the\\nauthenticity of visual data increasingly difficult and pose a misinformation\\nthreat to the trustworthiness of visual content in general. Although recent\\nwork has shown strong detection accuracy of such deepfakes, the success largely\\nrelies on identifying frequency artifacts in the generated images, which will\\nnot yield a sustainable detection approach as generative models continue\\nevolving and closing the gap to real images. In order to overcome this issue,\\nwe propose a novel fake detection that is designed to re-synthesize testing\\nimages and extract visual cues for detection. The re-synthesis procedure is\\nflexible, allowing us to incorporate a series of visual tasks - we adopt\\nsuper-resolution, denoising and colorization as the re-synthesis. We\\ndemonstrate the improved effectiveness, cross-GAN generalization, and\\nrobustness against perturbations of our approach in a variety of detection\\nscenarios involving multiple generators over CelebA-HQ, FFHQ, and LSUN\\ndatasets. Source code is available at\\nhttps://github.com/SSAW14/BeyondtheSpectrum.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yang He'}, {'name': 'Ning Yu'}, {'name': 'Margret Keuper'}, {'name': 'Mario Fritz'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Mario Fritz'}\n",
      "\n",
      "\n",
      "author\n",
      "Mario Fritz\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To appear in IJCAI2021. Source code at\n",
      "  https://github.com/SSAW14/BeyondtheSpectrum\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2105.14376v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.14376v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.13617v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.13617v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-28T06:54:10Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=28, tm_hour=6, tm_min=54, tm_sec=10, tm_wday=4, tm_yday=148, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-28T06:54:10Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=28, tm_hour=6, tm_min=54, tm_sec=10, tm_wday=4, tm_yday=148, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FReTAL: Generalizing Deepfake Detection using Knowledge Distillation and\n",
      "  Representation Learning\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'FReTAL: Generalizing Deepfake Detection using Knowledge Distillation and\\n  Representation Learning'}\n",
      "\n",
      "\n",
      "summary\n",
      "As GAN-based video and image manipulation technologies become more\n",
      "sophisticated and easily accessible, there is an urgent need for effective\n",
      "deepfake detection technologies. Moreover, various deepfake generation\n",
      "techniques have emerged over the past few years. While many deepfake detection\n",
      "methods have been proposed, their performance suffers from new types of\n",
      "deepfake methods on which they are not sufficiently trained. To detect new\n",
      "types of deepfakes, the model should learn from additional data without losing\n",
      "its prior knowledge about deepfakes (catastrophic forgetting), especially when\n",
      "new deepfakes are significantly different. In this work, we employ the\n",
      "Representation Learning (ReL) and Knowledge Distillation (KD) paradigms to\n",
      "introduce a transfer learning-based Feature Representation Transfer Adaptation\n",
      "Learning (FReTAL) method. We use FReTAL to perform domain adaptation tasks on\n",
      "new deepfake datasets while minimizing catastrophic forgetting. Our student\n",
      "model can quickly adapt to new types of deepfake by distilling knowledge from a\n",
      "pre-trained teacher model and applying transfer learning without using source\n",
      "domain data during domain adaptation. Through experiments on FaceForensics++\n",
      "datasets, we demonstrate that FReTAL outperforms all baselines on the domain\n",
      "adaptation task with up to 86.97% accuracy on low-quality deepfakes.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'As GAN-based video and image manipulation technologies become more\\nsophisticated and easily accessible, there is an urgent need for effective\\ndeepfake detection technologies. Moreover, various deepfake generation\\ntechniques have emerged over the past few years. While many deepfake detection\\nmethods have been proposed, their performance suffers from new types of\\ndeepfake methods on which they are not sufficiently trained. To detect new\\ntypes of deepfakes, the model should learn from additional data without losing\\nits prior knowledge about deepfakes (catastrophic forgetting), especially when\\nnew deepfakes are significantly different. In this work, we employ the\\nRepresentation Learning (ReL) and Knowledge Distillation (KD) paradigms to\\nintroduce a transfer learning-based Feature Representation Transfer Adaptation\\nLearning (FReTAL) method. We use FReTAL to perform domain adaptation tasks on\\nnew deepfake datasets while minimizing catastrophic forgetting. Our student\\nmodel can quickly adapt to new types of deepfake by distilling knowledge from a\\npre-trained teacher model and applying transfer learning without using source\\ndomain data during domain adaptation. Through experiments on FaceForensics++\\ndatasets, we demonstrate that FReTAL outperforms all baselines on the domain\\nadaptation task with up to 86.97% accuracy on low-quality deepfakes.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Minha Kim'}, {'name': 'Shahroz Tariq'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "12 pages, 2 figures, 5 tables, accepted for publication at the\n",
      "  Workshop on Media Forensics 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2105.13617v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.13617v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.4.9; I.5.4', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.12855v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.12855v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-26T21:25:27Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=26, tm_hour=21, tm_min=25, tm_sec=27, tm_wday=2, tm_yday=146, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-26T21:25:27Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=26, tm_hour=21, tm_min=25, tm_sec=27, tm_wday=2, tm_yday=146, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Multi-Modal Semantic Inconsistency Detection in Social Media News Posts\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Multi-Modal Semantic Inconsistency Detection in Social Media News Posts'}\n",
      "\n",
      "\n",
      "summary\n",
      "As computer-generated content and deepfakes make steady improvements,\n",
      "semantic approaches to multimedia forensics will become more important. In this\n",
      "paper, we introduce a novel classification architecture for identifying\n",
      "semantic inconsistencies between video appearance and text caption in social\n",
      "media news posts. We develop a multi-modal fusion framework to identify\n",
      "mismatches between videos and captions in social media posts by leveraging an\n",
      "ensemble method based on textual analysis of the caption, automatic audio\n",
      "transcription, semantic video analysis, object detection, named entity\n",
      "consistency, and facial verification. To train and test our approach, we curate\n",
      "a new video-based dataset of 4,000 real-world Facebook news posts for analysis.\n",
      "Our multi-modal approach achieves 60.5% classification accuracy on random\n",
      "mismatches between caption and appearance, compared to accuracy below 50% for\n",
      "uni-modal models. Further ablation studies confirm the necessity of fusion\n",
      "across modalities for correctly identifying semantic inconsistencies.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'As computer-generated content and deepfakes make steady improvements,\\nsemantic approaches to multimedia forensics will become more important. In this\\npaper, we introduce a novel classification architecture for identifying\\nsemantic inconsistencies between video appearance and text caption in social\\nmedia news posts. We develop a multi-modal fusion framework to identify\\nmismatches between videos and captions in social media posts by leveraging an\\nensemble method based on textual analysis of the caption, automatic audio\\ntranscription, semantic video analysis, object detection, named entity\\nconsistency, and facial verification. To train and test our approach, we curate\\na new video-based dataset of 4,000 real-world Facebook news posts for analysis.\\nOur multi-modal approach achieves 60.5% classification accuracy on random\\nmismatches between caption and appearance, compared to accuracy below 50% for\\nuni-modal models. Further ablation studies confirm the necessity of fusion\\nacross modalities for correctly identifying semantic inconsistencies.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Scott McCrae'}, {'name': 'Kehan Wang'}, {'name': 'Avideh Zakhor'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Avideh Zakhor'}\n",
      "\n",
      "\n",
      "author\n",
      "Avideh Zakhor\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2105.12855v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.12855v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.10872v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.10872v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-12-14T02:56:14Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=12, tm_mday=14, tm_hour=2, tm_min=56, tm_sec=14, tm_wday=1, tm_yday=348, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-23T07:28:36Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=23, tm_hour=7, tm_min=28, tm_sec=36, tm_wday=6, tm_yday=143, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for\n",
      "  Combating Deepfakes\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for\\n  Combating Deepfakes'}\n",
      "\n",
      "\n",
      "summary\n",
      "Malicious applications of deepfakes (i.e., technologies generating target\n",
      "facial attributes or entire faces from facial images) have posed a huge threat\n",
      "to individuals' reputation and security. To mitigate these threats, recent\n",
      "studies have proposed adversarial watermarks to combat deepfake models, leading\n",
      "them to generate distorted outputs. Despite achieving impressive results, these\n",
      "adversarial watermarks have low image-level and model-level transferability,\n",
      "meaning that they can protect only one facial image from one specific deepfake\n",
      "model. To address these issues, we propose a novel solution that can generate a\n",
      "Cross-Model Universal Adversarial Watermark (CMUA-Watermark), protecting a\n",
      "large number of facial images from multiple deepfake models. Specifically, we\n",
      "begin by proposing a cross-model universal attack pipeline that attacks\n",
      "multiple deepfake models iteratively. Then, we design a two-level perturbation\n",
      "fusion strategy to alleviate the conflict between the adversarial watermarks\n",
      "generated by different facial images and models. Moreover, we address the key\n",
      "problem in cross-model optimization with a heuristic approach to automatically\n",
      "find the suitable attack step sizes for different models, further weakening the\n",
      "model-level conflict. Finally, we introduce a more reasonable and comprehensive\n",
      "evaluation method to fully test the proposed method and compare it with\n",
      "existing ones. Extensive experimental results demonstrate that the proposed\n",
      "CMUA-Watermark can effectively distort the fake facial images generated by\n",
      "multiple deepfake models while achieving a better performance than existing\n",
      "methods.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Malicious applications of deepfakes (i.e., technologies generating target\\nfacial attributes or entire faces from facial images) have posed a huge threat\\nto individuals' reputation and security. To mitigate these threats, recent\\nstudies have proposed adversarial watermarks to combat deepfake models, leading\\nthem to generate distorted outputs. Despite achieving impressive results, these\\nadversarial watermarks have low image-level and model-level transferability,\\nmeaning that they can protect only one facial image from one specific deepfake\\nmodel. To address these issues, we propose a novel solution that can generate a\\nCross-Model Universal Adversarial Watermark (CMUA-Watermark), protecting a\\nlarge number of facial images from multiple deepfake models. Specifically, we\\nbegin by proposing a cross-model universal attack pipeline that attacks\\nmultiple deepfake models iteratively. Then, we design a two-level perturbation\\nfusion strategy to alleviate the conflict between the adversarial watermarks\\ngenerated by different facial images and models. Moreover, we address the key\\nproblem in cross-model optimization with a heuristic approach to automatically\\nfind the suitable attack step sizes for different models, further weakening the\\nmodel-level conflict. Finally, we introduce a more reasonable and comprehensive\\nevaluation method to fully test the proposed method and compare it with\\nexisting ones. Extensive experimental results demonstrate that the proposed\\nCMUA-Watermark can effectively distort the fake facial images generated by\\nmultiple deepfake models while achieving a better performance than existing\\nmethods.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hao Huang'}, {'name': 'Yongtao Wang'}, {'name': 'Zhaoyu Chen'}, {'name': 'Yuze Zhang'}, {'name': 'Yuheng Li'}, {'name': 'Zhi Tang'}, {'name': 'Wei Chu'}, {'name': 'Jingdong Chen'}, {'name': 'Weisi Lin'}, {'name': 'Kai-Kuang Ma'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Kai-Kuang Ma'}\n",
      "\n",
      "\n",
      "author\n",
      "Kai-Kuang Ma\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "9 pages, 7 figures, Thirty-Sixth AAAI Conference on Artificial\n",
      "  Intelligence, AAAI22\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2105.10872v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.10872v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.06496v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.06496v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-27T19:32:41Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=27, tm_hour=19, tm_min=32, tm_sec=41, tm_wday=2, tm_yday=300, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-13T18:22:16Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=13, tm_hour=18, tm_min=22, tm_sec=16, tm_wday=3, tm_yday=133, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Detection by Human Crowds, Machines, and Machine-informed\n",
      "  Crowds\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfake Detection by Human Crowds, Machines, and Machine-informed\\n  Crowds'}\n",
      "\n",
      "\n",
      "summary\n",
      "The recent emergence of machine-manipulated media raises an important\n",
      "societal question: how can we know if a video that we watch is real or fake? In\n",
      "two online studies with 15,016 participants, we present authentic videos and\n",
      "deepfakes and ask participants to identify which is which. We compare the\n",
      "performance of ordinary human observers against the leading computer vision\n",
      "deepfake detection model and find them similarly accurate while making\n",
      "different kinds of mistakes. Together, participants with access to the model's\n",
      "prediction are more accurate than either alone, but inaccurate model\n",
      "predictions often decrease participants' accuracy. To probe the relative\n",
      "strengths and weaknesses of humans and machines as detectors of deepfakes, we\n",
      "examine human and machine performance across video-level features, and we\n",
      "evaluate the impact of pre-registered randomized interventions on deepfake\n",
      "detection. We find that manipulations designed to disrupt visual processing of\n",
      "faces hinder human participants' performance while mostly not affecting the\n",
      "model's performance, suggesting a role for specialized cognitive capacities in\n",
      "explaining human deepfake detection performance.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"The recent emergence of machine-manipulated media raises an important\\nsocietal question: how can we know if a video that we watch is real or fake? In\\ntwo online studies with 15,016 participants, we present authentic videos and\\ndeepfakes and ask participants to identify which is which. We compare the\\nperformance of ordinary human observers against the leading computer vision\\ndeepfake detection model and find them similarly accurate while making\\ndifferent kinds of mistakes. Together, participants with access to the model's\\nprediction are more accurate than either alone, but inaccurate model\\npredictions often decrease participants' accuracy. To probe the relative\\nstrengths and weaknesses of humans and machines as detectors of deepfakes, we\\nexamine human and machine performance across video-level features, and we\\nevaluate the impact of pre-registered randomized interventions on deepfake\\ndetection. We find that manipulations designed to disrupt visual processing of\\nfaces hinder human participants' performance while mostly not affecting the\\nmodel's performance, suggesting a role for specialized cognitive capacities in\\nexplaining human deepfake detection performance.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Matthew Groh'}, {'name': 'Ziv Epstein'}, {'name': 'Chaz Firestone'}, {'name': 'Rosalind Picard'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Rosalind Picard'}\n",
      "\n",
      "\n",
      "author\n",
      "Rosalind Picard\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1073/pnas.2110013119\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1073/pnas.2110013119', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2105.06496v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.06496v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "Proceedings of the National Academy of Sciences Jan 2022, 119 (1)\n",
      "  e2110013119\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.06117v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.06117v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-13T07:31:08Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=13, tm_hour=7, tm_min=31, tm_sec=8, tm_wday=3, tm_yday=133, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-13T07:31:08Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=13, tm_hour=7, tm_min=31, tm_sec=8, tm_wday=3, tm_yday=133, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "TAR: Generalized Forensic Framework to Detect Deepfakes using Weakly\n",
      "  Supervised Learning\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'TAR: Generalized Forensic Framework to Detect Deepfakes using Weakly\\n  Supervised Learning'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes have become a critical social problem, and detecting them is of\n",
      "utmost importance. Also, deepfake generation methods are advancing, and it is\n",
      "becoming harder to detect. While many deepfake detection models can detect\n",
      "different types of deepfakes separately, they perform poorly on generalizing\n",
      "the detection performance over multiple types of deepfake. This motivates us to\n",
      "develop a generalized model to detect different types of deepfakes. Therefore,\n",
      "in this work, we introduce a practical digital forensic tool to detect\n",
      "different types of deepfakes simultaneously and propose Transfer learning-based\n",
      "Autoencoder with Residuals (TAR). The ultimate goal of our work is to develop a\n",
      "unified model to detect various types of deepfake videos with high accuracy,\n",
      "with only a small number of training samples that can work well in real-world\n",
      "settings. We develop an autoencoder-based detection model with Residual blocks\n",
      "and sequentially perform transfer learning to detect different types of\n",
      "deepfakes simultaneously. Our approach achieves a much higher generalized\n",
      "detection performance than the state-of-the-art methods on the FaceForensics++\n",
      "dataset. In addition, we evaluate our model on 200 real-world\n",
      "Deepfake-in-the-Wild (DW) videos of 50 celebrities available on the Internet\n",
      "and achieve 89.49% zero-shot accuracy, which is significantly higher than the\n",
      "best baseline model (gaining 10.77%), demonstrating and validating the\n",
      "practicability of our approach.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfakes have become a critical social problem, and detecting them is of\\nutmost importance. Also, deepfake generation methods are advancing, and it is\\nbecoming harder to detect. While many deepfake detection models can detect\\ndifferent types of deepfakes separately, they perform poorly on generalizing\\nthe detection performance over multiple types of deepfake. This motivates us to\\ndevelop a generalized model to detect different types of deepfakes. Therefore,\\nin this work, we introduce a practical digital forensic tool to detect\\ndifferent types of deepfakes simultaneously and propose Transfer learning-based\\nAutoencoder with Residuals (TAR). The ultimate goal of our work is to develop a\\nunified model to detect various types of deepfake videos with high accuracy,\\nwith only a small number of training samples that can work well in real-world\\nsettings. We develop an autoencoder-based detection model with Residual blocks\\nand sequentially perform transfer learning to detect different types of\\ndeepfakes simultaneously. Our approach achieves a much higher generalized\\ndetection performance than the state-of-the-art methods on the FaceForensics++\\ndataset. In addition, we evaluate our model on 200 real-world\\nDeepfake-in-the-Wild (DW) videos of 50 celebrities available on the Internet\\nand achieve 89.49% zero-shot accuracy, which is significantly higher than the\\nbest baseline model (gaining 10.77%), demonstrating and validating the\\npracticability of our approach.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sangyup Lee'}, {'name': 'Shahroz Tariq'}, {'name': 'Junyaup Kim'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "16 pages, 3 figures, to be published in IFIP-SEC 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2105.06117v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.06117v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.05902v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.05902v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-12T18:44:39Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=12, tm_hour=18, tm_min=44, tm_sec=39, tm_wday=2, tm_yday=132, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-12T18:44:39Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=12, tm_hour=18, tm_min=44, tm_sec=39, tm_wday=2, tm_yday=132, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "What's wrong with this video? Comparing Explainers for Deepfake\n",
      "  Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"What's wrong with this video? Comparing Explainers for Deepfake\\n  Detection\"}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes are computer manipulated videos where the face of an individual has\n",
      "been replaced with that of another. Software for creating such forgeries is\n",
      "easy to use and ever more popular, causing serious threats to personal\n",
      "reputation and public security. The quality of classifiers for detecting\n",
      "deepfakes has improved with the releasing of ever larger datasets, but the\n",
      "understanding of why a particular video has been labelled as fake has not kept\n",
      "pace.\n",
      "  In this work we develop, extend and compare white-box, black-box and\n",
      "model-specific techniques for explaining the labelling of real and fake videos.\n",
      "In particular, we adapt SHAP, GradCAM and self-attention models to the task of\n",
      "explaining the predictions of state-of-the-art detectors based on EfficientNet,\n",
      "trained on the Deepfake Detection Challenge (DFDC) dataset. We compare the\n",
      "obtained explanations, proposing metrics to quantify their visual features and\n",
      "desirable characteristics, and also perform a user survey collecting users'\n",
      "opinions regarding the usefulness of the explainers.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"Deepfakes are computer manipulated videos where the face of an individual has\\nbeen replaced with that of another. Software for creating such forgeries is\\neasy to use and ever more popular, causing serious threats to personal\\nreputation and public security. The quality of classifiers for detecting\\ndeepfakes has improved with the releasing of ever larger datasets, but the\\nunderstanding of why a particular video has been labelled as fake has not kept\\npace.\\n  In this work we develop, extend and compare white-box, black-box and\\nmodel-specific techniques for explaining the labelling of real and fake videos.\\nIn particular, we adapt SHAP, GradCAM and self-attention models to the task of\\nexplaining the predictions of state-of-the-art detectors based on EfficientNet,\\ntrained on the Deepfake Detection Challenge (DFDC) dataset. We compare the\\nobtained explanations, proposing metrics to quantify their visual features and\\ndesirable characteristics, and also perform a user survey collecting users'\\nopinions regarding the usefulness of the explainers.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Samuele Pino'}, {'name': 'Mark James Carman'}, {'name': 'Paolo Bestagini'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Paolo Bestagini'}\n",
      "\n",
      "\n",
      "author\n",
      "Paolo Bestagini\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "8 pages, 12 figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2105.05902v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.05902v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.04932v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.04932v2\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-19T09:22:41Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=19, tm_hour=9, tm_min=22, tm_sec=41, tm_wday=5, tm_yday=78, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-11T10:41:47Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=11, tm_hour=10, tm_min=41, tm_sec=47, tm_wday=1, tm_yday=131, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "One Shot Face Swapping on Megapixels\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'One Shot Face Swapping on Megapixels'}\n",
      "\n",
      "\n",
      "summary\n",
      "Face swapping has both positive applications such as entertainment,\n",
      "human-computer interaction, etc., and negative applications such as DeepFake\n",
      "threats to politics, economics, etc. Nevertheless, it is necessary to\n",
      "understand the scheme of advanced methods for high-quality face swapping and\n",
      "generate enough and representative face swapping images to train DeepFake\n",
      "detection algorithms. This paper proposes the first Megapixel level method for\n",
      "one shot Face Swapping (or MegaFS for short). Firstly, MegaFS organizes face\n",
      "representation hierarchically by the proposed Hierarchical Representation Face\n",
      "Encoder (HieRFE) in an extended latent space to maintain more facial details,\n",
      "rather than compressed representation in previous face swapping methods.\n",
      "Secondly, a carefully designed Face Transfer Module (FTM) is proposed to\n",
      "transfer the identity from a source image to the target by a non-linear\n",
      "trajectory without explicit feature disentanglement. Finally, the swapped faces\n",
      "can be synthesized by StyleGAN2 with the benefits of its training stability and\n",
      "powerful generative capability. Each part of MegaFS can be trained separately\n",
      "so the requirement of our model for GPU memory can be satisfied for megapixel\n",
      "face swapping. In summary, complete face representation, stable training, and\n",
      "limited memory usage are the three novel contributions to the success of our\n",
      "method. Extensive experiments demonstrate the superiority of MegaFS and the\n",
      "first megapixel level face swapping database is released for research on\n",
      "DeepFake detection and face image editing in the public domain. The dataset is\n",
      "at this link.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Face swapping has both positive applications such as entertainment,\\nhuman-computer interaction, etc., and negative applications such as DeepFake\\nthreats to politics, economics, etc. Nevertheless, it is necessary to\\nunderstand the scheme of advanced methods for high-quality face swapping and\\ngenerate enough and representative face swapping images to train DeepFake\\ndetection algorithms. This paper proposes the first Megapixel level method for\\none shot Face Swapping (or MegaFS for short). Firstly, MegaFS organizes face\\nrepresentation hierarchically by the proposed Hierarchical Representation Face\\nEncoder (HieRFE) in an extended latent space to maintain more facial details,\\nrather than compressed representation in previous face swapping methods.\\nSecondly, a carefully designed Face Transfer Module (FTM) is proposed to\\ntransfer the identity from a source image to the target by a non-linear\\ntrajectory without explicit feature disentanglement. Finally, the swapped faces\\ncan be synthesized by StyleGAN2 with the benefits of its training stability and\\npowerful generative capability. Each part of MegaFS can be trained separately\\nso the requirement of our model for GPU memory can be satisfied for megapixel\\nface swapping. In summary, complete face representation, stable training, and\\nlimited memory usage are the three novel contributions to the success of our\\nmethod. Extensive experiments demonstrate the superiority of MegaFS and the\\nfirst megapixel level face swapping database is released for research on\\nDeepFake detection and face image editing in the public domain. The dataset is\\nat this link.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yuhao Zhu'}, {'name': 'Qi Li'}, {'name': 'Jian Wang'}, {'name': 'Chengzhong Xu'}, {'name': 'Zhenan Sun'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Zhenan Sun'}\n",
      "\n",
      "\n",
      "author\n",
      "Zhenan Sun\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2105.04932v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.04932v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.00558v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.00558v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-02T21:55:04Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=2, tm_hour=21, tm_min=55, tm_sec=4, tm_wday=6, tm_yday=122, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-02T21:55:04Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=2, tm_hour=21, tm_min=55, tm_sec=4, tm_wday=6, tm_yday=122, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "An Examination of Fairness of AI Models for Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'An Examination of Fairness of AI Models for Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent studies have demonstrated that deep learning models can discriminate\n",
      "based on protected classes like race and gender. In this work, we evaluate bias\n",
      "present in deepfake datasets and detection models across protected subgroups.\n",
      "Using facial datasets balanced by race and gender, we examine three popular\n",
      "deepfake detectors and find large disparities in predictive performances across\n",
      "races, with up to 10.7% difference in error rate between subgroups. A closer\n",
      "look reveals that the widely used FaceForensics++ dataset is overwhelmingly\n",
      "composed of Caucasian subjects, with the majority being female Caucasians. Our\n",
      "investigation of the racial distribution of deepfakes reveals that the methods\n",
      "used to create deepfakes as positive training signals tend to produce\n",
      "\"irregular\" faces - when a person's face is swapped onto another person of a\n",
      "different race or gender. This causes detectors to learn spurious correlations\n",
      "between the foreground faces and fakeness. Moreover, when detectors are trained\n",
      "with the Blended Image (BI) dataset from Face X-Rays, we find that those\n",
      "detectors develop systematic discrimination towards certain racial subgroups,\n",
      "primarily female Asians.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Recent studies have demonstrated that deep learning models can discriminate\\nbased on protected classes like race and gender. In this work, we evaluate bias\\npresent in deepfake datasets and detection models across protected subgroups.\\nUsing facial datasets balanced by race and gender, we examine three popular\\ndeepfake detectors and find large disparities in predictive performances across\\nraces, with up to 10.7% difference in error rate between subgroups. A closer\\nlook reveals that the widely used FaceForensics++ dataset is overwhelmingly\\ncomposed of Caucasian subjects, with the majority being female Caucasians. Our\\ninvestigation of the racial distribution of deepfakes reveals that the methods\\nused to create deepfakes as positive training signals tend to produce\\n\"irregular\" faces - when a person\\'s face is swapped onto another person of a\\ndifferent race or gender. This causes detectors to learn spurious correlations\\nbetween the foreground faces and fakeness. Moreover, when detectors are trained\\nwith the Blended Image (BI) dataset from Face X-Rays, we find that those\\ndetectors develop systematic discrimination towards certain racial subgroups,\\nprimarily female Asians.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Loc Trinh'}, {'name': 'Yan Liu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yan Liu'}\n",
      "\n",
      "\n",
      "author\n",
      "Yan Liu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To appear in the 30th International Joint Conference on Artificial\n",
      "  Intelligence (IJCAI-21)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2105.00558v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.00558v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.00192v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.00192v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-01T08:25:43Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=1, tm_hour=8, tm_min=25, tm_sec=43, tm_wday=5, tm_yday=121, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-01T08:25:43Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=1, tm_hour=8, tm_min=25, tm_sec=43, tm_wday=5, tm_yday=121, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deep Insights of Deepfake Technology : A Review\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deep Insights of Deepfake Technology : A Review'}\n",
      "\n",
      "\n",
      "summary\n",
      "Under the aegis of computer vision and deep learning technology, a new\n",
      "emerging techniques has introduced that anyone can make highly realistic but\n",
      "fake videos, images even can manipulates the voices. This technology is widely\n",
      "known as Deepfake Technology. Although it seems interesting techniques to make\n",
      "fake videos or image of something or some individuals but it could spread as\n",
      "misinformation via internet. Deepfake contents could be dangerous for\n",
      "individuals as well as for our communities, organizations, countries religions\n",
      "etc. As Deepfake content creation involve a high level expertise with\n",
      "combination of several algorithms of deep learning, it seems almost real and\n",
      "genuine and difficult to differentiate. In this paper, a wide range of articles\n",
      "have been examined to understand Deepfake technology more extensively. We have\n",
      "examined several articles to find some insights such as what is Deepfake, who\n",
      "are responsible for this, is there any benefits of Deepfake and what are the\n",
      "challenges of this technology. We have also examined several creation and\n",
      "detection techniques. Our study revealed that although Deepfake is a threat to\n",
      "our societies, proper measures and strict regulations could prevent this.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Under the aegis of computer vision and deep learning technology, a new\\nemerging techniques has introduced that anyone can make highly realistic but\\nfake videos, images even can manipulates the voices. This technology is widely\\nknown as Deepfake Technology. Although it seems interesting techniques to make\\nfake videos or image of something or some individuals but it could spread as\\nmisinformation via internet. Deepfake contents could be dangerous for\\nindividuals as well as for our communities, organizations, countries religions\\netc. As Deepfake content creation involve a high level expertise with\\ncombination of several algorithms of deep learning, it seems almost real and\\ngenuine and difficult to differentiate. In this paper, a wide range of articles\\nhave been examined to understand Deepfake technology more extensively. We have\\nexamined several articles to find some insights such as what is Deepfake, who\\nare responsible for this, is there any benefits of Deepfake and what are the\\nchallenges of this technology. We have also examined several creation and\\ndetection techniques. Our study revealed that although Deepfake is a threat to\\nour societies, proper measures and strict regulations could prevent this.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Bahar Uddin Mahmud'}, {'name': 'Afsana Sharmin'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Afsana Sharmin'}\n",
      "\n",
      "\n",
      "author\n",
      "Afsana Sharmin\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "DUJASE Vol. 5(1 & 2) 13-23, 2020 (January & July)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2105.00192v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.00192v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2105.00187v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2105.00187v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-01T08:02:59Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=1, tm_hour=8, tm_min=2, tm_sec=59, tm_wday=5, tm_yday=121, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-05-01T08:02:59Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=1, tm_hour=8, tm_min=2, tm_sec=59, tm_wday=5, tm_yday=121, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "One Detector to Rule Them All: Towards a General Deepfake Attack\n",
      "  Detection Framework\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'One Detector to Rule Them All: Towards a General Deepfake Attack\\n  Detection Framework'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deep learning-based video manipulation methods have become widely accessible\n",
      "to the masses. With little to no effort, people can quickly learn how to\n",
      "generate deepfake (DF) videos. While deep learning-based detection methods have\n",
      "been proposed to identify specific types of DFs, their performance suffers for\n",
      "other types of deepfake methods, including real-world deepfakes, on which they\n",
      "are not sufficiently trained. In other words, most of the proposed deep\n",
      "learning-based detection methods lack transferability and generalizability.\n",
      "Beyond detecting a single type of DF from benchmark deepfake datasets, we focus\n",
      "on developing a generalized approach to detect multiple types of DFs, including\n",
      "deepfakes from unknown generation methods such as DeepFake-in-the-Wild (DFW)\n",
      "videos. To better cope with unknown and unseen deepfakes, we introduce a\n",
      "Convolutional LSTM-based Residual Network (CLRNet), which adopts a unique model\n",
      "training strategy and explores spatial as well as the temporal information in\n",
      "deepfakes. Through extensive experiments, we show that existing defense methods\n",
      "are not ready for real-world deployment. Whereas our defense method (CLRNet)\n",
      "achieves far better generalization when detecting various benchmark deepfake\n",
      "methods (97.57% on average). Furthermore, we evaluate our approach with a\n",
      "high-quality DeepFake-in-the-Wild dataset, collected from the Internet\n",
      "containing numerous videos and having more than 150,000 frames. Our CLRNet\n",
      "model demonstrated that it generalizes well against high-quality DFW videos by\n",
      "achieving 93.86% detection accuracy, outperforming existing state-of-the-art\n",
      "defense methods by a considerable margin.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deep learning-based video manipulation methods have become widely accessible\\nto the masses. With little to no effort, people can quickly learn how to\\ngenerate deepfake (DF) videos. While deep learning-based detection methods have\\nbeen proposed to identify specific types of DFs, their performance suffers for\\nother types of deepfake methods, including real-world deepfakes, on which they\\nare not sufficiently trained. In other words, most of the proposed deep\\nlearning-based detection methods lack transferability and generalizability.\\nBeyond detecting a single type of DF from benchmark deepfake datasets, we focus\\non developing a generalized approach to detect multiple types of DFs, including\\ndeepfakes from unknown generation methods such as DeepFake-in-the-Wild (DFW)\\nvideos. To better cope with unknown and unseen deepfakes, we introduce a\\nConvolutional LSTM-based Residual Network (CLRNet), which adopts a unique model\\ntraining strategy and explores spatial as well as the temporal information in\\ndeepfakes. Through extensive experiments, we show that existing defense methods\\nare not ready for real-world deployment. Whereas our defense method (CLRNet)\\nachieves far better generalization when detecting various benchmark deepfake\\nmethods (97.57% on average). Furthermore, we evaluate our approach with a\\nhigh-quality DeepFake-in-the-Wild dataset, collected from the Internet\\ncontaining numerous videos and having more than 150,000 frames. Our CLRNet\\nmodel demonstrated that it generalizes well against high-quality DFW videos by\\nachieving 93.86% detection accuracy, outperforming existing state-of-the-art\\ndefense methods by a considerable margin.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shahroz Tariq'}, {'name': 'Sangyup Lee'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1145/3442381.3449809\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3442381.3449809', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2105.00187v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.00187v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "14 pages, 8 Figures, 6 Tables, Accepted for publication in The Web\n",
      "  Conference WWW 2021\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.4.9; I.5.4', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2104.11507v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2104.11507v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-04-23T09:48:10Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=23, tm_hour=9, tm_min=48, tm_sec=10, tm_wday=4, tm_yday=113, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-04-23T09:48:10Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=23, tm_hour=9, tm_min=48, tm_sec=10, tm_wday=4, tm_yday=113, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepfakeUCL: Deepfake Detection via Unsupervised Contrastive Learning\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'DeepfakeUCL: Deepfake Detection via Unsupervised Contrastive Learning'}\n",
      "\n",
      "\n",
      "summary\n",
      "Face deepfake detection has seen impressive results recently. Nearly all\n",
      "existing deep learning techniques for face deepfake detection are fully\n",
      "supervised and require labels during training. In this paper, we design a novel\n",
      "deepfake detection method via unsupervised contrastive learning. We first\n",
      "generate two different transformed versions of an image and feed them into two\n",
      "sequential sub-networks, i.e., an encoder and a projection head. The\n",
      "unsupervised training is achieved by maximizing the correspondence degree of\n",
      "the outputs of the projection head. To evaluate the detection performance of\n",
      "our unsupervised method, we further use the unsupervised features to train an\n",
      "efficient linear classification network. Extensive experiments show that our\n",
      "unsupervised learning method enables comparable detection performance to\n",
      "state-of-the-art supervised techniques, in both the intra- and inter-dataset\n",
      "settings. We also conduct ablation studies for our method.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Face deepfake detection has seen impressive results recently. Nearly all\\nexisting deep learning techniques for face deepfake detection are fully\\nsupervised and require labels during training. In this paper, we design a novel\\ndeepfake detection method via unsupervised contrastive learning. We first\\ngenerate two different transformed versions of an image and feed them into two\\nsequential sub-networks, i.e., an encoder and a projection head. The\\nunsupervised training is achieved by maximizing the correspondence degree of\\nthe outputs of the projection head. To evaluate the detection performance of\\nour unsupervised method, we further use the unsupervised features to train an\\nefficient linear classification network. Extensive experiments show that our\\nunsupervised learning method enables comparable detection performance to\\nstate-of-the-art supervised techniques, in both the intra- and inter-dataset\\nsettings. We also conduct ablation studies for our method.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sheldon Fung'}, {'name': 'Xuequan Lu'}, {'name': 'Chao Zhang'}, {'name': 'Chang-Tsun Li'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Chang-Tsun Li'}\n",
      "\n",
      "\n",
      "author\n",
      "Chang-Tsun Li\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "accepted to IJCNN2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2104.11507v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.11507v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2104.09770v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2104.09770v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-04-21T12:59:29Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=21, tm_hour=12, tm_min=59, tm_sec=29, tm_wday=2, tm_yday=111, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-04-20T05:43:44Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=20, tm_hour=5, tm_min=43, tm_sec=44, tm_wday=1, tm_yday=110, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "The widespread dissemination of forged images generated by Deepfake\n",
      "techniques has posed a serious threat to the trustworthiness of digital\n",
      "information. This demands effective approaches that can detect perceptually\n",
      "convincing Deepfakes generated by advanced manipulation techniques. Most\n",
      "existing approaches combat Deepfakes with deep neural networks by mapping the\n",
      "input image to a binary prediction without capturing the consistency among\n",
      "different pixels. In this paper, we aim to capture the subtle manipulation\n",
      "artifacts at different scales for Deepfake detection. We achieve this with\n",
      "transformer models, which have recently demonstrated superior performance in\n",
      "modeling dependencies between pixels for a variety of recognition tasks in\n",
      "computer vision. In particular, we introduce a Multi-modal Multi-scale\n",
      "TRansformer (M2TR), which uses a multi-scale transformer that operates on\n",
      "patches of different sizes to detect the local inconsistency at different\n",
      "spatial levels. To improve the detection results and enhance the robustness of\n",
      "our method to image compression, M2TR also takes frequency information, which\n",
      "is further combined with RGB features using a cross modality fusion module.\n",
      "Developing and evaluating Deepfake detection methods requires large-scale\n",
      "datasets. However, we observe that samples in existing benchmarks contain\n",
      "severe artifacts and lack diversity. This motivates us to introduce a\n",
      "high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos\n",
      "generated by state-of-the-art face swapping and facial reenactment methods. On\n",
      "three Deepfake datasets, we conduct extensive experiments to verify the\n",
      "effectiveness of the proposed method, which outperforms state-of-the-art\n",
      "Deepfake detection methods.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'The widespread dissemination of forged images generated by Deepfake\\ntechniques has posed a serious threat to the trustworthiness of digital\\ninformation. This demands effective approaches that can detect perceptually\\nconvincing Deepfakes generated by advanced manipulation techniques. Most\\nexisting approaches combat Deepfakes with deep neural networks by mapping the\\ninput image to a binary prediction without capturing the consistency among\\ndifferent pixels. In this paper, we aim to capture the subtle manipulation\\nartifacts at different scales for Deepfake detection. We achieve this with\\ntransformer models, which have recently demonstrated superior performance in\\nmodeling dependencies between pixels for a variety of recognition tasks in\\ncomputer vision. In particular, we introduce a Multi-modal Multi-scale\\nTRansformer (M2TR), which uses a multi-scale transformer that operates on\\npatches of different sizes to detect the local inconsistency at different\\nspatial levels. To improve the detection results and enhance the robustness of\\nour method to image compression, M2TR also takes frequency information, which\\nis further combined with RGB features using a cross modality fusion module.\\nDeveloping and evaluating Deepfake detection methods requires large-scale\\ndatasets. However, we observe that samples in existing benchmarks contain\\nsevere artifacts and lack diversity. This motivates us to introduce a\\nhigh-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos\\ngenerated by state-of-the-art face swapping and facial reenactment methods. On\\nthree Deepfake datasets, we conduct extensive experiments to verify the\\neffectiveness of the proposed method, which outperforms state-of-the-art\\nDeepfake detection methods.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Junke Wang'}, {'name': 'Zuxuan Wu'}, {'name': 'Jingjing Chen'}, {'name': 'Yu-Gang Jiang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yu-Gang Jiang'}\n",
      "\n",
      "\n",
      "author\n",
      "Yu-Gang Jiang\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2104.09770v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.09770v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2104.04480v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2104.04480v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-04-09T16:57:55Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=9, tm_hour=16, tm_min=57, tm_sec=55, tm_wday=4, tm_yday=99, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-04-09T16:57:55Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=9, tm_hour=16, tm_min=57, tm_sec=55, tm_wday=4, tm_yday=99, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Improving the Efficiency and Robustness of Deepfakes Detection through\n",
      "  Precise Geometric Features\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Improving the Efficiency and Robustness of Deepfakes Detection through\\n  Precise Geometric Features'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes is a branch of malicious techniques that transplant a target face\n",
      "to the original one in videos, resulting in serious problems such as\n",
      "infringement of copyright, confusion of information, or even public panic.\n",
      "Previous efforts for Deepfakes videos detection mainly focused on appearance\n",
      "features, which have a risk of being bypassed by sophisticated manipulation,\n",
      "also resulting in high model complexity and sensitiveness to noise. Besides,\n",
      "how to mine the temporal features of manipulated videos and exploit them is\n",
      "still an open question. We propose an efficient and robust framework named\n",
      "LRNet for detecting Deepfakes videos through temporal modeling on precise\n",
      "geometric features. A novel calibration module is devised to enhance the\n",
      "precision of geometric features, making it more discriminative, and a\n",
      "two-stream Recurrent Neural Network (RNN) is constructed for sufficient\n",
      "exploitation of temporal features. Compared to previous methods, our proposed\n",
      "method is lighter-weighted and easier to train. Moreover, our method has shown\n",
      "robustness in detecting highly compressed or noise corrupted videos. Our model\n",
      "achieved 0.999 AUC on FaceForensics++ dataset. Meanwhile, it has a graceful\n",
      "decline in performance (-0.042 AUC) when faced with highly compressed videos.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Deepfakes is a branch of malicious techniques that transplant a target face\\nto the original one in videos, resulting in serious problems such as\\ninfringement of copyright, confusion of information, or even public panic.\\nPrevious efforts for Deepfakes videos detection mainly focused on appearance\\nfeatures, which have a risk of being bypassed by sophisticated manipulation,\\nalso resulting in high model complexity and sensitiveness to noise. Besides,\\nhow to mine the temporal features of manipulated videos and exploit them is\\nstill an open question. We propose an efficient and robust framework named\\nLRNet for detecting Deepfakes videos through temporal modeling on precise\\ngeometric features. A novel calibration module is devised to enhance the\\nprecision of geometric features, making it more discriminative, and a\\ntwo-stream Recurrent Neural Network (RNN) is constructed for sufficient\\nexploitation of temporal features. Compared to previous methods, our proposed\\nmethod is lighter-weighted and easier to train. Moreover, our method has shown\\nrobustness in detecting highly compressed or noise corrupted videos. Our model\\nachieved 0.999 AUC on FaceForensics++ dataset. Meanwhile, it has a graceful\\ndecline in performance (-0.042 AUC) when faced with highly compressed videos.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Zekun Sun'}, {'name': 'Yujie Han'}, {'name': 'Zeyu Hua'}, {'name': 'Na Ruan'}, {'name': 'Weijia Jia'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Weijia Jia'}\n",
      "\n",
      "\n",
      "author\n",
      "Weijia Jia\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition 2021\n",
      "  (CVPR 2021)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2104.04480v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.04480v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2104.04111v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2104.04111v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-06-26T00:14:30Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=26, tm_hour=0, tm_min=14, tm_sec=30, tm_wday=5, tm_yday=177, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-04-08T23:02:56Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=8, tm_hour=23, tm_min=2, tm_sec=56, tm_wday=3, tm_yday=98, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Generalized Spoofing Detection Inspired from Audio Generation Artifacts\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Generalized Spoofing Detection Inspired from Audio Generation Artifacts'}\n",
      "\n",
      "\n",
      "summary\n",
      "State-of-the-art methods for audio generation suffer from fingerprint\n",
      "artifacts and repeated inconsistencies across temporal and spectral domains.\n",
      "Such artifacts could be well captured by the frequency domain analysis over the\n",
      "spectrogram. Thus, we propose a novel use of long-range spectro-temporal\n",
      "modulation feature -- 2D DCT over log-Mel spectrogram for the audio deepfake\n",
      "detection. We show that this feature works better than log-Mel spectrogram,\n",
      "CQCC, MFCC, as a suitable candidate to capture such artifacts. We employ\n",
      "spectrum augmentation and feature normalization to decrease overfitting and\n",
      "bridge the gap between training and test dataset along with this novel feature\n",
      "introduction. We developed a CNN-based baseline that achieved a 0.0849 t-DCF\n",
      "and outperformed the previously top single systems reported in the ASVspoof\n",
      "2019 challenge. Finally, by combining our baseline with our proposed 2D DCT\n",
      "spectro-temporal feature, we decrease the t-DCF score down by 14% to 0.0737,\n",
      "making it a state-of-the-art system for spoofing detection. Furthermore, we\n",
      "evaluate our model using two external datasets, showing the proposed feature's\n",
      "generalization ability. We also provide analysis and ablation studies for our\n",
      "proposed feature and results.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': \"State-of-the-art methods for audio generation suffer from fingerprint\\nartifacts and repeated inconsistencies across temporal and spectral domains.\\nSuch artifacts could be well captured by the frequency domain analysis over the\\nspectrogram. Thus, we propose a novel use of long-range spectro-temporal\\nmodulation feature -- 2D DCT over log-Mel spectrogram for the audio deepfake\\ndetection. We show that this feature works better than log-Mel spectrogram,\\nCQCC, MFCC, as a suitable candidate to capture such artifacts. We employ\\nspectrum augmentation and feature normalization to decrease overfitting and\\nbridge the gap between training and test dataset along with this novel feature\\nintroduction. We developed a CNN-based baseline that achieved a 0.0849 t-DCF\\nand outperformed the previously top single systems reported in the ASVspoof\\n2019 challenge. Finally, by combining our baseline with our proposed 2D DCT\\nspectro-temporal feature, we decrease the t-DCF score down by 14% to 0.0737,\\nmaking it a state-of-the-art system for spoofing detection. Furthermore, we\\nevaluate our model using two external datasets, showing the proposed feature's\\ngeneralization ability. We also provide analysis and ablation studies for our\\nproposed feature and results.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yang Gao'}, {'name': 'Tyler Vuong'}, {'name': 'Mahsa Elyasi'}, {'name': 'Gaurav Bharaj'}, {'name': 'Rita Singh'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Rita Singh'}\n",
      "\n",
      "\n",
      "author\n",
      "Rita Singh\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Camera ready version. Accepted by INTERSPEECH 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2104.04111v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.04111v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2104.03123v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2104.03123v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-06-30T11:03:10Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=30, tm_hour=11, tm_min=3, tm_sec=10, tm_wday=2, tm_yday=181, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-04-07T13:53:20Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=7, tm_hour=13, tm_min=53, tm_sec=20, tm_wday=2, tm_yday=97, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Partially-Connected Differentiable Architecture Search for Deepfake and\n",
      "  Spoofing Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'Partially-Connected Differentiable Architecture Search for Deepfake and\\n  Spoofing Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "This paper reports the first successful application of a differentiable\n",
      "architecture search (DARTS) approach to the deepfake and spoofing detection\n",
      "problems. An example of neural architecture search, DARTS operates upon a\n",
      "continuous, differentiable search space which enables both the architecture and\n",
      "parameters to be optimised via gradient descent. Solutions based on\n",
      "partially-connected DARTS use random channel masking in the search space to\n",
      "reduce GPU time and automatically learn and optimise complex neural\n",
      "architectures composed of convolutional operations and residual blocks. Despite\n",
      "being learned quickly with little human effort, the resulting networks are\n",
      "competitive with the best performing systems reported in the literature. Some\n",
      "are also far less complex, containing 85% fewer parameters than a Res2Net\n",
      "competitor.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100', 'value': 'This paper reports the first successful application of a differentiable\\narchitecture search (DARTS) approach to the deepfake and spoofing detection\\nproblems. An example of neural architecture search, DARTS operates upon a\\ncontinuous, differentiable search space which enables both the architecture and\\nparameters to be optimised via gradient descent. Solutions based on\\npartially-connected DARTS use random channel masking in the search space to\\nreduce GPU time and automatically learn and optimise complex neural\\narchitectures composed of convolutional operations and residual blocks. Despite\\nbeing learned quickly with little human effort, the resulting networks are\\ncompetitive with the best performing systems reported in the literature. Some\\nare also far less complex, containing 85% fewer parameters than a Res2Net\\ncompetitor.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Wanying Ge'}, {'name': 'Michele Panariello'}, {'name': 'Jose Patino'}, {'name': 'Massimiliano Todisco'}, {'name': 'Nicholas Evans'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Nicholas Evans'}\n",
      "\n",
      "\n",
      "author\n",
      "Nicholas Evans\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to INTERSPEECH 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2104.03123v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.03123v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2104.05418v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2104.05418v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-27T21:30:05Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=27, tm_hour=21, tm_min=30, tm_sec=5, tm_wday=2, tm_yday=300, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-04-07T07:35:08Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=7, tm_hour=7, tm_min=35, tm_sec=8, tm_wday=2, tm_yday=97, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Contrastive Learning of Global-Local Video Representations\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Contrastive Learning of Global-Local Video Representations'}\n",
      "\n",
      "\n",
      "summary\n",
      "Contrastive learning has delivered impressive results for various tasks in\n",
      "the self-supervised regime. However, existing approaches optimize for learning\n",
      "representations specific to downstream scenarios, i.e., \\textit{global}\n",
      "representations suitable for tasks such as classification or \\textit{local}\n",
      "representations for tasks such as detection and localization. While they\n",
      "produce satisfactory results in the intended downstream scenarios, they often\n",
      "fail to generalize to tasks that they were not originally designed for. In this\n",
      "work, we propose to learn video representations that generalize to both the\n",
      "tasks which require global semantic information (e.g., classification) and the\n",
      "tasks that require local fine-grained spatio-temporal information (e.g.,\n",
      "localization). We achieve this by optimizing two contrastive objectives that\n",
      "together encourage our model to learn global-local visual information given\n",
      "audio signals. We show that the two objectives mutually improve the\n",
      "generalizability of the learned global-local representations, significantly\n",
      "outperforming their disjointly learned counterparts. We demonstrate our\n",
      "approach on various tasks including action/sound classification, lip reading,\n",
      "deepfake detection, event and sound localization\n",
      "(https://github.com/yunyikristy/global\\_local).\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Contrastive learning has delivered impressive results for various tasks in\\nthe self-supervised regime. However, existing approaches optimize for learning\\nrepresentations specific to downstream scenarios, i.e., \\\\textit{global}\\nrepresentations suitable for tasks such as classification or \\\\textit{local}\\nrepresentations for tasks such as detection and localization. While they\\nproduce satisfactory results in the intended downstream scenarios, they often\\nfail to generalize to tasks that they were not originally designed for. In this\\nwork, we propose to learn video representations that generalize to both the\\ntasks which require global semantic information (e.g., classification) and the\\ntasks that require local fine-grained spatio-temporal information (e.g.,\\nlocalization). We achieve this by optimizing two contrastive objectives that\\ntogether encourage our model to learn global-local visual information given\\naudio signals. We show that the two objectives mutually improve the\\ngeneralizability of the learned global-local representations, significantly\\noutperforming their disjointly learned counterparts. We demonstrate our\\napproach on various tasks including action/sound classification, lip reading,\\ndeepfake detection, event and sound localization\\n(https://github.com/yunyikristy/global\\\\_local).'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shuang Ma'}, {'name': 'Zhaoyang Zeng'}, {'name': 'Daniel McDuff'}, {'name': 'Yale Song'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yale Song'}\n",
      "\n",
      "\n",
      "author\n",
      "Yale Song\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2104.05418v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.05418v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2104.02821v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2104.02821v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-11-03T20:49:28Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=3, tm_hour=20, tm_min=49, tm_sec=28, tm_wday=2, tm_yday=307, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-04-06T22:48:22Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=6, tm_hour=22, tm_min=48, tm_sec=22, tm_wday=1, tm_yday=96, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Towards Measuring Fairness in AI: the Casual Conversations Dataset\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Towards Measuring Fairness in AI: the Casual Conversations Dataset'}\n",
      "\n",
      "\n",
      "summary\n",
      "This paper introduces a novel dataset to help researchers evaluate their\n",
      "computer vision and audio models for accuracy across a diverse set of age,\n",
      "genders, apparent skin tones and ambient lighting conditions. Our dataset is\n",
      "composed of 3,011 subjects and contains over 45,000 videos, with an average of\n",
      "15 videos per person. The videos were recorded in multiple U.S. states with a\n",
      "diverse set of adults in various age, gender and apparent skin tone groups. A\n",
      "key feature is that each subject agreed to participate for their likenesses to\n",
      "be used. Additionally, our age and gender annotations are provided by the\n",
      "subjects themselves. A group of trained annotators labeled the subjects'\n",
      "apparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations\n",
      "for videos recorded in low ambient lighting are also provided. As an\n",
      "application to measure robustness of predictions across certain attributes, we\n",
      "provide a comprehensive study on the top five winners of the DeepFake Detection\n",
      "Challenge (DFDC). Experimental evaluation shows that the winning models are\n",
      "less performant on some specific groups of people, such as subjects with darker\n",
      "skin tones and thus may not generalize to all people. In addition, we also\n",
      "evaluate the state-of-the-art apparent age and gender classification methods.\n",
      "Our experiments provides a thorough analysis on these models in terms of fair\n",
      "treatment of people from various backgrounds.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"This paper introduces a novel dataset to help researchers evaluate their\\ncomputer vision and audio models for accuracy across a diverse set of age,\\ngenders, apparent skin tones and ambient lighting conditions. Our dataset is\\ncomposed of 3,011 subjects and contains over 45,000 videos, with an average of\\n15 videos per person. The videos were recorded in multiple U.S. states with a\\ndiverse set of adults in various age, gender and apparent skin tone groups. A\\nkey feature is that each subject agreed to participate for their likenesses to\\nbe used. Additionally, our age and gender annotations are provided by the\\nsubjects themselves. A group of trained annotators labeled the subjects'\\napparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations\\nfor videos recorded in low ambient lighting are also provided. As an\\napplication to measure robustness of predictions across certain attributes, we\\nprovide a comprehensive study on the top five winners of the DeepFake Detection\\nChallenge (DFDC). Experimental evaluation shows that the winning models are\\nless performant on some specific groups of people, such as subjects with darker\\nskin tones and thus may not generalize to all people. In addition, we also\\nevaluate the state-of-the-art apparent age and gender classification methods.\\nOur experiments provides a thorough analysis on these models in terms of fair\\ntreatment of people from various backgrounds.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Caner Hazirbas'}, {'name': 'Joanna Bitton'}, {'name': 'Brian Dolhansky'}, {'name': 'Jacqueline Pan'}, {'name': 'Albert Gordo'}, {'name': 'Cristian Canton Ferrer'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Cristian Canton Ferrer'}\n",
      "\n",
      "\n",
      "author\n",
      "Cristian Canton Ferrer\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2104.02821v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.02821v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2104.01353v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2104.01353v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-04-03T09:13:05Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=3, tm_hour=9, tm_min=13, tm_sec=5, tm_wday=5, tm_yday=93, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-04-03T09:13:05Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=3, tm_hour=9, tm_min=13, tm_sec=5, tm_wday=5, tm_yday=93, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Detection Scheme Based on Vision Transformer and Distillation\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake Detection Scheme Based on Vision Transformer and Distillation'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake is the manipulated video made with a generative deep learning\n",
      "technique such as Generative Adversarial Networks (GANs) or Auto Encoder that\n",
      "anyone can utilize. Recently, with the increase of Deepfake videos, some\n",
      "classifiers consisting of the convolutional neural network that can distinguish\n",
      "fake videos as well as deepfake datasets have been actively created. However,\n",
      "the previous studies based on the CNN structure have the problem of not only\n",
      "overfitting, but also considerable misjudging fake video as real ones. In this\n",
      "paper, we propose a Vision Transformer model with distillation methodology for\n",
      "detecting fake videos. We design that a CNN features and patch-based\n",
      "positioning model learns to interact with all positions to find the artifact\n",
      "region for solving false negative problem. Through comparative analysis on\n",
      "Deepfake Detection (DFDC) Dataset, we verify that the proposed scheme with\n",
      "patch embedding as input outperforms the state-of-the-art using the combined\n",
      "CNN features. Without ensemble technique, our model obtains 0.978 of AUC and\n",
      "91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1\n",
      "score on the same condition.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake is the manipulated video made with a generative deep learning\\ntechnique such as Generative Adversarial Networks (GANs) or Auto Encoder that\\nanyone can utilize. Recently, with the increase of Deepfake videos, some\\nclassifiers consisting of the convolutional neural network that can distinguish\\nfake videos as well as deepfake datasets have been actively created. However,\\nthe previous studies based on the CNN structure have the problem of not only\\noverfitting, but also considerable misjudging fake video as real ones. In this\\npaper, we propose a Vision Transformer model with distillation methodology for\\ndetecting fake videos. We design that a CNN features and patch-based\\npositioning model learns to interact with all positions to find the artifact\\nregion for solving false negative problem. Through comparative analysis on\\nDeepfake Detection (DFDC) Dataset, we verify that the proposed scheme with\\npatch embedding as input outperforms the state-of-the-art using the combined\\nCNN features. Without ensemble technique, our model obtains 0.978 of AUC and\\n91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1\\nscore on the same condition.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Young-Jin Heo'}, {'name': 'Young-Ju Choi'}, {'name': 'Young-Woon Lee'}, {'name': 'Byung-Gyu Kim'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Byung-Gyu Kim'}\n",
      "\n",
      "\n",
      "author\n",
      "Byung-Gyu Kim\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "7 pages, 5 figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2104.01353v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.01353v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.14211v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.14211v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-26T01:57:04Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=26, tm_hour=1, tm_min=57, tm_sec=4, tm_wday=4, tm_yday=85, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-03-26T01:57:04Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=26, tm_hour=1, tm_min=57, tm_sec=4, tm_wday=4, tm_yday=85, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes raised serious concerns on the authenticity of visual contents.\n",
      "Prior works revealed the possibility to disrupt deepfakes by adding adversarial\n",
      "perturbations to the source data, but we argue that the threat has not been\n",
      "eliminated yet. This paper presents MagDR, a mask-guided detection and\n",
      "reconstruction pipeline for defending deepfakes from adversarial attacks. MagDR\n",
      "starts with a detection module that defines a few criteria to judge the\n",
      "abnormality of the output of deepfakes, and then uses it to guide a learnable\n",
      "reconstruction procedure. Adaptive masks are extracted to capture the change in\n",
      "local facial regions. In experiments, MagDR defends three main tasks of\n",
      "deepfakes, and the learned reconstruction pipeline transfers across input data,\n",
      "showing promising performance in defending both black-box and white-box\n",
      "attacks.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfakes raised serious concerns on the authenticity of visual contents.\\nPrior works revealed the possibility to disrupt deepfakes by adding adversarial\\nperturbations to the source data, but we argue that the threat has not been\\neliminated yet. This paper presents MagDR, a mask-guided detection and\\nreconstruction pipeline for defending deepfakes from adversarial attacks. MagDR\\nstarts with a detection module that defines a few criteria to judge the\\nabnormality of the output of deepfakes, and then uses it to guide a learnable\\nreconstruction procedure. Adaptive masks are extracted to capture the change in\\nlocal facial regions. In experiments, MagDR defends three main tasks of\\ndeepfakes, and the learned reconstruction pipeline transfers across input data,\\nshowing promising performance in defending both black-box and white-box\\nattacks.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Zhikai Chen'}, {'name': 'Lingxi Xie'}, {'name': 'Shanmin Pang'}, {'name': 'Yong He'}, {'name': 'Bo Zhang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Bo Zhang'}\n",
      "\n",
      "\n",
      "author\n",
      "Bo Zhang\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to CVPR2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.14211v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.14211v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.13567v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.13567v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-25T02:20:08Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=25, tm_hour=2, tm_min=20, tm_sec=8, tm_wday=3, tm_yday=84, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-03-25T02:20:08Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=25, tm_hour=2, tm_min=20, tm_sec=8, tm_wday=3, tm_yday=84, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Forensics via An Adversarial Game\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake Forensics via An Adversarial Game'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the progress in AI-based facial forgery (i.e., deepfake), people are\n",
      "increasingly concerned about its abuse. Albeit effort has been made for\n",
      "training classification (also known as deepfake detection) models to recognize\n",
      "such forgeries, existing models suffer from poor generalization to unseen\n",
      "forgery technologies and high sensitivity to changes in image/video quality. In\n",
      "this paper, we advocate adversarial training for improving the generalization\n",
      "ability to both unseen facial forgeries and unseen image/video qualities. We\n",
      "believe training with samples that are adversarially crafted to attack the\n",
      "classification models improves the generalization ability considerably.\n",
      "Considering that AI-based face manipulation often leads to high-frequency\n",
      "artifacts that can be easily spotted by models yet difficult to generalize, we\n",
      "further propose a new adversarial training method that attempts to blur out\n",
      "these specific artifacts, by introducing pixel-wise Gaussian blurring models.\n",
      "With adversarial training, the classification models are forced to learn more\n",
      "discriminative and generalizable features, and the effectiveness of our method\n",
      "can be verified by plenty of empirical evidence. Our code will be made publicly\n",
      "available.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'With the progress in AI-based facial forgery (i.e., deepfake), people are\\nincreasingly concerned about its abuse. Albeit effort has been made for\\ntraining classification (also known as deepfake detection) models to recognize\\nsuch forgeries, existing models suffer from poor generalization to unseen\\nforgery technologies and high sensitivity to changes in image/video quality. In\\nthis paper, we advocate adversarial training for improving the generalization\\nability to both unseen facial forgeries and unseen image/video qualities. We\\nbelieve training with samples that are adversarially crafted to attack the\\nclassification models improves the generalization ability considerably.\\nConsidering that AI-based face manipulation often leads to high-frequency\\nartifacts that can be easily spotted by models yet difficult to generalize, we\\nfurther propose a new adversarial training method that attempts to blur out\\nthese specific artifacts, by introducing pixel-wise Gaussian blurring models.\\nWith adversarial training, the classification models are forced to learn more\\ndiscriminative and generalizable features, and the effectiveness of our method\\ncan be verified by plenty of empirical evidence. Our code will be made publicly\\navailable.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Zhi Wang'}, {'name': 'Yiwen Guo'}, {'name': 'Wangmeng Zuo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Wangmeng Zuo'}\n",
      "\n",
      "\n",
      "author\n",
      "Wangmeng Zuo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "12 pages, 4 figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.13567v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.13567v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.10094v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.10094v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-23T12:00:25Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=23, tm_hour=12, tm_min=0, tm_sec=25, tm_wday=0, tm_yday=235, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-03-18T09:04:02Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=18, tm_hour=9, tm_min=4, tm_sec=2, tm_wday=3, tm_yday=77, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "KoDF: A Large-scale Korean DeepFake Detection Dataset\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'KoDF: A Large-scale Korean DeepFake Detection Dataset'}\n",
      "\n",
      "\n",
      "summary\n",
      "A variety of effective face-swap and face-reenactment methods have been\n",
      "publicized in recent years, democratizing the face synthesis technology to a\n",
      "great extent. Videos generated as such have come to be called deepfakes with a\n",
      "negative connotation, for various social problems they have caused. Facing the\n",
      "emerging threat of deepfakes, we have built the Korean DeepFake Detection\n",
      "Dataset (KoDF), a large-scale collection of synthesized and real videos focused\n",
      "on Korean subjects. In this paper, we provide a detailed description of methods\n",
      "used to construct the dataset, experimentally show the discrepancy between the\n",
      "distributions of KoDF and existing deepfake detection datasets, and underline\n",
      "the importance of using multiple datasets for real-world generalization. KoDF\n",
      "is publicly available at https://moneybrain-research.github.io/kodf in its\n",
      "entirety (i.e. real clips, synthesized clips, clips with adversarial attack,\n",
      "and metadata).\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'A variety of effective face-swap and face-reenactment methods have been\\npublicized in recent years, democratizing the face synthesis technology to a\\ngreat extent. Videos generated as such have come to be called deepfakes with a\\nnegative connotation, for various social problems they have caused. Facing the\\nemerging threat of deepfakes, we have built the Korean DeepFake Detection\\nDataset (KoDF), a large-scale collection of synthesized and real videos focused\\non Korean subjects. In this paper, we provide a detailed description of methods\\nused to construct the dataset, experimentally show the discrepancy between the\\ndistributions of KoDF and existing deepfake detection datasets, and underline\\nthe importance of using multiple datasets for real-world generalization. KoDF\\nis publicly available at https://moneybrain-research.github.io/kodf in its\\nentirety (i.e. real clips, synthesized clips, clips with adversarial attack,\\nand metadata).'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Patrick Kwon'}, {'name': 'Jaeseong You'}, {'name': 'Gyuhyeon Nam'}, {'name': 'Sungwoo Park'}, {'name': 'Gyeongsu Chae'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Gyeongsu Chae'}\n",
      "\n",
      "\n",
      "author\n",
      "Gyeongsu Chae\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to ICCV 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.10094v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.10094v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.09396v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.09396v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-10-03T01:05:56Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=10, tm_mday=3, tm_hour=1, tm_min=5, tm_sec=56, tm_wday=6, tm_yday=276, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-03-17T01:48:34Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=17, tm_hour=1, tm_min=48, tm_sec=34, tm_wday=2, tm_yday=76, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Pros and Cons of GAN Evaluation Measures: New Developments\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Pros and Cons of GAN Evaluation Measures: New Developments'}\n",
      "\n",
      "\n",
      "summary\n",
      "This work is an update of a previous paper on the same topic published a few\n",
      "years ago. With the dramatic progress in generative modeling, a suite of new\n",
      "quantitative and qualitative techniques to evaluate models has emerged.\n",
      "Although some measures such as Inception Score, Frechet Inception Distance,\n",
      "Precision-Recall, and Perceptual Path Length are relatively more popular, GAN\n",
      "evaluation is not a settled issue and there is still room for improvement.\n",
      "Here, I describe new dimensions that are becoming important in assessing models\n",
      "(e.g. bias and fairness) and discuss the connection between GAN evaluation and\n",
      "deepfakes. These are important areas of concern in the machine learning\n",
      "community today and progress in GAN evaluation can help mitigate them.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'This work is an update of a previous paper on the same topic published a few\\nyears ago. With the dramatic progress in generative modeling, a suite of new\\nquantitative and qualitative techniques to evaluate models has emerged.\\nAlthough some measures such as Inception Score, Frechet Inception Distance,\\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\\nevaluation is not a settled issue and there is still room for improvement.\\nHere, I describe new dimensions that are becoming important in assessing models\\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\\ndeepfakes. These are important areas of concern in the machine learning\\ncommunity today and progress in GAN evaluation can help mitigate them.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ali Borji'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ali Borji'}\n",
      "\n",
      "\n",
      "author\n",
      "Ali Borji\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "NA\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.09396v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.09396v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.06929v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.06929v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-11T20:01:30Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=11, tm_hour=20, tm_min=1, tm_sec=30, tm_wday=3, tm_yday=70, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-03-11T20:01:30Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=11, tm_hour=20, tm_min=1, tm_sec=30, tm_wday=3, tm_yday=70, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DefakeHop: A Light-Weight High-Performance Deepfake Detector\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DefakeHop: A Light-Weight High-Performance Deepfake Detector'}\n",
      "\n",
      "\n",
      "summary\n",
      "A light-weight high-performance Deepfake detection method, called DefakeHop,\n",
      "is proposed in this work. State-of-the-art Deepfake detection methods are built\n",
      "upon deep neural networks. DefakeHop extracts features automatically using the\n",
      "successive subspace learning (SSL) principle from various parts of face images.\n",
      "The features are extracted by c/w Saab transform and further processed by our\n",
      "feature distillation module using spatial dimension reduction and soft\n",
      "classification for each channel to get a more concise description of the face.\n",
      "Extensive experiments are conducted to demonstrate the effectiveness of the\n",
      "proposed DefakeHop method. With a small model size of 42,845 parameters,\n",
      "DefakeHop achieves state-of-the-art performance with the area under the ROC\n",
      "curve (AUC) of 100%, 94.95%, and 90.56% on UADFV, Celeb-DF v1 and Celeb-DF v2\n",
      "datasets, respectively.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'A light-weight high-performance Deepfake detection method, called DefakeHop,\\nis proposed in this work. State-of-the-art Deepfake detection methods are built\\nupon deep neural networks. DefakeHop extracts features automatically using the\\nsuccessive subspace learning (SSL) principle from various parts of face images.\\nThe features are extracted by c/w Saab transform and further processed by our\\nfeature distillation module using spatial dimension reduction and soft\\nclassification for each channel to get a more concise description of the face.\\nExtensive experiments are conducted to demonstrate the effectiveness of the\\nproposed DefakeHop method. With a small model size of 42,845 parameters,\\nDefakeHop achieves state-of-the-art performance with the area under the ROC\\ncurve (AUC) of 100%, 94.95%, and 90.56% on UADFV, Celeb-DF v1 and Celeb-DF v2\\ndatasets, respectively.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hong-Shuo Chen'}, {'name': 'Mozhdeh Rouhsedaghat'}, {'name': 'Hamza Ghani'}, {'name': 'Shuowen Hu'}, {'name': 'Suya You'}, {'name': 'C. -C. Jay Kuo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'C. -C. Jay Kuo'}\n",
      "\n",
      "\n",
      "author\n",
      "C. -C. Jay Kuo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted at ICME 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.06929v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.06929v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.04263v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.04263v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-11T01:08:38Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=11, tm_hour=1, tm_min=8, tm_sec=38, tm_wday=3, tm_yday=70, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-03-07T04:40:15Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=7, tm_hour=4, tm_min=40, tm_sec=15, tm_wday=6, tm_yday=66, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Videos in the Wild: Analysis and Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake Videos in the Wild: Analysis and Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "AI-manipulated videos, commonly known as deepfakes, are an emerging problem.\n",
      "Recently, researchers in academia and industry have contributed several\n",
      "(self-created) benchmark deepfake datasets, and deepfake detection algorithms.\n",
      "However, little effort has gone towards understanding deepfake videos in the\n",
      "wild, leading to a limited understanding of the real-world applicability of\n",
      "research contributions in this space. Even if detection schemes are shown to\n",
      "perform well on existing datasets, it is unclear how well the methods\n",
      "generalize to real-world deepfakes. To bridge this gap in knowledge, we make\n",
      "the following contributions: First, we collect and present the largest dataset\n",
      "of deepfake videos in the wild, containing 1,869 videos from YouTube and\n",
      "Bilibili, and extract over 4.8M frames of content. Second, we present a\n",
      "comprehensive analysis of the growth patterns, popularity, creators,\n",
      "manipulation strategies, and production methods of deepfake content in the\n",
      "real-world. Third, we systematically evaluate existing defenses using our new\n",
      "dataset, and observe that they are not ready for deployment in the real-world.\n",
      "Fourth, we explore the potential for transfer learning schemes and\n",
      "competition-winning techniques to improve defenses.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'AI-manipulated videos, commonly known as deepfakes, are an emerging problem.\\nRecently, researchers in academia and industry have contributed several\\n(self-created) benchmark deepfake datasets, and deepfake detection algorithms.\\nHowever, little effort has gone towards understanding deepfake videos in the\\nwild, leading to a limited understanding of the real-world applicability of\\nresearch contributions in this space. Even if detection schemes are shown to\\nperform well on existing datasets, it is unclear how well the methods\\ngeneralize to real-world deepfakes. To bridge this gap in knowledge, we make\\nthe following contributions: First, we collect and present the largest dataset\\nof deepfake videos in the wild, containing 1,869 videos from YouTube and\\nBilibili, and extract over 4.8M frames of content. Second, we present a\\ncomprehensive analysis of the growth patterns, popularity, creators,\\nmanipulation strategies, and production methods of deepfake content in the\\nreal-world. Third, we systematically evaluate existing defenses using our new\\ndataset, and observe that they are not ready for deployment in the real-world.\\nFourth, we explore the potential for transfer learning schemes and\\ncompetition-winning techniques to improve defenses.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Jiameng Pu'}, {'name': 'Neal Mangaokar'}, {'name': 'Lauren Kelly'}, {'name': 'Parantapa Bhattacharya'}, {'name': 'Kavya Sundaram'}, {'name': 'Mobin Javed'}, {'name': 'Bolun Wang'}, {'name': 'Bimal Viswanath'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Bimal Viswanath'}\n",
      "\n",
      "\n",
      "author\n",
      "Bimal Viswanath\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to The Web Conference 2021; First two authors contributed\n",
      "  equally to this work; 12 pages, 6 tables\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.04263v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.04263v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.02406v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.02406v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-08T13:10:36Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=8, tm_hour=13, tm_min=10, tm_sec=36, tm_wday=0, tm_yday=67, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-03-03T13:56:14Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=3, tm_hour=13, tm_min=56, tm_sec=14, tm_wday=2, tm_yday=62, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Multi-attentional Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Multi-attentional Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Face forgery by deepfake is widely spread over the internet and has raised\n",
      "severe societal concerns. Recently, how to detect such forgery contents has\n",
      "become a hot research topic and many deepfake detection methods have been\n",
      "proposed. Most of them model deepfake detection as a vanilla binary\n",
      "classification problem, i.e, first use a backbone network to extract a global\n",
      "feature and then feed it into a binary classifier (real/fake). But since the\n",
      "difference between the real and fake images in this task is often subtle and\n",
      "local, we argue this vanilla solution is not optimal. In this paper, we instead\n",
      "formulate deepfake detection as a fine-grained classification problem and\n",
      "propose a new multi-attentional deepfake detection network. Specifically, it\n",
      "consists of three key components: 1) multiple spatial attention heads to make\n",
      "the network attend to different local parts; 2) textural feature enhancement\n",
      "block to zoom in the subtle artifacts in shallow features; 3) aggregate the\n",
      "low-level textural feature and high-level semantic features guided by the\n",
      "attention maps. Moreover, to address the learning difficulty of this network,\n",
      "we further introduce a new regional independence loss and an attention guided\n",
      "data augmentation strategy. Through extensive experiments on different\n",
      "datasets, we demonstrate the superiority of our method over the vanilla binary\n",
      "classifier counterparts, and achieve state-of-the-art performance.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Face forgery by deepfake is widely spread over the internet and has raised\\nsevere societal concerns. Recently, how to detect such forgery contents has\\nbecome a hot research topic and many deepfake detection methods have been\\nproposed. Most of them model deepfake detection as a vanilla binary\\nclassification problem, i.e, first use a backbone network to extract a global\\nfeature and then feed it into a binary classifier (real/fake). But since the\\ndifference between the real and fake images in this task is often subtle and\\nlocal, we argue this vanilla solution is not optimal. In this paper, we instead\\nformulate deepfake detection as a fine-grained classification problem and\\npropose a new multi-attentional deepfake detection network. Specifically, it\\nconsists of three key components: 1) multiple spatial attention heads to make\\nthe network attend to different local parts; 2) textural feature enhancement\\nblock to zoom in the subtle artifacts in shallow features; 3) aggregate the\\nlow-level textural feature and high-level semantic features guided by the\\nattention maps. Moreover, to address the learning difficulty of this network,\\nwe further introduce a new regional independence loss and an attention guided\\ndata augmentation strategy. Through extensive experiments on different\\ndatasets, we demonstrate the superiority of our method over the vanilla binary\\nclassifier counterparts, and achieve state-of-the-art performance.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hanqing Zhao'}, {'name': 'Wenbo Zhou'}, {'name': 'Dongdong Chen'}, {'name': 'Tianyi Wei'}, {'name': 'Weiming Zhang'}, {'name': 'Nenghai Yu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Nenghai Yu'}\n",
      "\n",
      "\n",
      "author\n",
      "Nenghai Yu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "CVPR2021 preview\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.02406v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.02406v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.02018v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.02018v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-02T20:45:33Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=2, tm_hour=20, tm_min=45, tm_sec=33, tm_wday=1, tm_yday=61, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-03-02T20:45:33Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=2, tm_hour=20, tm_min=45, tm_sec=33, tm_wday=1, tm_yday=61, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFake-o-meter: An Open Platform for DeepFake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFake-o-meter: An Open Platform for DeepFake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "In recent years, the advent of deep learning-based techniques and the\n",
      "significant reduction in the cost of computation resulted in the feasibility of\n",
      "creating realistic videos of human faces, commonly known as DeepFakes. The\n",
      "availability of open-source tools to create DeepFakes poses as a threat to the\n",
      "trustworthiness of the online media. In this work, we develop an open-source\n",
      "online platform, known as DeepFake-o-meter, that integrates state-of-the-art\n",
      "DeepFake detection methods and provide a convenient interface for the users. We\n",
      "describe the design and function of DeepFake-o-meter in this work.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In recent years, the advent of deep learning-based techniques and the\\nsignificant reduction in the cost of computation resulted in the feasibility of\\ncreating realistic videos of human faces, commonly known as DeepFakes. The\\navailability of open-source tools to create DeepFakes poses as a threat to the\\ntrustworthiness of the online media. In this work, we develop an open-source\\nonline platform, known as DeepFake-o-meter, that integrates state-of-the-art\\nDeepFake detection methods and provide a convenient interface for the users. We\\ndescribe the design and function of DeepFake-o-meter in this work.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yuezun Li'}, {'name': 'Cong Zhang'}, {'name': 'Pu Sun'}, {'name': 'Honggang Qi'}, {'name': 'Siwei Lyu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Siwei Lyu'}\n",
      "\n",
      "\n",
      "author\n",
      "Siwei Lyu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "submitted to SAPDE 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.02018v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.02018v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.00847v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.00847v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-02T07:56:46Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=2, tm_hour=7, tm_min=56, tm_sec=46, tm_wday=1, tm_yday=61, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-03-01T08:40:10Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=1, tm_hour=8, tm_min=40, tm_sec=10, tm_wday=0, tm_yday=60, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web\n",
      "  APIs under Deepfake Impersonation Attack\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web\\n  APIs under Deepfake Impersonation Attack'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recently, significant advancements have been made in face recognition\n",
      "technologies using Deep Neural Networks. As a result, companies such as\n",
      "Microsoft, Amazon, and Naver offer highly accurate commercial face recognition\n",
      "web services for diverse applications to meet the end-user needs. Naturally,\n",
      "however, such technologies are threatened persistently, as virtually any\n",
      "individual can quickly implement impersonation attacks. In particular, these\n",
      "attacks can be a significant threat for authentication and identification\n",
      "services, which heavily rely on their underlying face recognition technologies'\n",
      "accuracy and robustness. Despite its gravity, the issue regarding deepfake\n",
      "abuse using commercial web APIs and their robustness has not yet been\n",
      "thoroughly investigated. This work provides a measurement study on the\n",
      "robustness of black-box commercial face recognition APIs against Deepfake\n",
      "Impersonation (DI) attacks using celebrity recognition APIs as an example case\n",
      "study. We use five deepfake datasets, two of which are created by us and\n",
      "planned to be released. More specifically, we measure attack performance based\n",
      "on two scenarios (targeted and non-targeted) and further analyze the differing\n",
      "system behaviors using fidelity, confidence, and similarity metrics.\n",
      "Accordingly, we demonstrate how vulnerable face recognition technologies from\n",
      "popular companies are to DI attack, achieving maximum success rates of 78.0%\n",
      "and 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with\n",
      "any celebrity) attacks, respectively. Moreover, we propose practical defense\n",
      "strategies to mitigate DI attacks, reducing the attack success rates to as low\n",
      "as 0% and 0.02% for targeted and non-targeted attacks, respectively.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"Recently, significant advancements have been made in face recognition\\ntechnologies using Deep Neural Networks. As a result, companies such as\\nMicrosoft, Amazon, and Naver offer highly accurate commercial face recognition\\nweb services for diverse applications to meet the end-user needs. Naturally,\\nhowever, such technologies are threatened persistently, as virtually any\\nindividual can quickly implement impersonation attacks. In particular, these\\nattacks can be a significant threat for authentication and identification\\nservices, which heavily rely on their underlying face recognition technologies'\\naccuracy and robustness. Despite its gravity, the issue regarding deepfake\\nabuse using commercial web APIs and their robustness has not yet been\\nthoroughly investigated. This work provides a measurement study on the\\nrobustness of black-box commercial face recognition APIs against Deepfake\\nImpersonation (DI) attacks using celebrity recognition APIs as an example case\\nstudy. We use five deepfake datasets, two of which are created by us and\\nplanned to be released. More specifically, we measure attack performance based\\non two scenarios (targeted and non-targeted) and further analyze the differing\\nsystem behaviors using fidelity, confidence, and similarity metrics.\\nAccordingly, we demonstrate how vulnerable face recognition technologies from\\npopular companies are to DI attack, achieving maximum success rates of 78.0%\\nand 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with\\nany celebrity) attacks, respectively. Moreover, we propose practical defense\\nstrategies to mitigate DI attacks, reducing the attack success rates to as low\\nas 0% and 0.02% for targeted and non-targeted attacks, respectively.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shahroz Tariq'}, {'name': 'Sowon Jeon'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "27 pages, preprint\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.00847v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.00847v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.4.9; I.5.4; K.4.2', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.00218v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.00218v3\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-23T02:51:21Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=2, tm_min=51, tm_sec=21, tm_wday=2, tm_yday=82, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-02-27T13:48:54Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=27, tm_hour=13, tm_min=48, tm_sec=54, tm_wday=5, tm_yday=58, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Countering Malicious DeepFakes: Survey, Battleground, and Horizon\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Countering Malicious DeepFakes: Survey, Battleground, and Horizon'}\n",
      "\n",
      "\n",
      "summary\n",
      "The creation or manipulation of facial appearance through deep generative\n",
      "approaches, known as DeepFake, have achieved significant progress and promoted\n",
      "a wide range of benign and malicious applications, e.g., visual effect\n",
      "assistance in movie and misinformation generation by faking famous persons. The\n",
      "evil side of this new technique poses another popular study, i.e., DeepFake\n",
      "detection aiming to identify the fake faces from the real ones. With the rapid\n",
      "development of the DeepFake-related studies in the community, both sides have\n",
      "formed the relationship of battleground, pushing the improvements of each other\n",
      "and inspiring new directions, e.g., the evasion of DeepFake detection.\n",
      "Nevertheless, the overview of such battleground and the new direction is\n",
      "unclear and neglected by recent surveys due to the rapid increase of related\n",
      "publications, limiting the in-depth understanding of the tendency and future\n",
      "works. To fill this gap, in this paper, we provide a comprehensive overview and\n",
      "detailed analysis of the research work on the topic of DeepFake generation,\n",
      "DeepFake detection as well as evasion of DeepFake detection, with more than 318\n",
      "research papers carefully surveyed. We present the taxonomy of various DeepFake\n",
      "generation methods and the categorization of various DeepFake detection\n",
      "methods, and more importantly, we showcase the battleground between the two\n",
      "parties with detailed interactions between the adversaries (DeepFake\n",
      "generation) and the defenders (DeepFake detection). The battleground allows\n",
      "fresh perspective into the latest landscape of the DeepFake research and can\n",
      "provide valuable analysis towards the research challenges and opportunities as\n",
      "well as research trends and future directions. We also elaborately design\n",
      "interactive diagrams (http://www.xujuefei.com/dfsurvey) to allow researchers to\n",
      "explore their own interests on popular DeepFake generators or detectors.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The creation or manipulation of facial appearance through deep generative\\napproaches, known as DeepFake, have achieved significant progress and promoted\\na wide range of benign and malicious applications, e.g., visual effect\\nassistance in movie and misinformation generation by faking famous persons. The\\nevil side of this new technique poses another popular study, i.e., DeepFake\\ndetection aiming to identify the fake faces from the real ones. With the rapid\\ndevelopment of the DeepFake-related studies in the community, both sides have\\nformed the relationship of battleground, pushing the improvements of each other\\nand inspiring new directions, e.g., the evasion of DeepFake detection.\\nNevertheless, the overview of such battleground and the new direction is\\nunclear and neglected by recent surveys due to the rapid increase of related\\npublications, limiting the in-depth understanding of the tendency and future\\nworks. To fill this gap, in this paper, we provide a comprehensive overview and\\ndetailed analysis of the research work on the topic of DeepFake generation,\\nDeepFake detection as well as evasion of DeepFake detection, with more than 318\\nresearch papers carefully surveyed. We present the taxonomy of various DeepFake\\ngeneration methods and the categorization of various DeepFake detection\\nmethods, and more importantly, we showcase the battleground between the two\\nparties with detailed interactions between the adversaries (DeepFake\\ngeneration) and the defenders (DeepFake detection). The battleground allows\\nfresh perspective into the latest landscape of the DeepFake research and can\\nprovide valuable analysis towards the research challenges and opportunities as\\nwell as research trends and future directions. We also elaborately design\\ninteractive diagrams (http://www.xujuefei.com/dfsurvey) to allow researchers to\\nexplore their own interests on popular DeepFake generators or detectors.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Felix Juefei-Xu'}, {'name': 'Run Wang'}, {'name': 'Yihao Huang'}, {'name': 'Qing Guo'}, {'name': 'Lei Ma'}, {'name': 'Yang Liu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yang Liu'}\n",
      "\n",
      "\n",
      "author\n",
      "Yang Liu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "49 pages. To appear in the International Journal of Computer Vision\n",
      "  (IJCV), 2022\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.00218v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.00218v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2103.00484v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2103.00484v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-11-23T04:48:32Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=23, tm_hour=4, tm_min=48, tm_sec=32, tm_wday=1, tm_yday=327, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-02-25T18:26:50Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=25, tm_hour=18, tm_min=26, tm_sec=50, tm_wday=3, tm_yday=56, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfakes Generation and Detection: State-of-the-art, open challenges,\n",
      "  countermeasures, and way forward\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfakes Generation and Detection: State-of-the-art, open challenges,\\n  countermeasures, and way forward'}\n",
      "\n",
      "\n",
      "summary\n",
      "Easy access to audio-visual content on social media, combined with the\n",
      "availability of modern tools such as Tensorflow or Keras, open-source trained\n",
      "models, and economical computing infrastructure, and the rapid evolution of\n",
      "deep-learning (DL) methods, especially Generative Adversarial Networks (GAN),\n",
      "have made it possible to generate deepfakes to disseminate disinformation,\n",
      "revenge porn, financial frauds, hoaxes, and to disrupt government functioning.\n",
      "The existing surveys have mainly focused on the detection of deepfake images\n",
      "and videos. This paper provides a comprehensive review and detailed analysis of\n",
      "existing tools and machine learning (ML) based approaches for deepfake\n",
      "generation and the methodologies used to detect such manipulations for both\n",
      "audio and visual deepfakes. For each category of deepfake, we discuss\n",
      "information related to manipulation approaches, current public datasets, and\n",
      "key standards for the performance evaluation of deepfake detection techniques\n",
      "along with their results. Additionally, we also discuss open challenges and\n",
      "enumerate future directions to guide future researchers on issues that need to\n",
      "be considered to improve the domains of both deepfake generation and detection.\n",
      "This work is expected to assist the readers in understanding the creation and\n",
      "detection mechanisms of deepfakes, along with their current limitations and\n",
      "future direction.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Easy access to audio-visual content on social media, combined with the\\navailability of modern tools such as Tensorflow or Keras, open-source trained\\nmodels, and economical computing infrastructure, and the rapid evolution of\\ndeep-learning (DL) methods, especially Generative Adversarial Networks (GAN),\\nhave made it possible to generate deepfakes to disseminate disinformation,\\nrevenge porn, financial frauds, hoaxes, and to disrupt government functioning.\\nThe existing surveys have mainly focused on the detection of deepfake images\\nand videos. This paper provides a comprehensive review and detailed analysis of\\nexisting tools and machine learning (ML) based approaches for deepfake\\ngeneration and the methodologies used to detect such manipulations for both\\naudio and visual deepfakes. For each category of deepfake, we discuss\\ninformation related to manipulation approaches, current public datasets, and\\nkey standards for the performance evaluation of deepfake detection techniques\\nalong with their results. Additionally, we also discuss open challenges and\\nenumerate future directions to guide future researchers on issues that need to\\nbe considered to improve the domains of both deepfake generation and detection.\\nThis work is expected to assist the readers in understanding the creation and\\ndetection mechanisms of deepfakes, along with their current limitations and\\nfuture direction.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Momina Masood'}, {'name': 'Marriam Nawaz'}, {'name': 'Khalid Mahmood Malik'}, {'name': 'Ali Javed'}, {'name': 'Aun Irtaza'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Aun Irtaza'}\n",
      "\n",
      "\n",
      "author\n",
      "Aun Irtaza\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2103.00484v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.00484v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2102.11126v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2102.11126v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-11T13:45:17Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=11, tm_hour=13, tm_min=45, tm_sec=17, tm_wday=3, tm_yday=70, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-02-22T15:56:05Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=22, tm_hour=15, tm_min=56, tm_sec=5, tm_wday=0, tm_yday=53, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Video Detection Using Convolutional Vision Transformer\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake Video Detection Using Convolutional Vision Transformer'}\n",
      "\n",
      "\n",
      "summary\n",
      "The rapid advancement of deep learning models that can generate and synthesis\n",
      "hyper-realistic videos known as Deepfakes and their ease of access to the\n",
      "general public have raised concern from all concerned bodies to their possible\n",
      "malicious intent use. Deep learning techniques can now generate faces, swap\n",
      "faces between two subjects in a video, alter facial expressions, change gender,\n",
      "and alter facial features, to list a few. These powerful video manipulation\n",
      "methods have potential use in many fields. However, they also pose a looming\n",
      "threat to everyone if used for harmful purposes such as identity theft,\n",
      "phishing, and scam. In this work, we propose a Convolutional Vision Transformer\n",
      "for the detection of Deepfakes. The Convolutional Vision Transformer has two\n",
      "components: Convolutional Neural Network (CNN) and Vision Transformer (ViT).\n",
      "The CNN extracts learnable features while the ViT takes in the learned features\n",
      "as input and categorizes them using an attention mechanism. We trained our\n",
      "model on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5\n",
      "percent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our\n",
      "contribution is that we have added a CNN module to the ViT architecture and\n",
      "have achieved a competitive result on the DFDC dataset.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The rapid advancement of deep learning models that can generate and synthesis\\nhyper-realistic videos known as Deepfakes and their ease of access to the\\ngeneral public have raised concern from all concerned bodies to their possible\\nmalicious intent use. Deep learning techniques can now generate faces, swap\\nfaces between two subjects in a video, alter facial expressions, change gender,\\nand alter facial features, to list a few. These powerful video manipulation\\nmethods have potential use in many fields. However, they also pose a looming\\nthreat to everyone if used for harmful purposes such as identity theft,\\nphishing, and scam. In this work, we propose a Convolutional Vision Transformer\\nfor the detection of Deepfakes. The Convolutional Vision Transformer has two\\ncomponents: Convolutional Neural Network (CNN) and Vision Transformer (ViT).\\nThe CNN extracts learnable features while the ViT takes in the learned features\\nas input and categorizes them using an attention mechanism. We trained our\\nmodel on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5\\npercent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our\\ncontribution is that we have added a CNN module to the ViT architecture and\\nhave achieved a competitive result on the DFDC dataset.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Deressa Wodajo'}, {'name': 'Solomon Atnafu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Solomon Atnafu'}\n",
      "\n",
      "\n",
      "author\n",
      "Solomon Atnafu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "9 pages, 6 figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2102.11126v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2102.11126v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2102.09603v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2102.09603v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-25T22:02:04Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=25, tm_hour=22, tm_min=2, tm_sec=4, tm_wday=2, tm_yday=237, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-02-18T20:25:45Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=18, tm_hour=20, tm_min=25, tm_sec=45, tm_wday=3, tm_yday=49, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Towards Solving the DeepFake Problem : An Analysis on Improving DeepFake\n",
      "  Detection using Dynamic Face Augmentation\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Towards Solving the DeepFake Problem : An Analysis on Improving DeepFake\\n  Detection using Dynamic Face Augmentation'}\n",
      "\n",
      "\n",
      "summary\n",
      "The creation of altered and manipulated faces has become more common due to\n",
      "the improvement of DeepFake generation methods. Simultaneously, we have seen\n",
      "detection models' development for differentiating between a manipulated and\n",
      "original face from image or video content. In this paper, we focus on\n",
      "identifying the limitations and shortcomings of existing deepfake detection\n",
      "frameworks. We identified some key problems surrounding deepfake detection\n",
      "through quantitative and qualitative analysis of existing methods and datasets.\n",
      "We found that deepfake datasets are highly oversampled, causing models to\n",
      "become easily overfitted. The datasets are created using a small set of real\n",
      "faces to generate multiple fake samples. When trained on these datasets, models\n",
      "tend to memorize the actors' faces and labels instead of learning fake\n",
      "features. To mitigate this problem, we propose a simple data augmentation\n",
      "method termed Face-Cutout. Our method dynamically cuts out regions of an image\n",
      "using the face landmark information. It helps the model selectively attend to\n",
      "only the relevant regions of the input. Our evaluation experiments show that\n",
      "Face-Cutout can successfully improve the data variation and alleviate the\n",
      "problem of overfitting. Our method achieves a reduction in LogLoss of 15.2% to\n",
      "35.3% on different datasets, compared to other occlusion-based techniques.\n",
      "Moreover, we also propose a general-purpose data pre-processing guideline to\n",
      "train and evaluate existing architectures allowing us to improve the\n",
      "generalizability of these models for deepfake detection.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"The creation of altered and manipulated faces has become more common due to\\nthe improvement of DeepFake generation methods. Simultaneously, we have seen\\ndetection models' development for differentiating between a manipulated and\\noriginal face from image or video content. In this paper, we focus on\\nidentifying the limitations and shortcomings of existing deepfake detection\\nframeworks. We identified some key problems surrounding deepfake detection\\nthrough quantitative and qualitative analysis of existing methods and datasets.\\nWe found that deepfake datasets are highly oversampled, causing models to\\nbecome easily overfitted. The datasets are created using a small set of real\\nfaces to generate multiple fake samples. When trained on these datasets, models\\ntend to memorize the actors' faces and labels instead of learning fake\\nfeatures. To mitigate this problem, we propose a simple data augmentation\\nmethod termed Face-Cutout. Our method dynamically cuts out regions of an image\\nusing the face landmark information. It helps the model selectively attend to\\nonly the relevant regions of the input. Our evaluation experiments show that\\nFace-Cutout can successfully improve the data variation and alleviate the\\nproblem of overfitting. Our method achieves a reduction in LogLoss of 15.2% to\\n35.3% on different datasets, compared to other occlusion-based techniques.\\nMoreover, we also propose a general-purpose data pre-processing guideline to\\ntrain and evaluate existing architectures allowing us to improve the\\ngeneralizability of these models for deepfake detection.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sowmen Das'}, {'name': 'Selim Seferbekov'}, {'name': 'Arup Datta'}, {'name': 'Md. Saiful Islam'}, {'name': 'Md. Ruhul Amin'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Md. Ruhul Amin'}\n",
      "\n",
      "\n",
      "author\n",
      "Md. Ruhul Amin\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2102.09603v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2102.09603v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2102.08054v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2102.08054v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-02-16T10:05:11Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=16, tm_hour=10, tm_min=5, tm_sec=11, tm_wday=1, tm_yday=47, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-02-16T10:05:11Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=16, tm_hour=10, tm_min=5, tm_sec=11, tm_wday=1, tm_yday=47, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Could you become more credible by being White? Assessing Impact of Race\n",
      "  on Credibility with Deepfakes\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Could you become more credible by being White? Assessing Impact of Race\\n  on Credibility with Deepfakes'}\n",
      "\n",
      "\n",
      "summary\n",
      "Computer mediated conversations (e.g., videoconferencing) is now the new\n",
      "mainstream media. How would credibility be impacted if one could change their\n",
      "race on the fly in these environments? We propose an approach using Deepfakes\n",
      "and a supporting GAN architecture to isolate visual features and alter racial\n",
      "perception. We then crowd-sourced over 800 survey responses to measure how\n",
      "credibility was influenced by changing the perceived race. We evaluate the\n",
      "effect of showing a still image of a Black person versus a still image of a\n",
      "White person using the same audio clip for each survey. We also test the effect\n",
      "of showing either an original video or an altered video where the appearance of\n",
      "the person in the original video is modified to appear more White. We measure\n",
      "credibility as the percent of participant responses who believed the speaker\n",
      "was telling the truth. We found that changing the race of a person in a static\n",
      "image has negligible impact on credibility. However, the same manipulation of\n",
      "race on a video increases credibility significantly (61\\% to 73\\% with p $<$\n",
      "0.05). Furthermore, a VADER sentiment analysis over the free response survey\n",
      "questions reveals that more positive sentiment is used to justify the\n",
      "credibility of a White individual in a video.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Computer mediated conversations (e.g., videoconferencing) is now the new\\nmainstream media. How would credibility be impacted if one could change their\\nrace on the fly in these environments? We propose an approach using Deepfakes\\nand a supporting GAN architecture to isolate visual features and alter racial\\nperception. We then crowd-sourced over 800 survey responses to measure how\\ncredibility was influenced by changing the perceived race. We evaluate the\\neffect of showing a still image of a Black person versus a still image of a\\nWhite person using the same audio clip for each survey. We also test the effect\\nof showing either an original video or an altered video where the appearance of\\nthe person in the original video is modified to appear more White. We measure\\ncredibility as the percent of participant responses who believed the speaker\\nwas telling the truth. We found that changing the race of a person in a static\\nimage has negligible impact on credibility. However, the same manipulation of\\nrace on a video increases credibility significantly (61\\\\% to 73\\\\% with p $<$\\n0.05). Furthermore, a VADER sentiment analysis over the free response survey\\nquestions reveals that more positive sentiment is used to justify the\\ncredibility of a White individual in a video.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Kurtis Haut'}, {'name': 'Caleb Wohn'}, {'name': 'Victor Antony'}, {'name': 'Aidan Goldfarb'}, {'name': 'Melissa Welsh'}, {'name': 'Dillanie Sumanthiran'}, {'name': 'Ji-ze Jang'}, {'name': 'Md. Rafayet Ali'}, {'name': 'Ehsan Hoque'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ehsan Hoque'}\n",
      "\n",
      "\n",
      "author\n",
      "Ehsan Hoque\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "10 pages, 5 figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2102.08054v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2102.08054v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2102.06109v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2102.06109v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-02-11T16:44:09Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=11, tm_hour=16, tm_min=44, tm_sec=9, tm_wday=3, tm_yday=42, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-02-11T16:44:09Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=11, tm_hour=16, tm_min=44, tm_sec=9, tm_wday=3, tm_yday=42, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "The Deepfake Detection Dilemma: A Multistakeholder Exploration of\n",
      "  Adversarial Dynamics in Synthetic Media\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The Deepfake Detection Dilemma: A Multistakeholder Exploration of\\n  Adversarial Dynamics in Synthetic Media'}\n",
      "\n",
      "\n",
      "summary\n",
      "Synthetic media detection technologies label media as either synthetic or\n",
      "non-synthetic and are increasingly used by journalists, web platforms, and the\n",
      "general public to identify misinformation and other forms of problematic\n",
      "content. As both well-resourced organizations and the non-technical general\n",
      "public generate more sophisticated synthetic media, the capacity for purveyors\n",
      "of problematic content to adapt induces a \\newterm{detection dilemma}: as\n",
      "detection practices become more accessible, they become more easily\n",
      "circumvented. This paper describes how a multistakeholder cohort from academia,\n",
      "technology platforms, media entities, and civil society organizations active in\n",
      "synthetic media detection and its socio-technical implications evaluates the\n",
      "detection dilemma. Specifically, we offer an assessment of detection contexts\n",
      "and adversary capacities sourced from the broader, global AI and media\n",
      "integrity community concerned with mitigating the spread of harmful synthetic\n",
      "media. A collection of personas illustrates the intersection between\n",
      "unsophisticated and highly-resourced sponsors of misinformation in the context\n",
      "of their technical capacities. This work concludes that there is no \"best\"\n",
      "approach to navigating the detector dilemma, but derives a set of implications\n",
      "from multistakeholder input to better inform detection process decisions and\n",
      "policies, in practice.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Synthetic media detection technologies label media as either synthetic or\\nnon-synthetic and are increasingly used by journalists, web platforms, and the\\ngeneral public to identify misinformation and other forms of problematic\\ncontent. As both well-resourced organizations and the non-technical general\\npublic generate more sophisticated synthetic media, the capacity for purveyors\\nof problematic content to adapt induces a \\\\newterm{detection dilemma}: as\\ndetection practices become more accessible, they become more easily\\ncircumvented. This paper describes how a multistakeholder cohort from academia,\\ntechnology platforms, media entities, and civil society organizations active in\\nsynthetic media detection and its socio-technical implications evaluates the\\ndetection dilemma. Specifically, we offer an assessment of detection contexts\\nand adversary capacities sourced from the broader, global AI and media\\nintegrity community concerned with mitigating the spread of harmful synthetic\\nmedia. A collection of personas illustrates the intersection between\\nunsophisticated and highly-resourced sponsors of misinformation in the context\\nof their technical capacities. This work concludes that there is no \"best\"\\napproach to navigating the detector dilemma, but derives a set of implications\\nfrom multistakeholder input to better inform detection process decisions and\\npolicies, in practice.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Claire Leibowicz'}, {'name': 'Sean McGregor'}, {'name': 'Aviv Ovadya'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Aviv Ovadya'}\n",
      "\n",
      "\n",
      "author\n",
      "Aviv Ovadya\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "11 pages, 8 Figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2102.06109v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2102.06109v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2102.05950v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2102.05950v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-02-11T11:28:00Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=11, tm_hour=11, tm_min=28, tm_sec=0, tm_wday=3, tm_yday=42, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-02-11T11:28:00Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=11, tm_hour=11, tm_min=28, tm_sec=0, tm_wday=3, tm_yday=42, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Adversarially robust deepfake media detection using fused convolutional\n",
      "  neural network predictions\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Adversarially robust deepfake media detection using fused convolutional\\n  neural network predictions'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes are synthetically generated images, videos or audios, which\n",
      "fraudsters use to manipulate legitimate information. Current deepfake detection\n",
      "systems struggle against unseen data. To address this, we employ three\n",
      "different deep Convolutional Neural Network (CNN) models, (1) VGG16, (2)\n",
      "InceptionV3, and (3) XceptionNet to classify fake and real images extracted\n",
      "from videos. We also constructed a fusion of the deep CNN models to improve the\n",
      "robustness and generalisation capability. The proposed technique outperforms\n",
      "state-of-the-art models with 96.5% accuracy, when tested on publicly available\n",
      "DeepFake Detection Challenge (DFDC) test data, comprising of 400 videos. The\n",
      "fusion model achieves 99% accuracy on lower quality DeepFake-TIMIT dataset\n",
      "videos and 91.88% on higher quality DeepFake-TIMIT videos. In addition to this,\n",
      "we prove that prediction fusion is more robust against adversarial attacks. If\n",
      "one model is compromised by an adversarial attack, the prediction fusion does\n",
      "not let it affect the overall classification.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfakes are synthetically generated images, videos or audios, which\\nfraudsters use to manipulate legitimate information. Current deepfake detection\\nsystems struggle against unseen data. To address this, we employ three\\ndifferent deep Convolutional Neural Network (CNN) models, (1) VGG16, (2)\\nInceptionV3, and (3) XceptionNet to classify fake and real images extracted\\nfrom videos. We also constructed a fusion of the deep CNN models to improve the\\nrobustness and generalisation capability. The proposed technique outperforms\\nstate-of-the-art models with 96.5% accuracy, when tested on publicly available\\nDeepFake Detection Challenge (DFDC) test data, comprising of 400 videos. The\\nfusion model achieves 99% accuracy on lower quality DeepFake-TIMIT dataset\\nvideos and 91.88% on higher quality DeepFake-TIMIT videos. In addition to this,\\nwe prove that prediction fusion is more robust against adversarial attacks. If\\none model is compromised by an adversarial attack, the prediction fusion does\\nnot let it affect the overall classification.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sohail Ahmed Khan'}, {'name': 'Alessandro Artusi'}, {'name': 'Hang Dai'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Hang Dai'}\n",
      "\n",
      "\n",
      "author\n",
      "Hang Dai\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2102.05950v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2102.05950v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2102.00798v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2102.00798v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-02-01T12:27:08Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=1, tm_hour=12, tm_min=27, tm_sec=8, tm_wday=0, tm_yday=32, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-02-01T12:27:08Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=2, tm_mday=1, tm_hour=12, tm_min=27, tm_sec=8, tm_wday=0, tm_yday=32, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Landmark Breaker: Obstructing DeepFake By Disturbing Landmark Extraction\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Landmark Breaker: Obstructing DeepFake By Disturbing Landmark Extraction'}\n",
      "\n",
      "\n",
      "summary\n",
      "The recent development of Deep Neural Networks (DNN) has significantly\n",
      "increased the realism of AI-synthesized faces, with the most notable examples\n",
      "being the DeepFakes. The DeepFake technology can synthesize a face of target\n",
      "subject from a face of another subject, while retains the same face attributes.\n",
      "With the rapidly increased social media portals (Facebook, Instagram, etc),\n",
      "these realistic fake faces rapidly spread though the Internet, causing a broad\n",
      "negative impact to the society. In this paper, we describe Landmark Breaker,\n",
      "the first dedicated method to disrupt facial landmark extraction, and apply it\n",
      "to the obstruction of the generation of DeepFake videos.Our motivation is that\n",
      "disrupting the facial landmark extraction can affect the alignment of input\n",
      "face so as to degrade the DeepFake quality. Our method is achieved using\n",
      "adversarial perturbations. Compared to the detection methods that only work\n",
      "after DeepFake generation, Landmark Breaker goes one step ahead to prevent\n",
      "DeepFake generation. The experiments are conducted on three state-of-the-art\n",
      "facial landmark extractors using the recent Celeb-DF dataset.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The recent development of Deep Neural Networks (DNN) has significantly\\nincreased the realism of AI-synthesized faces, with the most notable examples\\nbeing the DeepFakes. The DeepFake technology can synthesize a face of target\\nsubject from a face of another subject, while retains the same face attributes.\\nWith the rapidly increased social media portals (Facebook, Instagram, etc),\\nthese realistic fake faces rapidly spread though the Internet, causing a broad\\nnegative impact to the society. In this paper, we describe Landmark Breaker,\\nthe first dedicated method to disrupt facial landmark extraction, and apply it\\nto the obstruction of the generation of DeepFake videos.Our motivation is that\\ndisrupting the facial landmark extraction can affect the alignment of input\\nface so as to degrade the DeepFake quality. Our method is achieved using\\nadversarial perturbations. Compared to the detection methods that only work\\nafter DeepFake generation, Landmark Breaker goes one step ahead to prevent\\nDeepFake generation. The experiments are conducted on three state-of-the-art\\nfacial landmark extractors using the recent Celeb-DF dataset.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Pu Sun'}, {'name': 'Yuezun Li'}, {'name': 'Honggang Qi'}, {'name': 'Siwei Lyu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Siwei Lyu'}\n",
      "\n",
      "\n",
      "author\n",
      "Siwei Lyu\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2102.00798v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2102.00798v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2101.11563v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2101.11563v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-01-27T17:37:23Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=27, tm_hour=17, tm_min=37, tm_sec=23, tm_wday=2, tm_yday=27, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-01-27T17:37:23Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=27, tm_hour=17, tm_min=37, tm_sec=23, tm_wday=2, tm_yday=27, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Detecting Deepfake Videos Using Euler Video Magnification\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Detecting Deepfake Videos Using Euler Video Magnification'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent advances in artificial intelligence make it progressively hard to\n",
      "distinguish between genuine and counterfeit media, especially images and\n",
      "videos. One recent development is the rise of deepfake videos, based on\n",
      "manipulating videos using advanced machine learning techniques. This involves\n",
      "replacing the face of an individual from a source video with the face of a\n",
      "second person, in the destination video. This idea is becoming progressively\n",
      "refined as deepfakes are getting progressively seamless and simpler to compute.\n",
      "Combined with the outreach and speed of social media, deepfakes could easily\n",
      "fool individuals when depicting someone saying things that never happened and\n",
      "thus could persuade people in believing fictional scenarios, creating distress,\n",
      "and spreading fake news. In this paper, we examine a technique for possible\n",
      "identification of deepfake videos. We use Euler video magnification which\n",
      "applies spatial decomposition and temporal filtering on video data to highlight\n",
      "and magnify hidden features like skin pulsation and subtle motions. Our\n",
      "approach uses features extracted from the Euler technique to train three models\n",
      "to classify counterfeit and unaltered videos and compare the results with\n",
      "existing techniques.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Recent advances in artificial intelligence make it progressively hard to\\ndistinguish between genuine and counterfeit media, especially images and\\nvideos. One recent development is the rise of deepfake videos, based on\\nmanipulating videos using advanced machine learning techniques. This involves\\nreplacing the face of an individual from a source video with the face of a\\nsecond person, in the destination video. This idea is becoming progressively\\nrefined as deepfakes are getting progressively seamless and simpler to compute.\\nCombined with the outreach and speed of social media, deepfakes could easily\\nfool individuals when depicting someone saying things that never happened and\\nthus could persuade people in believing fictional scenarios, creating distress,\\nand spreading fake news. In this paper, we examine a technique for possible\\nidentification of deepfake videos. We use Euler video magnification which\\napplies spatial decomposition and temporal filtering on video data to highlight\\nand magnify hidden features like skin pulsation and subtle motions. Our\\napproach uses features extracted from the Euler technique to train three models\\nto classify counterfeit and unaltered videos and compare the results with\\nexisting techniques.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Rashmiranjan Das'}, {'name': 'Gaurav Negi'}, {'name': 'Alan F. Smeaton'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Alan F. Smeaton'}\n",
      "\n",
      "\n",
      "author\n",
      "Alan F. Smeaton\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.2352/ISSN.2470-1173.2021.4.MWSF-272\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.2352/ISSN.2470-1173.2021.4.MWSF-272', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2101.11563v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2101.11563v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Presented at Electronic Imaging: Media Watermarking, Security, and\n",
      "  Forensics, 27 January 2021, 6 pages, 6 figures\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2101.09781v4\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2101.09781v4\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-11T08:41:03Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=11, tm_hour=8, tm_min=41, tm_sec=3, tm_wday=2, tm_yday=223, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-01-24T19:45:11Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=24, tm_hour=19, tm_min=45, tm_sec=11, tm_wday=6, tm_yday=24, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Fighting deepfakes by detecting GAN DCT anomalies\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Fighting deepfakes by detecting GAN DCT anomalies'}\n",
      "\n",
      "\n",
      "summary\n",
      "To properly contrast the Deepfake phenomenon the need to design new Deepfake\n",
      "detection algorithms arises; the misuse of this formidable A.I. technology\n",
      "brings serious consequences in the private life of every involved person.\n",
      "State-of-the-art proliferates with solutions using deep neural networks to\n",
      "detect a fake multimedia content but unfortunately these algorithms appear to\n",
      "be neither generalizable nor explainable. However, traces left by Generative\n",
      "Adversarial Network (GAN) engines during the creation of the Deepfakes can be\n",
      "detected by analyzing ad-hoc frequencies. For this reason, in this paper we\n",
      "propose a new pipeline able to detect the so-called GAN Specific Frequencies\n",
      "(GSF) representing a unique fingerprint of the different generative\n",
      "architectures. By employing Discrete Cosine Transform (DCT), anomalous\n",
      "frequencies were detected. The \\BETA statistics inferred by the AC coefficients\n",
      "distribution have been the key to recognize GAN-engine generated data.\n",
      "Robustness tests were also carried out in order to demonstrate the\n",
      "effectiveness of the technique using different attacks on images such as JPEG\n",
      "Compression, mirroring, rotation, scaling, addition of random sized rectangles.\n",
      "Experiments demonstrated that the method is innovative, exceeds the state of\n",
      "the art and also give many insights in terms of explainability.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'To properly contrast the Deepfake phenomenon the need to design new Deepfake\\ndetection algorithms arises; the misuse of this formidable A.I. technology\\nbrings serious consequences in the private life of every involved person.\\nState-of-the-art proliferates with solutions using deep neural networks to\\ndetect a fake multimedia content but unfortunately these algorithms appear to\\nbe neither generalizable nor explainable. However, traces left by Generative\\nAdversarial Network (GAN) engines during the creation of the Deepfakes can be\\ndetected by analyzing ad-hoc frequencies. For this reason, in this paper we\\npropose a new pipeline able to detect the so-called GAN Specific Frequencies\\n(GSF) representing a unique fingerprint of the different generative\\narchitectures. By employing Discrete Cosine Transform (DCT), anomalous\\nfrequencies were detected. The \\\\BETA statistics inferred by the AC coefficients\\ndistribution have been the key to recognize GAN-engine generated data.\\nRobustness tests were also carried out in order to demonstrate the\\neffectiveness of the technique using different attacks on images such as JPEG\\nCompression, mirroring, rotation, scaling, addition of random sized rectangles.\\nExperiments demonstrated that the method is innovative, exceeds the state of\\nthe art and also give many insights in terms of explainability.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Oliver Giudice'}, {'name': 'Luca Guarnera'}, {'name': 'Sebastiano Battiato'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Sebastiano Battiato'}\n",
      "\n",
      "\n",
      "arxiv_affiliation\n",
      "iCTLab s.r.l. - Spin-off of University of Catania\n",
      "\n",
      "\n",
      "author\n",
      "Sebastiano Battiato\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.3390/jimaging7080128\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.3390/jimaging7080128', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2101.09781v4', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2101.09781v4', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "Journal Imaging 2021, 7(8), 128\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2101.09345v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2101.09345v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-01-22T21:50:38Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=22, tm_hour=21, tm_min=50, tm_sec=38, tm_wday=4, tm_yday=22, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-01-22T21:50:38Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=22, tm_hour=21, tm_min=50, tm_sec=38, tm_wday=4, tm_yday=22, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "BERT Transformer model for Detecting Arabic GPT2 Auto-Generated Tweets\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'BERT Transformer model for Detecting Arabic GPT2 Auto-Generated Tweets'}\n",
      "\n",
      "\n",
      "summary\n",
      "During the last two decades, we have progressively turned to the Internet and\n",
      "social media to find news, entertain conversations and share opinion. Recently,\n",
      "OpenAI has developed a ma-chine learning system called GPT-2 for Generative\n",
      "Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate\n",
      "blocks of text based on brief writing prompts that look like they were written\n",
      "by humans, facilitating the spread false or auto-generated text. In line with\n",
      "this progress, and in order to counteract potential dangers, several methods\n",
      "have been pro-posed for detecting text written by these language models. In\n",
      "this paper, we propose a transfer learning based model that will be able to\n",
      "detect if an Arabic sentence is written by humans or automatically generated by\n",
      "bots. Our dataset is based on tweets from a previous work, which we have\n",
      "crawled and extended using the Twitter API. We used GPT2-Small-Arabic to\n",
      "generate fake Arabic Sentences. For evaluation, we compared different recurrent\n",
      "neural network (RNN) word embeddings based baseline models, namely: LSTM,\n",
      "BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new\n",
      "transfer-learning model has obtained an accuracy up to 98%. To the best of our\n",
      "knowledge, this work is the first study where ARABERT and GPT2 were combined to\n",
      "detect and classify the Arabic auto-generated texts.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'During the last two decades, we have progressively turned to the Internet and\\nsocial media to find news, entertain conversations and share opinion. Recently,\\nOpenAI has developed a ma-chine learning system called GPT-2 for Generative\\nPre-trained Transformer-2, which can pro-duce deepfake texts. It can generate\\nblocks of text based on brief writing prompts that look like they were written\\nby humans, facilitating the spread false or auto-generated text. In line with\\nthis progress, and in order to counteract potential dangers, several methods\\nhave been pro-posed for detecting text written by these language models. In\\nthis paper, we propose a transfer learning based model that will be able to\\ndetect if an Arabic sentence is written by humans or automatically generated by\\nbots. Our dataset is based on tweets from a previous work, which we have\\ncrawled and extended using the Twitter API. We used GPT2-Small-Arabic to\\ngenerate fake Arabic Sentences. For evaluation, we compared different recurrent\\nneural network (RNN) word embeddings based baseline models, namely: LSTM,\\nBI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new\\ntransfer-learning model has obtained an accuracy up to 98%. To the best of our\\nknowledge, this work is the first study where ARABERT and GPT2 were combined to\\ndetect and classify the Arabic auto-generated texts.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Fouzi Harrag'}, {'name': 'Maria Debbah'}, {'name': 'Kareem Darwish'}, {'name': 'Ahmed Abdelali'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ahmed Abdelali'}\n",
      "\n",
      "\n",
      "author\n",
      "Ahmed Abdelali\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "Proceedings of the Fifth Arabic Natural Language Processing\n",
      "  Workshop (WANLP @ COLING 2020)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2101.09345v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2101.09345v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2101.09092v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2101.09092v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-01-22T13:10:47Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=22, tm_hour=13, tm_min=10, tm_sec=47, tm_wday=4, tm_yday=22, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-01-22T13:10:47Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=22, tm_hour=13, tm_min=10, tm_sec=47, tm_wday=4, tm_yday=22, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfakes and the 2020 US elections: what (did not) happen\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfakes and the 2020 US elections: what (did not) happen'}\n",
      "\n",
      "\n",
      "summary\n",
      "Alarmed by the volume of disinformation that was assumed to have taken place\n",
      "during the 2016 US elections, scholars, politics and journalists predicted the\n",
      "worst when the first deepfakes began to emerge in 2018. After all, US Elections\n",
      "2020 were believed to be the most secure in American history. This paper seeks\n",
      "explanations for an apparent contradiction: we believe that it was precisely\n",
      "the multiplication and conjugation of different types of warnings and fears\n",
      "that created the conditions that prevented malicious political deepfakes from\n",
      "affecting the 2020 US elections. From these warnings, we identified four\n",
      "factors (more active role of social networks, new laws, difficulties in\n",
      "accessing Artificial Intelligence and better awareness of society). But while\n",
      "this formula has proven to be effective in the case of the United States, 2020,\n",
      "it is not correct to assume that it can be repeated in other political\n",
      "contexts.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Alarmed by the volume of disinformation that was assumed to have taken place\\nduring the 2016 US elections, scholars, politics and journalists predicted the\\nworst when the first deepfakes began to emerge in 2018. After all, US Elections\\n2020 were believed to be the most secure in American history. This paper seeks\\nexplanations for an apparent contradiction: we believe that it was precisely\\nthe multiplication and conjugation of different types of warnings and fears\\nthat created the conditions that prevented malicious political deepfakes from\\naffecting the 2020 US elections. From these warnings, we identified four\\nfactors (more active role of social networks, new laws, difficulties in\\naccessing Artificial Intelligence and better awareness of society). But while\\nthis formula has proven to be effective in the case of the United States, 2020,\\nit is not correct to assume that it can be repeated in other political\\ncontexts.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'João Paulo Meneses'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'João Paulo Meneses'}\n",
      "\n",
      "\n",
      "author\n",
      "João Paulo Meneses\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "13 pages\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2101.09092v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2101.09092v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.SI', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.SI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2101.06278v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2101.06278v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-04-21T18:00:07Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=21, tm_hour=18, tm_min=0, tm_sec=7, tm_wday=2, tm_yday=111, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-01-15T19:00:42Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=15, tm_hour=19, tm_min=0, tm_sec=42, tm_wday=4, tm_yday=15, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "COSMOS: Catching Out-of-Context Misinformation with Self-Supervised\n",
      "  Learning\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'COSMOS: Catching Out-of-Context Misinformation with Self-Supervised\\n  Learning'}\n",
      "\n",
      "\n",
      "summary\n",
      "Despite the recent attention to DeepFakes, one of the most prevalent ways to\n",
      "mislead audiences on social media is the use of unaltered images in a new but\n",
      "false context. To address these challenges and support fact-checkers, we\n",
      "propose a new method that automatically detects out-of-context image and text\n",
      "pairs. Our key insight is to leverage the grounding of image with text to\n",
      "distinguish out-of-context scenarios that cannot be disambiguated with language\n",
      "alone. We propose a self-supervised training strategy where we only need a set\n",
      "of captioned images. At train time, our method learns to selectively align\n",
      "individual objects in an image with textual claims, without explicit\n",
      "supervision. At test time, we check if both captions correspond to the same\n",
      "object(s) in the image but are semantically different, which allows us to make\n",
      "fairly accurate out-of-context predictions. Our method achieves 85%\n",
      "out-of-context detection accuracy. To facilitate benchmarking of this task, we\n",
      "create a large-scale dataset of 200K images with 450K textual captions from a\n",
      "variety of news websites, blogs, and social media posts. The dataset and source\n",
      "code is publicly available at\n",
      "https://shivangi-aneja.github.io/projects/cosmos/.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Despite the recent attention to DeepFakes, one of the most prevalent ways to\\nmislead audiences on social media is the use of unaltered images in a new but\\nfalse context. To address these challenges and support fact-checkers, we\\npropose a new method that automatically detects out-of-context image and text\\npairs. Our key insight is to leverage the grounding of image with text to\\ndistinguish out-of-context scenarios that cannot be disambiguated with language\\nalone. We propose a self-supervised training strategy where we only need a set\\nof captioned images. At train time, our method learns to selectively align\\nindividual objects in an image with textual claims, without explicit\\nsupervision. At test time, we check if both captions correspond to the same\\nobject(s) in the image but are semantically different, which allows us to make\\nfairly accurate out-of-context predictions. Our method achieves 85%\\nout-of-context detection accuracy. To facilitate benchmarking of this task, we\\ncreate a large-scale dataset of 200K images with 450K textual captions from a\\nvariety of news websites, blogs, and social media posts. The dataset and source\\ncode is publicly available at\\nhttps://shivangi-aneja.github.io/projects/cosmos/.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shivangi Aneja'}, {'name': 'Chris Bregler'}, {'name': 'Matthias Nießner'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Matthias Nießner'}\n",
      "\n",
      "\n",
      "author\n",
      "Matthias Nießner\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Video : https://youtu.be/riI3Cl2xy10\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2101.06278v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2101.06278v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2101.03321v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2101.03321v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-01-09T09:06:08Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=9, tm_hour=9, tm_min=6, tm_sec=8, tm_wday=5, tm_yday=9, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-01-09T09:06:08Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=9, tm_hour=9, tm_min=6, tm_sec=8, tm_wday=5, tm_yday=9, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FakeBuster: A DeepFakes Detection Tool for Video Conferencing Scenarios\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'FakeBuster: A DeepFakes Detection Tool for Video Conferencing Scenarios'}\n",
      "\n",
      "\n",
      "summary\n",
      "This paper proposes a new DeepFake detector FakeBuster for detecting\n",
      "impostors during video conferencing and manipulated faces on social media.\n",
      "FakeBuster is a standalone deep learning based solution, which enables a user\n",
      "to detect if another person's video is manipulated or spoofed during a video\n",
      "conferencing based meeting. This tool is independent of video conferencing\n",
      "solutions and has been tested with Zoom and Skype applications. It uses a 3D\n",
      "convolutional neural network for predicting video segment-wise fakeness scores.\n",
      "The network is trained on a combination of datasets such as Deeperforensics,\n",
      "DFDC, VoxCeleb, and deepfake videos created using locally captured (for video\n",
      "conferencing scenarios) images. This leads to different environments and\n",
      "perturbations in the dataset, which improves the generalization of the deepfake\n",
      "network.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"This paper proposes a new DeepFake detector FakeBuster for detecting\\nimpostors during video conferencing and manipulated faces on social media.\\nFakeBuster is a standalone deep learning based solution, which enables a user\\nto detect if another person's video is manipulated or spoofed during a video\\nconferencing based meeting. This tool is independent of video conferencing\\nsolutions and has been tested with Zoom and Skype applications. It uses a 3D\\nconvolutional neural network for predicting video segment-wise fakeness scores.\\nThe network is trained on a combination of datasets such as Deeperforensics,\\nDFDC, VoxCeleb, and deepfake videos created using locally captured (for video\\nconferencing scenarios) images. This leads to different environments and\\nperturbations in the dataset, which improves the generalization of the deepfake\\nnetwork.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Vineet Mehta'}, {'name': 'Parul Gupta'}, {'name': 'Ramanathan Subramanian'}, {'name': 'Abhinav Dhall'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Abhinav Dhall'}\n",
      "\n",
      "\n",
      "author\n",
      "Abhinav Dhall\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "5 Pages, 3 Figures\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2101.03321v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2101.03321v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2101.03272v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2101.03272v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-19T07:14:32Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=19, tm_hour=7, tm_min=14, tm_sec=32, tm_wday=2, tm_yday=139, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-01-09T02:08:59Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=9, tm_hour=2, tm_min=8, tm_sec=59, tm_wday=5, tm_yday=9, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Exploring Adversarial Fake Images on Face Manifold\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Exploring Adversarial Fake Images on Face Manifold'}\n",
      "\n",
      "\n",
      "summary\n",
      "Images synthesized by powerful generative adversarial network (GAN) based\n",
      "methods have drawn moral and privacy concerns. Although image forensic models\n",
      "have reached great performance in detecting fake images from real ones, these\n",
      "models can be easily fooled with a simple adversarial attack. But, the noise\n",
      "adding adversarial samples are also arousing suspicion. In this paper, instead\n",
      "of adding adversarial noise, we optimally search adversarial points on face\n",
      "manifold to generate anti-forensic fake face images. We iteratively do a\n",
      "gradient-descent with each small step in the latent space of a generative\n",
      "model, e.g. Style-GAN, to find an adversarial latent vector, which is similar\n",
      "to norm-based adversarial attack but in latent space. Then, the generated fake\n",
      "images driven by the adversarial latent vectors with the help of GANs can\n",
      "defeat main-stream forensic models. For examples, they make the accuracy of\n",
      "deepfake detection models based on Xception or EfficientNet drop from over 90%\n",
      "to nearly 0%, meanwhile maintaining high visual quality. In addition, we find\n",
      "manipulating style vector $z$ or noise vectors $n$ at different levels have\n",
      "impacts on attack success rate. The generated adversarial images mainly have\n",
      "facial texture or face attributes changing.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Images synthesized by powerful generative adversarial network (GAN) based\\nmethods have drawn moral and privacy concerns. Although image forensic models\\nhave reached great performance in detecting fake images from real ones, these\\nmodels can be easily fooled with a simple adversarial attack. But, the noise\\nadding adversarial samples are also arousing suspicion. In this paper, instead\\nof adding adversarial noise, we optimally search adversarial points on face\\nmanifold to generate anti-forensic fake face images. We iteratively do a\\ngradient-descent with each small step in the latent space of a generative\\nmodel, e.g. Style-GAN, to find an adversarial latent vector, which is similar\\nto norm-based adversarial attack but in latent space. Then, the generated fake\\nimages driven by the adversarial latent vectors with the help of GANs can\\ndefeat main-stream forensic models. For examples, they make the accuracy of\\ndeepfake detection models based on Xception or EfficientNet drop from over 90%\\nto nearly 0%, meanwhile maintaining high visual quality. In addition, we find\\nmanipulating style vector $z$ or noise vectors $n$ at different levels have\\nimpacts on attack success rate. The generated adversarial images mainly have\\nfacial texture or face attributes changing.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Dongze Li'}, {'name': 'Wei Wang'}, {'name': 'Hongxing Fan'}, {'name': 'Jing Dong'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jing Dong'}\n",
      "\n",
      "\n",
      "author\n",
      "Jing Dong\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2101.03272v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2101.03272v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2101.01456v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2101.01456v1\n",
      "\n",
      "\n",
      "updated\n",
      "2021-01-05T11:10:32Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=5, tm_hour=11, tm_min=10, tm_sec=32, tm_wday=1, tm_yday=5, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2021-01-05T11:10:32Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=5, tm_hour=11, tm_min=10, tm_sec=32, tm_wday=1, tm_yday=5, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "In recent years, the abuse of a face swap technique called deepfake Deepfake\n",
      "has raised enormous public concerns. So far, a large number of deepfake videos\n",
      "(known as \"deepfakes\") have been crafted and uploaded to the internet, calling\n",
      "for effective countermeasures. One promising countermeasure against deepfakes\n",
      "is deepfake detection. Several deepfake datasets have been released to support\n",
      "the training and testing of deepfake detectors, such as DeepfakeDetection and\n",
      "FaceForensics++. While this has greatly advanced deepfake detection, most of\n",
      "the real videos in these datasets are filmed with a few volunteer actors in\n",
      "limited scenes, and the fake videos are crafted by researchers using a few\n",
      "popular deepfake softwares. Detectors developed on these datasets may become\n",
      "less effective against real-world deepfakes on the internet. To better support\n",
      "detection against real-world deepfakes, in this paper, we introduce a new\n",
      "dataset WildDeepfake, which consists of 7,314 face sequences extracted from 707\n",
      "deepfake videos collected completely from the internet. WildDeepfake is a small\n",
      "dataset that can be used, in addition to existing datasets, to develop and test\n",
      "the effectiveness of deepfake detectors against real-world deepfakes. We\n",
      "conduct a systematic evaluation of a set of baseline detection networks on both\n",
      "existing and our WildDeepfake datasets, and show that WildDeepfake is indeed a\n",
      "more challenging dataset, where the detection performance can decrease\n",
      "drastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake\n",
      "Detection Networks (ADDNets) to leverage the attention masks on real/fake faces\n",
      "for improved detection. We empirically verify the effectiveness of ADDNets on\n",
      "both existing datasets and WildDeepfake. The dataset is available\n",
      "at:https://github.com/deepfakeinthewild/deepfake-in-the-wild.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In recent years, the abuse of a face swap technique called deepfake Deepfake\\nhas raised enormous public concerns. So far, a large number of deepfake videos\\n(known as \"deepfakes\") have been crafted and uploaded to the internet, calling\\nfor effective countermeasures. One promising countermeasure against deepfakes\\nis deepfake detection. Several deepfake datasets have been released to support\\nthe training and testing of deepfake detectors, such as DeepfakeDetection and\\nFaceForensics++. While this has greatly advanced deepfake detection, most of\\nthe real videos in these datasets are filmed with a few volunteer actors in\\nlimited scenes, and the fake videos are crafted by researchers using a few\\npopular deepfake softwares. Detectors developed on these datasets may become\\nless effective against real-world deepfakes on the internet. To better support\\ndetection against real-world deepfakes, in this paper, we introduce a new\\ndataset WildDeepfake, which consists of 7,314 face sequences extracted from 707\\ndeepfake videos collected completely from the internet. WildDeepfake is a small\\ndataset that can be used, in addition to existing datasets, to develop and test\\nthe effectiveness of deepfake detectors against real-world deepfakes. We\\nconduct a systematic evaluation of a set of baseline detection networks on both\\nexisting and our WildDeepfake datasets, and show that WildDeepfake is indeed a\\nmore challenging dataset, where the detection performance can decrease\\ndrastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake\\nDetection Networks (ADDNets) to leverage the attention masks on real/fake faces\\nfor improved detection. We empirically verify the effectiveness of ADDNets on\\nboth existing datasets and WildDeepfake. The dataset is available\\nat:https://github.com/deepfakeinthewild/deepfake-in-the-wild.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Bojia Zi'}, {'name': 'Minghao Chang'}, {'name': 'Jingjing Chen'}, {'name': 'Xingjun Ma'}, {'name': 'Yu-Gang Jiang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yu-Gang Jiang'}\n",
      "\n",
      "\n",
      "author\n",
      "Yu-Gang Jiang\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2101.01456v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2101.01456v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2012.10580v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2012.10580v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-12-19T03:02:15Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=19, tm_hour=3, tm_min=2, tm_sec=15, tm_wday=5, tm_yday=354, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-12-19T03:02:15Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=19, tm_hour=3, tm_min=2, tm_sec=15, tm_wday=5, tm_yday=354, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Identifying Invariant Texture Violation for Robust Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Identifying Invariant Texture Violation for Robust Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Existing deepfake detection methods have reported promising in-distribution\n",
      "results, by accessing published large-scale dataset. However, due to the\n",
      "non-smooth synthesis method, the fake samples in this dataset may expose\n",
      "obvious artifacts (e.g., stark visual contrast, non-smooth boundary), which\n",
      "were heavily relied on by most of the frame-level detection methods above. As\n",
      "these artifacts do not come up in real media forgeries, the above methods can\n",
      "suffer from a large degradation when applied to fake images that close to\n",
      "reality. To improve the robustness for high-realism fake data, we propose the\n",
      "Invariant Texture Learning (InTeLe) framework, which only accesses the\n",
      "published dataset with low visual quality. Our method is based on the prior\n",
      "that the microscopic facial texture of the source face is inevitably violated\n",
      "by the texture transferred from the target person, which can hence be regarded\n",
      "as the invariant characterization shared among all fake images. To learn such\n",
      "an invariance for deepfake detection, our InTeLe introduces an auto-encoder\n",
      "framework with different decoders for pristine and fake images, which are\n",
      "further appended with a shallow classifier in order to separate out the obvious\n",
      "artifact-effect. Equipped with such a separation, the extracted embedding by\n",
      "encoder can capture the texture violation in fake images, followed by the\n",
      "classifier for the final pristine/fake prediction. As a theoretical guarantee,\n",
      "we prove the identifiability of such an invariance texture violation, i.e., to\n",
      "be precisely inferred from observational data. The effectiveness and utility of\n",
      "our method are demonstrated by promising generalization ability from\n",
      "low-quality images with obvious artifacts to fake images with high realism.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Existing deepfake detection methods have reported promising in-distribution\\nresults, by accessing published large-scale dataset. However, due to the\\nnon-smooth synthesis method, the fake samples in this dataset may expose\\nobvious artifacts (e.g., stark visual contrast, non-smooth boundary), which\\nwere heavily relied on by most of the frame-level detection methods above. As\\nthese artifacts do not come up in real media forgeries, the above methods can\\nsuffer from a large degradation when applied to fake images that close to\\nreality. To improve the robustness for high-realism fake data, we propose the\\nInvariant Texture Learning (InTeLe) framework, which only accesses the\\npublished dataset with low visual quality. Our method is based on the prior\\nthat the microscopic facial texture of the source face is inevitably violated\\nby the texture transferred from the target person, which can hence be regarded\\nas the invariant characterization shared among all fake images. To learn such\\nan invariance for deepfake detection, our InTeLe introduces an auto-encoder\\nframework with different decoders for pristine and fake images, which are\\nfurther appended with a shallow classifier in order to separate out the obvious\\nartifact-effect. Equipped with such a separation, the extracted embedding by\\nencoder can capture the texture violation in fake images, followed by the\\nclassifier for the final pristine/fake prediction. As a theoretical guarantee,\\nwe prove the identifiability of such an invariance texture violation, i.e., to\\nbe precisely inferred from observational data. The effectiveness and utility of\\nour method are demonstrated by promising generalization ability from\\nlow-quality images with obvious artifacts to fake images with high realism.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Xinwei Sun'}, {'name': 'Botong Wu'}, {'name': 'Wei Chen'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Wei Chen'}\n",
      "\n",
      "\n",
      "author\n",
      "Wei Chen\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2012.10580v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2012.10580v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2012.09311v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2012.09311v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-07-26T18:05:20Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=7, tm_mday=26, tm_hour=18, tm_min=5, tm_sec=20, tm_wday=0, tm_yday=207, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-12-16T23:06:56Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=16, tm_hour=23, tm_min=6, tm_sec=56, tm_wday=2, tm_yday=351, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Learning Self-Consistency for Deepfake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Learning Self-Consistency for Deepfake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "We propose a new method to detect deepfake images using the cue of the source\n",
      "feature inconsistency within the forged images. It is based on the hypothesis\n",
      "that images' distinct source features can be preserved and extracted after\n",
      "going through state-of-the-art deepfake generation processes. We introduce a\n",
      "novel representation learning approach, called pair-wise self-consistency\n",
      "learning (PCL), for training ConvNets to extract these source features and\n",
      "detect deepfake images. It is accompanied by a new image synthesis approach,\n",
      "called inconsistency image generator (I2G), to provide richly annotated\n",
      "training data for PCL. Experimental results on seven popular datasets show that\n",
      "our models improve averaged AUC over the state of the art from 96.45% to 98.05%\n",
      "in the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset\n",
      "evaluation.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"We propose a new method to detect deepfake images using the cue of the source\\nfeature inconsistency within the forged images. It is based on the hypothesis\\nthat images' distinct source features can be preserved and extracted after\\ngoing through state-of-the-art deepfake generation processes. We introduce a\\nnovel representation learning approach, called pair-wise self-consistency\\nlearning (PCL), for training ConvNets to extract these source features and\\ndetect deepfake images. It is accompanied by a new image synthesis approach,\\ncalled inconsistency image generator (I2G), to provide richly annotated\\ntraining data for PCL. Experimental results on seven popular datasets show that\\nour models improve averaged AUC over the state of the art from 96.45% to 98.05%\\nin the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset\\nevaluation.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Tianchen Zhao'}, {'name': 'Xiang Xu'}, {'name': 'Mingze Xu'}, {'name': 'Hui Ding'}, {'name': 'Yuanjun Xiong'}, {'name': 'Wei Xia'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Wei Xia'}\n",
      "\n",
      "\n",
      "author\n",
      "Wei Xia\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "ICCV 2021 Oral\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2012.09311v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2012.09311v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2012.07989v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2012.07989v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-12-14T22:40:49Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=14, tm_hour=22, tm_min=40, tm_sec=49, tm_wday=0, tm_yday=349, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-12-14T22:40:49Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=14, tm_hour=22, tm_min=40, tm_sec=49, tm_wday=0, tm_yday=349, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "The Emerging Threats of Deepfake Attacks and Countermeasures\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The Emerging Threats of Deepfake Attacks and Countermeasures'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake technology (DT) has taken a new level of sophistication.\n",
      "Cybercriminals now can manipulate sounds, images, and videos to defraud and\n",
      "misinform individuals and businesses. This represents a growing threat to\n",
      "international institutions and individuals which needs to be addressed. This\n",
      "paper provides an overview of deepfakes, their benefits to society, and how DT\n",
      "works. Highlights the threats that are presented by deepfakes to businesses,\n",
      "politics, and judicial systems worldwide. Additionally, the paper will explore\n",
      "potential solutions to deepfakes and conclude with future research direction.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake technology (DT) has taken a new level of sophistication.\\nCybercriminals now can manipulate sounds, images, and videos to defraud and\\nmisinform individuals and businesses. This represents a growing threat to\\ninternational institutions and individuals which needs to be addressed. This\\npaper provides an overview of deepfakes, their benefits to society, and how DT\\nworks. Highlights the threats that are presented by deepfakes to businesses,\\npolitics, and judicial systems worldwide. Additionally, the paper will explore\\npotential solutions to deepfakes and conclude with future research direction.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shadrack Awah Buo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Shadrack Awah Buo'}\n",
      "\n",
      "\n",
      "author\n",
      "Shadrack Awah Buo\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.13140/RG.2.2.23089.81762\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.13140/RG.2.2.23089.81762', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2012.07989v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2012.07989v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "5\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2012.04726v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2012.04726v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-12-08T20:30:43Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=8, tm_hour=20, tm_min=30, tm_sec=43, tm_wday=1, tm_yday=343, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-12-08T20:30:43Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=8, tm_hour=20, tm_min=30, tm_sec=43, tm_wday=1, tm_yday=343, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Edited Media Understanding: Reasoning About Implications of Manipulated\n",
      "  Images\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Edited Media Understanding: Reasoning About Implications of Manipulated\\n  Images'}\n",
      "\n",
      "\n",
      "summary\n",
      "Multimodal disinformation, from `deepfakes' to simple edits that deceive, is\n",
      "an important societal problem. Yet at the same time, the vast majority of media\n",
      "edits are harmless -- such as a filtered vacation photo. The difference between\n",
      "this example, and harmful edits that spread disinformation, is one of intent.\n",
      "Recognizing and describing this intent is a major challenge for today's AI\n",
      "systems.\n",
      "  We present the task of Edited Media Understanding, requiring models to answer\n",
      "open-ended questions that capture the intent and implications of an image edit.\n",
      "We introduce a dataset for our task, EMU, with 48k question-answer pairs\n",
      "written in rich natural language. We evaluate a wide variety of\n",
      "vision-and-language models for our task, and introduce a new model PELICAN,\n",
      "which builds upon recent progress in pretrained multimodal representations. Our\n",
      "model obtains promising results on our dataset, with humans rating its answers\n",
      "as accurate 40.35% of the time. At the same time, there is still much work to\n",
      "be done -- humans prefer human-annotated captions 93.56% of the time -- and we\n",
      "provide analysis that highlights areas for further progress.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"Multimodal disinformation, from `deepfakes' to simple edits that deceive, is\\nan important societal problem. Yet at the same time, the vast majority of media\\nedits are harmless -- such as a filtered vacation photo. The difference between\\nthis example, and harmful edits that spread disinformation, is one of intent.\\nRecognizing and describing this intent is a major challenge for today's AI\\nsystems.\\n  We present the task of Edited Media Understanding, requiring models to answer\\nopen-ended questions that capture the intent and implications of an image edit.\\nWe introduce a dataset for our task, EMU, with 48k question-answer pairs\\nwritten in rich natural language. We evaluate a wide variety of\\nvision-and-language models for our task, and introduce a new model PELICAN,\\nwhich builds upon recent progress in pretrained multimodal representations. Our\\nmodel obtains promising results on our dataset, with humans rating its answers\\nas accurate 40.35% of the time. At the same time, there is still much work to\\nbe done -- humans prefer human-annotated captions 93.56% of the time -- and we\\nprovide analysis that highlights areas for further progress.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Jeff Da'}, {'name': 'Maxwell Forbes'}, {'name': 'Rowan Zellers'}, {'name': 'Anthony Zheng'}, {'name': 'Jena D. Hwang'}, {'name': 'Antoine Bosselut'}, {'name': 'Yejin Choi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yejin Choi'}\n",
      "\n",
      "\n",
      "author\n",
      "Yejin Choi\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2012.04726v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2012.04726v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2012.04199v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2012.04199v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-12-08T04:06:02Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=8, tm_hour=4, tm_min=6, tm_sec=2, tm_wday=1, tm_yday=343, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-12-08T04:06:02Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=8, tm_hour=4, tm_min=6, tm_sec=2, tm_wday=1, tm_yday=343, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Cost Sensitive Optimization of Deepfake Detector\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Cost Sensitive Optimization of Deepfake Detector'}\n",
      "\n",
      "\n",
      "summary\n",
      "Since the invention of cinema, the manipulated videos have existed. But\n",
      "generating manipulated videos that can fool the viewer has been a\n",
      "time-consuming endeavor. With the dramatic improvements in the deep generative\n",
      "modeling, generating believable looking fake videos has become a reality. In\n",
      "the present work, we concentrate on the so-called deepfake videos, where the\n",
      "source face is swapped with the targets. We argue that deepfake detection task\n",
      "should be viewed as a screening task, where the user, such as the video\n",
      "streaming platform, will screen a large number of videos daily. It is clear\n",
      "then that only a small fraction of the uploaded videos are deepfakes, so the\n",
      "detection performance needs to be measured in a cost-sensitive way. Preferably,\n",
      "the model parameters also need to be estimated in the same way. This is\n",
      "precisely what we propose here.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Since the invention of cinema, the manipulated videos have existed. But\\ngenerating manipulated videos that can fool the viewer has been a\\ntime-consuming endeavor. With the dramatic improvements in the deep generative\\nmodeling, generating believable looking fake videos has become a reality. In\\nthe present work, we concentrate on the so-called deepfake videos, where the\\nsource face is swapped with the targets. We argue that deepfake detection task\\nshould be viewed as a screening task, where the user, such as the video\\nstreaming platform, will screen a large number of videos daily. It is clear\\nthen that only a small fraction of the uploaded videos are deepfakes, so the\\ndetection performance needs to be measured in a cost-sensitive way. Preferably,\\nthe model parameters also need to be estimated in the same way. This is\\nprecisely what we propose here.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ivan Kukanov'}, {'name': 'Janne Karttunen'}, {'name': 'Hannu Sillanpää'}, {'name': 'Ville Hautamäki'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ville Hautamäki'}\n",
      "\n",
      "\n",
      "author\n",
      "Ville Hautamäki\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2012.04199v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2012.04199v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2012.04142v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2012.04142v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-12-08T01:01:38Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=8, tm_hour=1, tm_min=1, tm_sec=38, tm_wday=1, tm_yday=343, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-12-08T01:01:38Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=8, tm_hour=1, tm_min=1, tm_sec=38, tm_wday=1, tm_yday=343, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Technology-driven Alteration of Nonverbal Cues and its Effects on\n",
      "  Negotiation\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Technology-driven Alteration of Nonverbal Cues and its Effects on\\n  Negotiation'}\n",
      "\n",
      "\n",
      "summary\n",
      "A person's appearance, identity, and other nonverbal cues can substantially\n",
      "influence how one is perceived by a negotiation counterpart, potentially\n",
      "impacting the outcome of the negotiation. With recent advances in technology,\n",
      "it is now possible to alter such cues through real-time video communication. In\n",
      "many cases, a person's physical presence can explicitly be replaced by 2D/3D\n",
      "representations in live interactive media. In other cases, technologies such as\n",
      "deepfake can subtly and implicitly alter many nonverbal cues -- including a\n",
      "person's appearance and identity -- in real-time. In this article, we look at\n",
      "some state-of-the-art technological advances that can enable such explicit and\n",
      "implicit alteration of nonverbal cues. We also discuss the implications of such\n",
      "technology for the negotiation landscape and highlight ethical considerations\n",
      "that warrant deep, ongoing attention from stakeholders.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"A person's appearance, identity, and other nonverbal cues can substantially\\ninfluence how one is perceived by a negotiation counterpart, potentially\\nimpacting the outcome of the negotiation. With recent advances in technology,\\nit is now possible to alter such cues through real-time video communication. In\\nmany cases, a person's physical presence can explicitly be replaced by 2D/3D\\nrepresentations in live interactive media. In other cases, technologies such as\\ndeepfake can subtly and implicitly alter many nonverbal cues -- including a\\nperson's appearance and identity -- in real-time. In this article, we look at\\nsome state-of-the-art technological advances that can enable such explicit and\\nimplicit alteration of nonverbal cues. We also discuss the implications of such\\ntechnology for the negotiation landscape and highlight ethical considerations\\nthat warrant deep, ongoing attention from stakeholders.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Raiyan Abdul Baten'}, {'name': 'Ehsan Hoque'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ehsan Hoque'}\n",
      "\n",
      "\n",
      "author\n",
      "Ehsan Hoque\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2012.04142v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2012.04142v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2012.03930v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2012.03930v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-12-07T18:59:08Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=7, tm_hour=18, tm_min=59, tm_sec=8, tm_wday=0, tm_yday=342, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-12-07T18:59:08Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=7, tm_hour=18, tm_min=59, tm_sec=8, tm_wday=0, tm_yday=342, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Identity-Driven DeepFake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Identity-Driven DeepFake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "DeepFake detection has so far been dominated by ``artifact-driven'' methods\n",
      "and the detection performance significantly degrades when either the type of\n",
      "image artifacts is unknown or the artifacts are simply too hard to find. In\n",
      "this work, we present an alternative approach: Identity-Driven DeepFake\n",
      "Detection. Our approach takes as input the suspect image/video as well as the\n",
      "target identity information (a reference image or video). We output a decision\n",
      "on whether the identity in the suspect image/video is the same as the target\n",
      "identity. Our motivation is to prevent the most common and harmful DeepFakes\n",
      "that spread false information of a targeted person. The identity-based approach\n",
      "is fundamentally different in that it does not attempt to detect image\n",
      "artifacts. Instead, it focuses on whether the identity in the suspect\n",
      "image/video is true. To facilitate research on identity-based detection, we\n",
      "present a new large scale dataset ``Vox-DeepFake\", in which each suspect\n",
      "content is associated with multiple reference images collected from videos of a\n",
      "target identity. We also present a simple identity-based detection algorithm\n",
      "called the OuterFace, which may serve as a baseline for further research. Even\n",
      "trained without fake videos, the OuterFace algorithm achieves superior\n",
      "detection accuracy and generalizes well to different DeepFake methods, and is\n",
      "robust with respect to video degradation techniques -- a performance not\n",
      "achievable with existing detection algorithms.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFake detection has so far been dominated by ``artifact-driven\\'\\' methods\\nand the detection performance significantly degrades when either the type of\\nimage artifacts is unknown or the artifacts are simply too hard to find. In\\nthis work, we present an alternative approach: Identity-Driven DeepFake\\nDetection. Our approach takes as input the suspect image/video as well as the\\ntarget identity information (a reference image or video). We output a decision\\non whether the identity in the suspect image/video is the same as the target\\nidentity. Our motivation is to prevent the most common and harmful DeepFakes\\nthat spread false information of a targeted person. The identity-based approach\\nis fundamentally different in that it does not attempt to detect image\\nartifacts. Instead, it focuses on whether the identity in the suspect\\nimage/video is true. To facilitate research on identity-based detection, we\\npresent a new large scale dataset ``Vox-DeepFake\", in which each suspect\\ncontent is associated with multiple reference images collected from videos of a\\ntarget identity. We also present a simple identity-based detection algorithm\\ncalled the OuterFace, which may serve as a baseline for further research. Even\\ntrained without fake videos, the OuterFace algorithm achieves superior\\ndetection accuracy and generalizes well to different DeepFake methods, and is\\nrobust with respect to video degradation techniques -- a performance not\\nachievable with existing detection algorithms.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Xiaoyi Dong'}, {'name': 'Jianmin Bao'}, {'name': 'Dongdong Chen'}, {'name': 'Weiming Zhang'}, {'name': 'Nenghai Yu'}, {'name': 'Dong Chen'}, {'name': 'Fang Wen'}, {'name': 'Baining Guo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Baining Guo'}\n",
      "\n",
      "\n",
      "author\n",
      "Baining Guo\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2012.03930v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2012.03930v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2012.02512v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2012.02512v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-08-20T21:33:53Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=8, tm_mday=20, tm_hour=21, tm_min=33, tm_sec=53, tm_wday=4, tm_yday=232, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-12-04T10:43:16Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=4, tm_hour=10, tm_min=43, tm_sec=16, tm_wday=4, tm_yday=339, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "ID-Reveal: Identity-aware DeepFake Video Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'ID-Reveal: Identity-aware DeepFake Video Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "A major challenge in DeepFake forgery detection is that state-of-the-art\n",
      "algorithms are mostly trained to detect a specific fake method. As a result,\n",
      "these approaches show poor generalization across different types of facial\n",
      "manipulations, e.g., from face swapping to facial reenactment. To this end, we\n",
      "introduce ID-Reveal, a new approach that learns temporal facial features,\n",
      "specific of how a person moves while talking, by means of metric learning\n",
      "coupled with an adversarial training strategy. The advantage is that we do not\n",
      "need any training data of fakes, but only train on real videos. Moreover, we\n",
      "utilize high-level semantic features, which enables robustness to widespread\n",
      "and disruptive forms of post-processing. We perform a thorough experimental\n",
      "analysis on several publicly available benchmarks. Compared to state of the\n",
      "art, our method improves generalization and is more robust to low-quality\n",
      "videos, that are usually spread over social networks. In particular, we obtain\n",
      "an average improvement of more than 15% in terms of accuracy for facial\n",
      "reenactment on high compressed videos.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'A major challenge in DeepFake forgery detection is that state-of-the-art\\nalgorithms are mostly trained to detect a specific fake method. As a result,\\nthese approaches show poor generalization across different types of facial\\nmanipulations, e.g., from face swapping to facial reenactment. To this end, we\\nintroduce ID-Reveal, a new approach that learns temporal facial features,\\nspecific of how a person moves while talking, by means of metric learning\\ncoupled with an adversarial training strategy. The advantage is that we do not\\nneed any training data of fakes, but only train on real videos. Moreover, we\\nutilize high-level semantic features, which enables robustness to widespread\\nand disruptive forms of post-processing. We perform a thorough experimental\\nanalysis on several publicly available benchmarks. Compared to state of the\\nart, our method improves generalization and is more robust to low-quality\\nvideos, that are usually spread over social networks. In particular, we obtain\\nan average improvement of more than 15% in terms of accuracy for facial\\nreenactment on high compressed videos.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Davide Cozzolino'}, {'name': 'Andreas Rössler'}, {'name': 'Justus Thies'}, {'name': 'Matthias Nießner'}, {'name': 'Luisa Verdoliva'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Luisa Verdoliva'}\n",
      "\n",
      "\n",
      "author\n",
      "Luisa Verdoliva\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Video: https://www.youtube.com/watch?v=RsFxsOLvRdY\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2012.02512v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2012.02512v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2011.09957v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2011.09957v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-11-19T16:53:38Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=19, tm_hour=16, tm_min=53, tm_sec=38, tm_wday=3, tm_yday=324, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-11-19T16:53:38Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=19, tm_hour=16, tm_min=53, tm_sec=38, tm_wday=3, tm_yday=324, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Adversarial Threats to DeepFake Detection: A Practical Perspective\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Adversarial Threats to DeepFake Detection: A Practical Perspective'}\n",
      "\n",
      "\n",
      "summary\n",
      "Facially manipulated images and videos or DeepFakes can be used maliciously\n",
      "to fuel misinformation or defame individuals. Therefore, detecting DeepFakes is\n",
      "crucial to increase the credibility of social media platforms and other media\n",
      "sharing web sites. State-of-the art DeepFake detection techniques rely on\n",
      "neural network based classification models which are known to be vulnerable to\n",
      "adversarial examples. In this work, we study the vulnerabilities of\n",
      "state-of-the-art DeepFake detection methods from a practical stand point. We\n",
      "perform adversarial attacks on DeepFake detectors in a black box setting where\n",
      "the adversary does not have complete knowledge of the classification models. We\n",
      "study the extent to which adversarial perturbations transfer across different\n",
      "models and propose techniques to improve the transferability of adversarial\n",
      "examples. We also create more accessible attacks using Universal Adversarial\n",
      "Perturbations which pose a very feasible attack scenario since they can be\n",
      "easily shared amongst attackers. We perform our evaluations on the winning\n",
      "entries of the DeepFake Detection Challenge (DFDC) and demonstrate that they\n",
      "can be easily bypassed in a practical attack scenario by designing transferable\n",
      "and accessible adversarial attacks.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Facially manipulated images and videos or DeepFakes can be used maliciously\\nto fuel misinformation or defame individuals. Therefore, detecting DeepFakes is\\ncrucial to increase the credibility of social media platforms and other media\\nsharing web sites. State-of-the art DeepFake detection techniques rely on\\nneural network based classification models which are known to be vulnerable to\\nadversarial examples. In this work, we study the vulnerabilities of\\nstate-of-the-art DeepFake detection methods from a practical stand point. We\\nperform adversarial attacks on DeepFake detectors in a black box setting where\\nthe adversary does not have complete knowledge of the classification models. We\\nstudy the extent to which adversarial perturbations transfer across different\\nmodels and propose techniques to improve the transferability of adversarial\\nexamples. We also create more accessible attacks using Universal Adversarial\\nPerturbations which pose a very feasible attack scenario since they can be\\neasily shared amongst attackers. We perform our evaluations on the winning\\nentries of the DeepFake Detection Challenge (DFDC) and demonstrate that they\\ncan be easily bypassed in a practical attack scenario by designing transferable\\nand accessible adversarial attacks.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Paarth Neekhara'}, {'name': 'Brian Dolhansky'}, {'name': 'Joanna Bitton'}, {'name': 'Cristian Canton Ferrer'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Cristian Canton Ferrer'}\n",
      "\n",
      "\n",
      "author\n",
      "Cristian Canton Ferrer\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2011.09957v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2011.09957v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2011.07792v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2011.07792v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-11-16T08:50:56Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=16, tm_hour=8, tm_min=50, tm_sec=56, tm_wday=0, tm_yday=321, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-11-16T08:50:56Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=16, tm_hour=8, tm_min=50, tm_sec=56, tm_wday=0, tm_yday=321, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Training Strategies and Data Augmentations in CNN-based DeepFake Video\n",
      "  Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Training Strategies and Data Augmentations in CNN-based DeepFake Video\\n  Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "The fast and continuous growth in number and quality of deepfake videos calls\n",
      "for the development of reliable detection systems capable of automatically\n",
      "warning users on social media and on the Internet about the potential\n",
      "untruthfulness of such contents. While algorithms, software, and smartphone\n",
      "apps are getting better every day in generating manipulated videos and swapping\n",
      "faces, the accuracy of automated systems for face forgery detection in videos\n",
      "is still quite limited and generally biased toward the dataset used to design\n",
      "and train a specific detection system. In this paper we analyze how different\n",
      "training strategies and data augmentation techniques affect CNN-based deepfake\n",
      "detectors when training and testing on the same dataset or across different\n",
      "datasets.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The fast and continuous growth in number and quality of deepfake videos calls\\nfor the development of reliable detection systems capable of automatically\\nwarning users on social media and on the Internet about the potential\\nuntruthfulness of such contents. While algorithms, software, and smartphone\\napps are getting better every day in generating manipulated videos and swapping\\nfaces, the accuracy of automated systems for face forgery detection in videos\\nis still quite limited and generally biased toward the dataset used to design\\nand train a specific detection system. In this paper we analyze how different\\ntraining strategies and data augmentation techniques affect CNN-based deepfake\\ndetectors when training and testing on the same dataset or across different\\ndatasets.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Luca Bondi'}, {'name': 'Edoardo Daniele Cannas'}, {'name': 'Paolo Bestagini'}, {'name': 'Stefano Tubaro'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Stefano Tubaro'}\n",
      "\n",
      "\n",
      "author\n",
      "Stefano Tubaro\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2011.07792v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2011.07792v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2011.05421v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2011.05421v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-11-10T22:05:38Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=10, tm_hour=22, tm_min=5, tm_sec=38, tm_wday=1, tm_yday=315, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-11-10T22:05:38Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=10, tm_hour=22, tm_min=5, tm_sec=38, tm_wday=1, tm_yday=315, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Using GANs to Synthesise Minimum Training Data for Deepfake Generation\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Using GANs to Synthesise Minimum Training Data for Deepfake Generation'}\n",
      "\n",
      "\n",
      "summary\n",
      "There are many applications of Generative Adversarial Networks (GANs) in\n",
      "fields like computer vision, natural language processing, speech synthesis, and\n",
      "more. Undoubtedly the most notable results have been in the area of image\n",
      "synthesis and in particular in the generation of deepfake videos. While\n",
      "deepfakes have received much negative media coverage, they can be a useful\n",
      "technology in applications like entertainment, customer relations, or even\n",
      "assistive care. One problem with generating deepfakes is the requirement for a\n",
      "lot of image training data of the subject which is not an issue if the subject\n",
      "is a celebrity for whom many images already exist. If there are only a small\n",
      "number of training images then the quality of the deepfake will be poor. Some\n",
      "media reports have indicated that a good deepfake can be produced with as few\n",
      "as 500 images but in practice, quality deepfakes require many thousands of\n",
      "images, one of the reasons why deepfakes of celebrities and politicians have\n",
      "become so popular. In this study, we exploit the property of a GAN to produce\n",
      "images of an individual with variable facial expressions which we then use to\n",
      "generate a deepfake. We observe that with such variability in facial\n",
      "expressions of synthetic GAN-generated training images and a reduced quantity\n",
      "of them, we can produce a near-realistic deepfake videos.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'There are many applications of Generative Adversarial Networks (GANs) in\\nfields like computer vision, natural language processing, speech synthesis, and\\nmore. Undoubtedly the most notable results have been in the area of image\\nsynthesis and in particular in the generation of deepfake videos. While\\ndeepfakes have received much negative media coverage, they can be a useful\\ntechnology in applications like entertainment, customer relations, or even\\nassistive care. One problem with generating deepfakes is the requirement for a\\nlot of image training data of the subject which is not an issue if the subject\\nis a celebrity for whom many images already exist. If there are only a small\\nnumber of training images then the quality of the deepfake will be poor. Some\\nmedia reports have indicated that a good deepfake can be produced with as few\\nas 500 images but in practice, quality deepfakes require many thousands of\\nimages, one of the reasons why deepfakes of celebrities and politicians have\\nbecome so popular. In this study, we exploit the property of a GAN to produce\\nimages of an individual with variable facial expressions which we then use to\\ngenerate a deepfake. We observe that with such variability in facial\\nexpressions of synthetic GAN-generated training images and a reduced quantity\\nof them, we can produce a near-realistic deepfake videos.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Simranjeet Singh'}, {'name': 'Rajneesh Sharma'}, {'name': 'Alan F. Smeaton'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Alan F. Smeaton'}\n",
      "\n",
      "\n",
      "author\n",
      "Alan F. Smeaton\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "13 pages, 6 figures, 2 tables, appears in Proceedings of 28th Irish\n",
      "  Conference on Artificial Intelligence and Cognitive Science AICS2020,\n",
      "  December 2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2011.05421v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2011.05421v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2011.03689v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2011.03689v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-11-25T03:20:36Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=25, tm_hour=3, tm_min=20, tm_sec=36, tm_wday=2, tm_yday=330, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-11-07T04:42:27Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=7, tm_hour=4, tm_min=42, tm_sec=27, tm_wday=5, tm_yday=312, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Detection and Evaluation of human and machine generated speech in\n",
      "  spoofing attacks on automatic speaker verification systems\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Detection and Evaluation of human and machine generated speech in\\n  spoofing attacks on automatic speaker verification systems'}\n",
      "\n",
      "\n",
      "summary\n",
      "Automatic speaker verification (ASV) systems utilize the biometric\n",
      "information in human speech to verify the speaker's identity. The techniques\n",
      "used for performing speaker verification are often vulnerable to malicious\n",
      "attacks that attempt to induce the ASV system to return wrong results, allowing\n",
      "an impostor to bypass the system and gain access. Attackers use a multitude of\n",
      "spoofing techniques for this, such as voice conversion, audio replay, speech\n",
      "synthesis, etc. In recent years, easily available tools to generate deepfaked\n",
      "audio have increased the potential threat to ASV systems. In this paper, we\n",
      "compare the potential of human impersonation (voice disguise) based attacks\n",
      "with attacks based on machine-generated speech, on black-box and white-box ASV\n",
      "systems. We also study countermeasures by using features that capture the\n",
      "unique aspects of human speech production, under the hypothesis that machines\n",
      "cannot emulate many of the fine-level intricacies of the human speech\n",
      "production mechanism. We show that fundamental frequency sequence-related\n",
      "entropy, spectral envelope, and aperiodic parameters are promising candidates\n",
      "for robust detection of deepfaked speech generated by unknown methods.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"Automatic speaker verification (ASV) systems utilize the biometric\\ninformation in human speech to verify the speaker's identity. The techniques\\nused for performing speaker verification are often vulnerable to malicious\\nattacks that attempt to induce the ASV system to return wrong results, allowing\\nan impostor to bypass the system and gain access. Attackers use a multitude of\\nspoofing techniques for this, such as voice conversion, audio replay, speech\\nsynthesis, etc. In recent years, easily available tools to generate deepfaked\\naudio have increased the potential threat to ASV systems. In this paper, we\\ncompare the potential of human impersonation (voice disguise) based attacks\\nwith attacks based on machine-generated speech, on black-box and white-box ASV\\nsystems. We also study countermeasures by using features that capture the\\nunique aspects of human speech production, under the hypothesis that machines\\ncannot emulate many of the fine-level intricacies of the human speech\\nproduction mechanism. We show that fundamental frequency sequence-related\\nentropy, spectral envelope, and aperiodic parameters are promising candidates\\nfor robust detection of deepfaked speech generated by unknown methods.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yang Gao'}, {'name': 'Jiachen Lian'}, {'name': 'Bhiksha Raj'}, {'name': 'Rita Singh'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Rita Singh'}\n",
      "\n",
      "\n",
      "author\n",
      "Rita Singh\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "6 pages excluding references. Paper accepted by IEEE Spoken Language\n",
      "  Technology (SLT) 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2011.03689v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2011.03689v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2011.02674v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2011.02674v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-11-05T06:17:04Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=5, tm_hour=6, tm_min=17, tm_sec=4, tm_wday=3, tm_yday=310, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-11-05T06:17:04Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=5, tm_hour=6, tm_min=17, tm_sec=4, tm_wday=3, tm_yday=310, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "AOT: Appearance Optimal Transport Based Identity Swapping for Forgery\n",
      "  Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'AOT: Appearance Optimal Transport Based Identity Swapping for Forgery\\n  Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent studies have shown that the performance of forgery detection can be\n",
      "improved with diverse and challenging Deepfakes datasets. However, due to the\n",
      "lack of Deepfakes datasets with large variance in appearance, which can be\n",
      "hardly produced by recent identity swapping methods, the detection algorithm\n",
      "may fail in this situation. In this work, we provide a new identity swapping\n",
      "algorithm with large differences in appearance for face forgery detection. The\n",
      "appearance gaps mainly arise from the large discrepancies in illuminations and\n",
      "skin colors that widely exist in real-world scenarios. However, due to the\n",
      "difficulties of modeling the complex appearance mapping, it is challenging to\n",
      "transfer fine-grained appearances adaptively while preserving identity traits.\n",
      "This paper formulates appearance mapping as an optimal transport problem and\n",
      "proposes an Appearance Optimal Transport model (AOT) to formulate it in both\n",
      "latent and pixel space. Specifically, a relighting generator is designed to\n",
      "simulate the optimal transport plan. It is solved via minimizing the\n",
      "Wasserstein distance of the learned features in the latent space, enabling\n",
      "better performance and less computation than conventional optimization. To\n",
      "further refine the solution of the optimal transport plan, we develop a\n",
      "segmentation game to minimize the Wasserstein distance in the pixel space. A\n",
      "discriminator is introduced to distinguish the fake parts from a mix of real\n",
      "and fake image patches. Extensive experiments reveal that the superiority of\n",
      "our method when compared with state-of-the-art methods and the ability of our\n",
      "generated data to improve the performance of face forgery detection.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Recent studies have shown that the performance of forgery detection can be\\nimproved with diverse and challenging Deepfakes datasets. However, due to the\\nlack of Deepfakes datasets with large variance in appearance, which can be\\nhardly produced by recent identity swapping methods, the detection algorithm\\nmay fail in this situation. In this work, we provide a new identity swapping\\nalgorithm with large differences in appearance for face forgery detection. The\\nappearance gaps mainly arise from the large discrepancies in illuminations and\\nskin colors that widely exist in real-world scenarios. However, due to the\\ndifficulties of modeling the complex appearance mapping, it is challenging to\\ntransfer fine-grained appearances adaptively while preserving identity traits.\\nThis paper formulates appearance mapping as an optimal transport problem and\\nproposes an Appearance Optimal Transport model (AOT) to formulate it in both\\nlatent and pixel space. Specifically, a relighting generator is designed to\\nsimulate the optimal transport plan. It is solved via minimizing the\\nWasserstein distance of the learned features in the latent space, enabling\\nbetter performance and less computation than conventional optimization. To\\nfurther refine the solution of the optimal transport plan, we develop a\\nsegmentation game to minimize the Wasserstein distance in the pixel space. A\\ndiscriminator is introduced to distinguish the fake parts from a mix of real\\nand fake image patches. Extensive experiments reveal that the superiority of\\nour method when compared with state-of-the-art methods and the ability of our\\ngenerated data to improve the performance of face forgery detection.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hao Zhu'}, {'name': 'Chaoyou Fu'}, {'name': 'Qianyi Wu'}, {'name': 'Wayne Wu'}, {'name': 'Chen Qian'}, {'name': 'Ran He'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ran He'}\n",
      "\n",
      "\n",
      "author\n",
      "Ran He\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "NeurIPS 2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2011.02674v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2011.02674v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2010.11844v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2010.11844v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-10-22T16:28:50Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=10, tm_mday=22, tm_hour=16, tm_min=28, tm_sec=50, tm_wday=3, tm_yday=296, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-10-22T16:28:50Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=10, tm_mday=22, tm_hour=16, tm_min=28, tm_sec=50, tm_wday=3, tm_yday=296, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Spatio-temporal Features for Generalized Detection of Deepfake Videos\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Spatio-temporal Features for Generalized Detection of Deepfake Videos'}\n",
      "\n",
      "\n",
      "summary\n",
      "For deepfake detection, video-level detectors have not been explored as\n",
      "extensively as image-level detectors, which do not exploit temporal data. In\n",
      "this paper, we empirically show that existing approaches on image and sequence\n",
      "classifiers generalize poorly to new manipulation techniques. To this end, we\n",
      "propose spatio-temporal features, modeled by 3D CNNs, to extend the\n",
      "generalization capabilities to detect new sorts of deepfake videos. We show\n",
      "that spatial features learn distinct deepfake-method-specific attributes, while\n",
      "spatio-temporal features capture shared attributes between deepfake methods. We\n",
      "provide an in-depth analysis of how the sequential and spatio-temporal video\n",
      "encoders are utilizing temporal information using DFDC dataset\n",
      "arXiv:2006.07397. Thus, we unravel that our approach captures local\n",
      "spatio-temporal relations and inconsistencies in the deepfake videos while\n",
      "existing sequence encoders are indifferent to it. Through large scale\n",
      "experiments conducted on the FaceForensics++ arXiv:1901.08971 and Deeper\n",
      "Forensics arXiv:2001.03024 datasets, we show that our approach outperforms\n",
      "existing methods in terms of generalization capabilities.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'For deepfake detection, video-level detectors have not been explored as\\nextensively as image-level detectors, which do not exploit temporal data. In\\nthis paper, we empirically show that existing approaches on image and sequence\\nclassifiers generalize poorly to new manipulation techniques. To this end, we\\npropose spatio-temporal features, modeled by 3D CNNs, to extend the\\ngeneralization capabilities to detect new sorts of deepfake videos. We show\\nthat spatial features learn distinct deepfake-method-specific attributes, while\\nspatio-temporal features capture shared attributes between deepfake methods. We\\nprovide an in-depth analysis of how the sequential and spatio-temporal video\\nencoders are utilizing temporal information using DFDC dataset\\narXiv:2006.07397. Thus, we unravel that our approach captures local\\nspatio-temporal relations and inconsistencies in the deepfake videos while\\nexisting sequence encoders are indifferent to it. Through large scale\\nexperiments conducted on the FaceForensics++ arXiv:1901.08971 and Deeper\\nForensics arXiv:2001.03024 datasets, we show that our approach outperforms\\nexisting methods in terms of generalization capabilities.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ipek Ganiyusufoglu'}, {'name': 'L. Minh Ngô'}, {'name': 'Nedko Savov'}, {'name': 'Sezer Karaoglu'}, {'name': 'Theo Gevers'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Theo Gevers'}\n",
      "\n",
      "\n",
      "author\n",
      "Theo Gevers\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Submitted to Computer Vision and Image Understanding (CVIU)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2010.11844v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2010.11844v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2010.07475v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2010.07475v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-10-15T02:35:31Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=10, tm_mday=15, tm_hour=2, tm_min=35, tm_sec=31, tm_wday=3, tm_yday=289, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-10-15T02:35:31Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=10, tm_mday=15, tm_hour=2, tm_min=35, tm_sec=31, tm_wday=3, tm_yday=289, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Neural Deepfake Detection with Factual Structure of Text\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Neural Deepfake Detection with Factual Structure of Text'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake detection, the task of automatically discriminating\n",
      "machine-generated text, is increasingly critical with recent advances in\n",
      "natural language generative models. Existing approaches to deepfake detection\n",
      "typically represent documents with coarse-grained representations. However,\n",
      "they struggle to capture factual structures of documents, which is a\n",
      "discriminative factor between machine-generated and human-written text\n",
      "according to our statistical analysis. To address this, we propose a\n",
      "graph-based model that utilizes the factual structure of a document for\n",
      "deepfake detection of text. Our approach represents the factual structure of a\n",
      "given document as an entity graph, which is further utilized to learn sentence\n",
      "representations with a graph neural network. Sentence representations are then\n",
      "composed to a document representation for making predictions, where consistent\n",
      "relations between neighboring sentences are sequentially modeled. Results of\n",
      "experiments on two public deepfake datasets show that our approach\n",
      "significantly improves strong base models built with RoBERTa. Model analysis\n",
      "further indicates that our model can distinguish the difference in the factual\n",
      "structure between machine-generated text and human-written text.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake detection, the task of automatically discriminating\\nmachine-generated text, is increasingly critical with recent advances in\\nnatural language generative models. Existing approaches to deepfake detection\\ntypically represent documents with coarse-grained representations. However,\\nthey struggle to capture factual structures of documents, which is a\\ndiscriminative factor between machine-generated and human-written text\\naccording to our statistical analysis. To address this, we propose a\\ngraph-based model that utilizes the factual structure of a document for\\ndeepfake detection of text. Our approach represents the factual structure of a\\ngiven document as an entity graph, which is further utilized to learn sentence\\nrepresentations with a graph neural network. Sentence representations are then\\ncomposed to a document representation for making predictions, where consistent\\nrelations between neighboring sentences are sequentially modeled. Results of\\nexperiments on two public deepfake datasets show that our approach\\nsignificantly improves strong base models built with RoBERTa. Model analysis\\nfurther indicates that our model can distinguish the difference in the factual\\nstructure between machine-generated text and human-written text.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Wanjun Zhong'}, {'name': 'Duyu Tang'}, {'name': 'Zenan Xu'}, {'name': 'Ruize Wang'}, {'name': 'Nan Duan'}, {'name': 'Ming Zhou'}, {'name': 'Jiahai Wang'}, {'name': 'Jian Yin'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jian Yin'}\n",
      "\n",
      "\n",
      "author\n",
      "Jian Yin\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "EMNLP2020;10 pages\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2010.07475v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2010.07475v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2010.00400v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2010.00400v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-12-14T14:34:23Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=14, tm_hour=14, tm_min=34, tm_sec=23, tm_wday=0, tm_yday=349, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-10-01T13:37:58Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=10, tm_mday=1, tm_hour=13, tm_min=37, tm_sec=58, tm_wday=3, tm_yday=275, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation'}\n",
      "\n",
      "\n",
      "summary\n",
      "This work introduces a novel DeepFake detection framework based on\n",
      "physiological measurement. In particular, we consider information related to\n",
      "the heart rate using remote photoplethysmography (rPPG). rPPG methods analyze\n",
      "video sequences looking for subtle color changes in the human skin, revealing\n",
      "the presence of human blood under the tissues. In this work we investigate to\n",
      "what extent rPPG is useful for the detection of DeepFake videos.\n",
      "  The proposed fake detector named DeepFakesON-Phys uses a Convolutional\n",
      "Attention Network (CAN), which extracts spatial and temporal information from\n",
      "video frames, analyzing and combining both sources to better detect fake\n",
      "videos. This detection approach has been experimentally evaluated using the\n",
      "latest public databases in the field: Celeb-DF and DFDC. The results achieved,\n",
      "above 98% AUC (Area Under the Curve) on both databases, outperform the state of\n",
      "the art and prove the success of fake detectors based on physiological\n",
      "measurement to detect the latest DeepFake videos.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'This work introduces a novel DeepFake detection framework based on\\nphysiological measurement. In particular, we consider information related to\\nthe heart rate using remote photoplethysmography (rPPG). rPPG methods analyze\\nvideo sequences looking for subtle color changes in the human skin, revealing\\nthe presence of human blood under the tissues. In this work we investigate to\\nwhat extent rPPG is useful for the detection of DeepFake videos.\\n  The proposed fake detector named DeepFakesON-Phys uses a Convolutional\\nAttention Network (CAN), which extracts spatial and temporal information from\\nvideo frames, analyzing and combining both sources to better detect fake\\nvideos. This detection approach has been experimentally evaluated using the\\nlatest public databases in the field: Celeb-DF and DFDC. The results achieved,\\nabove 98% AUC (Area Under the Curve) on both databases, outperform the state of\\nthe art and prove the success of fake detectors based on physiological\\nmeasurement to detect the latest DeepFake videos.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Javier Hernandez-Ortega'}, {'name': 'Ruben Tolosana'}, {'name': 'Julian Fierrez'}, {'name': 'Aythami Morales'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Aythami Morales'}\n",
      "\n",
      "\n",
      "author\n",
      "Aythami Morales\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "Proc. 35th AAAI Conference on Artificial Intelligence Workshops,\n",
      "  2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2010.00400v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2010.00400v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2009.09869v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2009.09869v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-09-26T11:45:47Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=9, tm_mday=26, tm_hour=11, tm_min=45, tm_sec=47, tm_wday=6, tm_yday=269, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-09-21T13:41:24Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=9, tm_mday=21, tm_hour=13, tm_min=41, tm_sec=24, tm_wday=0, tm_yday=265, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FakeTagger: Robust Safeguards against DeepFake Dissemination via\n",
      "  Provenance Tracking\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'FakeTagger: Robust Safeguards against DeepFake Dissemination via\\n  Provenance Tracking'}\n",
      "\n",
      "\n",
      "summary\n",
      "In recent years, DeepFake is becoming a common threat to our society, due to\n",
      "the remarkable progress of generative adversarial networks (GAN) in image\n",
      "synthesis. Unfortunately, existing studies that propose various approaches, in\n",
      "fighting against DeepFake and determining if the facial image is real or fake,\n",
      "is still at an early stage. Obviously, the current DeepFake detection method\n",
      "struggles to catch the rapid progress of GANs, especially in the adversarial\n",
      "scenarios where attackers can evade the detection intentionally, such as adding\n",
      "perturbations to fool the DNN-based detectors. While passive detection simply\n",
      "tells whether the image is fake or real, DeepFake provenance, on the other\n",
      "hand, provides clues for tracking the sources in DeepFake forensics. Thus, the\n",
      "tracked fake images could be blocked immediately by administrators and avoid\n",
      "further spread in social networks.\n",
      "  In this paper, we investigate the potentials of image tagging in serving the\n",
      "DeepFake provenance tracking. Specifically, we devise a deep learning-based\n",
      "approach, named FakeTagger, with a simple yet effective encoder and decoder\n",
      "design along with channel coding to embed message to the facial image, which is\n",
      "to recover the embedded message after various drastic GAN-based DeepFake\n",
      "transformation with high confidence. The embedded message could be employed to\n",
      "represent the identity of facial images, which further contributed to DeepFake\n",
      "detection and provenance. Experimental results demonstrate that our proposed\n",
      "approach could recover the embedded message with an average accuracy of more\n",
      "than 95% over the four common types of DeepFakes. Our research finding confirms\n",
      "effective privacy-preserving techniques for protecting personal photos from\n",
      "being DeepFaked.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In recent years, DeepFake is becoming a common threat to our society, due to\\nthe remarkable progress of generative adversarial networks (GAN) in image\\nsynthesis. Unfortunately, existing studies that propose various approaches, in\\nfighting against DeepFake and determining if the facial image is real or fake,\\nis still at an early stage. Obviously, the current DeepFake detection method\\nstruggles to catch the rapid progress of GANs, especially in the adversarial\\nscenarios where attackers can evade the detection intentionally, such as adding\\nperturbations to fool the DNN-based detectors. While passive detection simply\\ntells whether the image is fake or real, DeepFake provenance, on the other\\nhand, provides clues for tracking the sources in DeepFake forensics. Thus, the\\ntracked fake images could be blocked immediately by administrators and avoid\\nfurther spread in social networks.\\n  In this paper, we investigate the potentials of image tagging in serving the\\nDeepFake provenance tracking. Specifically, we devise a deep learning-based\\napproach, named FakeTagger, with a simple yet effective encoder and decoder\\ndesign along with channel coding to embed message to the facial image, which is\\nto recover the embedded message after various drastic GAN-based DeepFake\\ntransformation with high confidence. The embedded message could be employed to\\nrepresent the identity of facial images, which further contributed to DeepFake\\ndetection and provenance. Experimental results demonstrate that our proposed\\napproach could recover the embedded message with an average accuracy of more\\nthan 95% over the four common types of DeepFakes. Our research finding confirms\\neffective privacy-preserving techniques for protecting personal photos from\\nbeing DeepFaked.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Run Wang'}, {'name': 'Felix Juefei-Xu'}, {'name': 'Meng Luo'}, {'name': 'Yang Liu'}, {'name': 'Lina Wang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Lina Wang'}\n",
      "\n",
      "\n",
      "author\n",
      "Lina Wang\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to ACM Multimedia 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2009.09869v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2009.09869v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2009.09213v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2009.09213v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-11-25T08:53:51Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=25, tm_hour=8, tm_min=53, tm_sec=51, tm_wday=3, tm_yday=329, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-09-19T11:26:01Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=9, tm_mday=19, tm_hour=11, tm_min=26, tm_sec=1, tm_wday=5, tm_yday=263, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering'}\n",
      "\n",
      "\n",
      "summary\n",
      "The current high-fidelity generation and high-precision detection of DeepFake\n",
      "images are at an arms race. We believe that producing DeepFakes that are highly\n",
      "realistic and ``detection evasive'' can serve the ultimate goal of improving\n",
      "future generation DeepFake detection capabilities. In this paper, we propose a\n",
      "simple yet powerful pipeline to reduce the artifact patterns of fake images\n",
      "without hurting image quality by performing implicit spatial-domain notch\n",
      "filtering. We first demonstrate that frequency-domain notch filtering, although\n",
      "famously shown to be effective in removing periodic noise in the spatial\n",
      "domain, is infeasible for our task at hand due to manual designs required for\n",
      "the notch filters. We, therefore, resort to a learning-based approach to\n",
      "reproduce the notch filtering effects, but solely in the spatial domain. We\n",
      "adopt a combination of adding overwhelming spatial noise for breaking the\n",
      "periodic noise pattern and deep image filtering to reconstruct the noise-free\n",
      "fake images, and we name our method DeepNotch. Deep image filtering provides a\n",
      "specialized filter for each pixel in the noisy image, producing filtered images\n",
      "with high fidelity compared to their DeepFake counterparts. Moreover, we also\n",
      "use the semantic information of the image to generate an adversarial guidance\n",
      "map to add noise intelligently. Our large-scale evaluation on 3 representative\n",
      "state-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\n",
      "has demonstrated that our technique significantly reduces the accuracy of these\n",
      "3 fake image detection methods, 36.79% on average and up to 97.02% in the best\n",
      "case.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"The current high-fidelity generation and high-precision detection of DeepFake\\nimages are at an arms race. We believe that producing DeepFakes that are highly\\nrealistic and ``detection evasive'' can serve the ultimate goal of improving\\nfuture generation DeepFake detection capabilities. In this paper, we propose a\\nsimple yet powerful pipeline to reduce the artifact patterns of fake images\\nwithout hurting image quality by performing implicit spatial-domain notch\\nfiltering. We first demonstrate that frequency-domain notch filtering, although\\nfamously shown to be effective in removing periodic noise in the spatial\\ndomain, is infeasible for our task at hand due to manual designs required for\\nthe notch filters. We, therefore, resort to a learning-based approach to\\nreproduce the notch filtering effects, but solely in the spatial domain. We\\nadopt a combination of adding overwhelming spatial noise for breaking the\\nperiodic noise pattern and deep image filtering to reconstruct the noise-free\\nfake images, and we name our method DeepNotch. Deep image filtering provides a\\nspecialized filter for each pixel in the noisy image, producing filtered images\\nwith high fidelity compared to their DeepFake counterparts. Moreover, we also\\nuse the semantic information of the image to generate an adversarial guidance\\nmap to add noise intelligently. Our large-scale evaluation on 3 representative\\nstate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\\nhas demonstrated that our technique significantly reduces the accuracy of these\\n3 fake image detection methods, 36.79% on average and up to 97.02% in the best\\ncase.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yihao Huang'}, {'name': 'Felix Juefei-Xu'}, {'name': 'Qing Guo'}, {'name': 'Lei Ma'}, {'name': 'Xiaofei Xie'}, {'name': 'Weikai Miao'}, {'name': 'Yang Liu'}, {'name': 'Geguang Pu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Geguang Pu'}\n",
      "\n",
      "\n",
      "author\n",
      "Geguang Pu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "10 pages\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2009.09213v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2009.09213v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2009.07480v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2009.07480v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-09-16T05:57:06Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=9, tm_mday=16, tm_hour=5, tm_min=57, tm_sec=6, tm_wday=2, tm_yday=260, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-09-16T05:57:06Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=9, tm_mday=16, tm_hour=5, tm_min=57, tm_sec=6, tm_wday=2, tm_yday=260, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "A Convolutional LSTM based Residual Network for Deepfake Video Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'A Convolutional LSTM based Residual Network for Deepfake Video Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "In recent years, deep learning-based video manipulation methods have become\n",
      "widely accessible to masses. With little to no effort, people can easily learn\n",
      "how to generate deepfake videos with only a few victims or target images. This\n",
      "creates a significant social problem for everyone whose photos are publicly\n",
      "available on the Internet, especially on social media websites. Several deep\n",
      "learning-based detection methods have been developed to identify these\n",
      "deepfakes. However, these methods lack generalizability, because they perform\n",
      "well only for a specific type of deepfake method. Therefore, those methods are\n",
      "not transferable to detect other deepfake methods. Also, they do not take\n",
      "advantage of the temporal information of the video. In this paper, we addressed\n",
      "these limitations. We developed a Convolutional LSTM based Residual Network\n",
      "(CLRNet), which takes a sequence of consecutive images as an input from a video\n",
      "to learn the temporal information that helps in detecting unnatural looking\n",
      "artifacts that are present between frames of deepfake videos. We also propose a\n",
      "transfer learning-based approach to generalize different deepfake methods.\n",
      "Through rigorous experimentations using the FaceForensics++ dataset, we showed\n",
      "that our method outperforms five of the previously proposed state-of-the-art\n",
      "deepfake detection methods by better generalizing at detecting different\n",
      "deepfake methods using the same model.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In recent years, deep learning-based video manipulation methods have become\\nwidely accessible to masses. With little to no effort, people can easily learn\\nhow to generate deepfake videos with only a few victims or target images. This\\ncreates a significant social problem for everyone whose photos are publicly\\navailable on the Internet, especially on social media websites. Several deep\\nlearning-based detection methods have been developed to identify these\\ndeepfakes. However, these methods lack generalizability, because they perform\\nwell only for a specific type of deepfake method. Therefore, those methods are\\nnot transferable to detect other deepfake methods. Also, they do not take\\nadvantage of the temporal information of the video. In this paper, we addressed\\nthese limitations. We developed a Convolutional LSTM based Residual Network\\n(CLRNet), which takes a sequence of consecutive images as an input from a video\\nto learn the temporal information that helps in detecting unnatural looking\\nartifacts that are present between frames of deepfake videos. We also propose a\\ntransfer learning-based approach to generalize different deepfake methods.\\nThrough rigorous experimentations using the FaceForensics++ dataset, we showed\\nthat our method outperforms five of the previously proposed state-of-the-art\\ndeepfake detection methods by better generalizing at detecting different\\ndeepfake methods using the same model.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shahroz Tariq'}, {'name': 'Sangyup Lee'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2009.07480v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2009.07480v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.4.9; I.5.4', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2009.03155v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2009.03155v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-09-07T15:20:37Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=9, tm_mday=7, tm_hour=15, tm_min=20, tm_sec=37, tm_wday=0, tm_yday=251, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-09-07T15:20:37Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=9, tm_mday=7, tm_hour=15, tm_min=20, tm_sec=37, tm_wday=0, tm_yday=251, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake detection: humans vs. machines\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake detection: humans vs. machines'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake videos, where a person's face is automatically swapped with a face\n",
      "of someone else, are becoming easier to generate with more realistic results.\n",
      "In response to the threat such manipulations can pose to our trust in video\n",
      "evidence, several large datasets of deepfake videos and many methods to detect\n",
      "them were proposed recently. However, it is still unclear how realistic\n",
      "deepfake videos are for an average person and whether the algorithms are\n",
      "significantly better than humans at detecting them. In this paper, we present a\n",
      "subjective study conducted in a crowdsourcing-like scenario, which\n",
      "systematically evaluates how hard it is for humans to see if the video is\n",
      "deepfake or not. For the evaluation, we used 120 different videos (60 deepfakes\n",
      "and 60 originals) manually pre-selected from the Facebook deepfake database,\n",
      "which was provided in the Kaggle's Deepfake Detection Challenge 2020. For each\n",
      "video, a simple question: \"Is face of the person in the video real of fake?\"\n",
      "was answered on average by 19 na\\\"ive subjects. The results of the subjective\n",
      "evaluation were compared with the performance of two different state of the art\n",
      "deepfake detection methods, based on Xception and EfficientNets (B4 variant)\n",
      "neural networks, which were pre-trained on two other large public databases:\n",
      "the Google's subset from FaceForensics++ and the recent Celeb-DF dataset. The\n",
      "evaluation demonstrates that while the human perception is very different from\n",
      "the perception of a machine, both successfully but in different ways are fooled\n",
      "by deepfakes. Specifically, algorithms struggle to detect those deepfake\n",
      "videos, which human subjects found to be very easy to spot.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake videos, where a person\\'s face is automatically swapped with a face\\nof someone else, are becoming easier to generate with more realistic results.\\nIn response to the threat such manipulations can pose to our trust in video\\nevidence, several large datasets of deepfake videos and many methods to detect\\nthem were proposed recently. However, it is still unclear how realistic\\ndeepfake videos are for an average person and whether the algorithms are\\nsignificantly better than humans at detecting them. In this paper, we present a\\nsubjective study conducted in a crowdsourcing-like scenario, which\\nsystematically evaluates how hard it is for humans to see if the video is\\ndeepfake or not. For the evaluation, we used 120 different videos (60 deepfakes\\nand 60 originals) manually pre-selected from the Facebook deepfake database,\\nwhich was provided in the Kaggle\\'s Deepfake Detection Challenge 2020. For each\\nvideo, a simple question: \"Is face of the person in the video real of fake?\"\\nwas answered on average by 19 na\\\\\"ive subjects. The results of the subjective\\nevaluation were compared with the performance of two different state of the art\\ndeepfake detection methods, based on Xception and EfficientNets (B4 variant)\\nneural networks, which were pre-trained on two other large public databases:\\nthe Google\\'s subset from FaceForensics++ and the recent Celeb-DF dataset. The\\nevaluation demonstrates that while the human perception is very different from\\nthe perception of a machine, both successfully but in different ways are fooled\\nby deepfakes. Specifically, algorithms struggle to detect those deepfake\\nvideos, which human subjects found to be very easy to spot.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Pavel Korshunov'}, {'name': 'Sébastien Marcel'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Sébastien Marcel'}\n",
      "\n",
      "\n",
      "author\n",
      "Sébastien Marcel\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2009.03155v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2009.03155v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2008.12262v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2008.12262v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-27T17:04:46Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=27, tm_hour=17, tm_min=4, tm_sec=46, tm_wday=3, tm_yday=240, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-08-27T17:04:46Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=27, tm_hour=17, tm_min=4, tm_sec=46, tm_wday=3, tm_yday=240, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFake Detection Based on the Discrepancy Between the Face and its\n",
      "  Context\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFake Detection Based on the Discrepancy Between the Face and its\\n  Context'}\n",
      "\n",
      "\n",
      "summary\n",
      "We propose a method for detecting face swapping and other identity\n",
      "manipulations in single images. Face swapping methods, such as DeepFake,\n",
      "manipulate the face region, aiming to adjust the face to the appearance of its\n",
      "context, while leaving the context unchanged. We show that this modus operandi\n",
      "produces discrepancies between the two regions. These discrepancies offer\n",
      "exploitable telltale signs of manipulation. Our approach involves two networks:\n",
      "(i) a face identification network that considers the face region bounded by a\n",
      "tight semantic segmentation, and (ii) a context recognition network that\n",
      "considers the face context (e.g., hair, ears, neck). We describe a method which\n",
      "uses the recognition signals from our two networks to detect such\n",
      "discrepancies, providing a complementary detection signal that improves\n",
      "conventional real vs. fake classifiers commonly used for detecting fake images.\n",
      "Our method achieves state of the art results on the FaceForensics++,\n",
      "Celeb-DF-v2, and DFDC benchmarks for face manipulation detection, and even\n",
      "generalizes to detect fakes produced by unseen methods.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'We propose a method for detecting face swapping and other identity\\nmanipulations in single images. Face swapping methods, such as DeepFake,\\nmanipulate the face region, aiming to adjust the face to the appearance of its\\ncontext, while leaving the context unchanged. We show that this modus operandi\\nproduces discrepancies between the two regions. These discrepancies offer\\nexploitable telltale signs of manipulation. Our approach involves two networks:\\n(i) a face identification network that considers the face region bounded by a\\ntight semantic segmentation, and (ii) a context recognition network that\\nconsiders the face context (e.g., hair, ears, neck). We describe a method which\\nuses the recognition signals from our two networks to detect such\\ndiscrepancies, providing a complementary detection signal that improves\\nconventional real vs. fake classifiers commonly used for detecting fake images.\\nOur method achieves state of the art results on the FaceForensics++,\\nCeleb-DF-v2, and DFDC benchmarks for face manipulation detection, and even\\ngeneralizes to detect fakes produced by unseen methods.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yuval Nirkin'}, {'name': 'Lior Wolf'}, {'name': 'Yosi Keller'}, {'name': 'Tal Hassner'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Tal Hassner'}\n",
      "\n",
      "\n",
      "author\n",
      "Tal Hassner\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2008.12262v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2008.12262v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2008.12199v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2008.12199v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-09T03:43:39Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=9, tm_hour=3, tm_min=43, tm_sec=39, tm_wday=6, tm_yday=129, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-08-27T15:52:16Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=27, tm_hour=15, tm_min=52, tm_sec=16, tm_wday=3, tm_yday=240, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Privacy Intelligence: A Survey on Image Privacy in Online Social\n",
      "  Networks\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Privacy Intelligence: A Survey on Image Privacy in Online Social\\n  Networks'}\n",
      "\n",
      "\n",
      "summary\n",
      "Image sharing on online social networks (OSNs) has become an indispensable\n",
      "part of daily social activities, but it has also led to an increased risk of\n",
      "privacy invasion. The recent image leaks from popular OSN services and the\n",
      "abuse of personal photos using advanced algorithms (e.g. DeepFake) have\n",
      "prompted the public to rethink individual privacy needs in OSN image sharing.\n",
      "However, OSN image privacy itself is quite complicated, and solutions currently\n",
      "in place for privacy management in reality are insufficient to provide\n",
      "personalized, accurate and flexible privacy protection. A more intelligent\n",
      "environment for privacy-friendly OSN image sharing is in demand. To fill the\n",
      "gap, we contribute a survey of \"privacy intelligence\" that targets modern\n",
      "privacy issues in dynamic OSN image sharing from a user-centric perspective.\n",
      "Specifically, we present a definition and a taxonomy of OSN image privacy, and\n",
      "a high-level privacy analysis framework based on the lifecycle of OSN image\n",
      "sharing. The framework consists of three stages with different principles of\n",
      "privacy by design. At each stage, we identify typical user behaviors in OSN\n",
      "image sharing and the privacy issues associated with these behaviors. Then a\n",
      "systematic review on the representative intelligent solutions targeting those\n",
      "privacy issues is conducted, also in a stage-based manner. The resulting\n",
      "analysis describes an intelligent privacy firewall for closed-loop privacy\n",
      "management. We also discuss the challenges and future directions in this area.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Image sharing on online social networks (OSNs) has become an indispensable\\npart of daily social activities, but it has also led to an increased risk of\\nprivacy invasion. The recent image leaks from popular OSN services and the\\nabuse of personal photos using advanced algorithms (e.g. DeepFake) have\\nprompted the public to rethink individual privacy needs in OSN image sharing.\\nHowever, OSN image privacy itself is quite complicated, and solutions currently\\nin place for privacy management in reality are insufficient to provide\\npersonalized, accurate and flexible privacy protection. A more intelligent\\nenvironment for privacy-friendly OSN image sharing is in demand. To fill the\\ngap, we contribute a survey of \"privacy intelligence\" that targets modern\\nprivacy issues in dynamic OSN image sharing from a user-centric perspective.\\nSpecifically, we present a definition and a taxonomy of OSN image privacy, and\\na high-level privacy analysis framework based on the lifecycle of OSN image\\nsharing. The framework consists of three stages with different principles of\\nprivacy by design. At each stage, we identify typical user behaviors in OSN\\nimage sharing and the privacy issues associated with these behaviors. Then a\\nsystematic review on the representative intelligent solutions targeting those\\nprivacy issues is conducted, also in a stage-based manner. The resulting\\nanalysis describes an intelligent privacy firewall for closed-loop privacy\\nmanagement. We also discuss the challenges and future directions in this area.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Chi Liu'}, {'name': 'Tianqing Zhu'}, {'name': 'Jun Zhang'}, {'name': 'Wanlei Zhou'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Wanlei Zhou'}\n",
      "\n",
      "\n",
      "author\n",
      "Wanlei Zhou\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "32 pages, 9 figures. Under review\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2008.12199v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2008.12199v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2008.09194v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2008.09194v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-03T21:41:33Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=3, tm_hour=21, tm_min=41, tm_sec=33, tm_wday=2, tm_yday=62, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-08-20T20:25:18Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=20, tm_hour=20, tm_min=25, tm_sec=18, tm_wday=3, tm_yday=233, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "On Attribution of Deepfakes\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'On Attribution of Deepfakes'}\n",
      "\n",
      "\n",
      "summary\n",
      "Progress in generative modelling, especially generative adversarial networks,\n",
      "have made it possible to efficiently synthesize and alter media at scale.\n",
      "Malicious individuals now rely on these machine-generated media, or deepfakes,\n",
      "to manipulate social discourse. In order to ensure media authenticity, existing\n",
      "research is focused on deepfake detection. Yet, the adversarial nature of\n",
      "frameworks used for generative modeling suggests that progress towards\n",
      "detecting deepfakes will enable more realistic deepfake generation. Therefore,\n",
      "it comes at no surprise that developers of generative models are under the\n",
      "scrutiny of stakeholders dealing with misinformation campaigns. At the same\n",
      "time, generative models have a lot of positive applications. As such, there is\n",
      "a clear need to develop tools that ensure the transparent use of generative\n",
      "modeling, while minimizing the harm caused by malicious applications.\n",
      "  Our technique optimizes over the source of entropy of each generative model\n",
      "to probabilistically attribute a deepfake to one of the models. We evaluate our\n",
      "method on the seminal example of face synthesis, demonstrating that our\n",
      "approach achieves 97.62% attribution accuracy, and is less sensitive to\n",
      "perturbations and adversarial examples. We discuss the ethical implications of\n",
      "our work, identify where our technique can be used, and highlight that a more\n",
      "meaningful legislative framework is required for a more transparent and ethical\n",
      "use of generative modeling. Finally, we argue that model developers should be\n",
      "capable of claiming plausible deniability and propose a second framework to do\n",
      "so -- this allows a model developer to produce evidence that they did not\n",
      "produce media that they are being accused of having produced.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Progress in generative modelling, especially generative adversarial networks,\\nhave made it possible to efficiently synthesize and alter media at scale.\\nMalicious individuals now rely on these machine-generated media, or deepfakes,\\nto manipulate social discourse. In order to ensure media authenticity, existing\\nresearch is focused on deepfake detection. Yet, the adversarial nature of\\nframeworks used for generative modeling suggests that progress towards\\ndetecting deepfakes will enable more realistic deepfake generation. Therefore,\\nit comes at no surprise that developers of generative models are under the\\nscrutiny of stakeholders dealing with misinformation campaigns. At the same\\ntime, generative models have a lot of positive applications. As such, there is\\na clear need to develop tools that ensure the transparent use of generative\\nmodeling, while minimizing the harm caused by malicious applications.\\n  Our technique optimizes over the source of entropy of each generative model\\nto probabilistically attribute a deepfake to one of the models. We evaluate our\\nmethod on the seminal example of face synthesis, demonstrating that our\\napproach achieves 97.62% attribution accuracy, and is less sensitive to\\nperturbations and adversarial examples. We discuss the ethical implications of\\nour work, identify where our technique can be used, and highlight that a more\\nmeaningful legislative framework is required for a more transparent and ethical\\nuse of generative modeling. Finally, we argue that model developers should be\\ncapable of claiming plausible deniability and propose a second framework to do\\nso -- this allows a model developer to produce evidence that they did not\\nproduce media that they are being accused of having produced.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Baiwu Zhang'}, {'name': 'Jin Peng Zhou'}, {'name': 'Ilia Shumailov'}, {'name': 'Nicolas Papernot'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Nicolas Papernot'}\n",
      "\n",
      "\n",
      "author\n",
      "Nicolas Papernot\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2008.09194v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2008.09194v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2008.04848v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2008.04848v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-11T16:47:02Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=11, tm_hour=16, tm_min=47, tm_sec=2, tm_wday=1, tm_yday=224, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-08-11T16:47:02Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=11, tm_hour=16, tm_min=47, tm_sec=2, tm_wday=1, tm_yday=224, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent deep learning based video synthesis approaches, in particular with\n",
      "applications that can forge identities such as \"DeepFake\", have raised great\n",
      "security concerns. Therefore, corresponding deep forensic methods are proposed\n",
      "to tackle this problem. However, existing methods are either based on\n",
      "unexplainable deep networks which greatly degrades the principal\n",
      "interpretability factor to media forensic, or rely on fragile image statistics\n",
      "such as noise pattern, which in real-world scenarios can be easily deteriorated\n",
      "by data compression. In this paper, we propose an fully-interpretable video\n",
      "forensic method that is designed specifically to expose deep-faked videos. To\n",
      "enhance generalizability on videos with various content, we model the temporal\n",
      "motion of multiple specific spatial locations in the videos to extract a robust\n",
      "and reliable representation, called Co-Motion Pattern. Such kind of conjoint\n",
      "pattern is mined across local motion features which is independent of the video\n",
      "contents so that the instance-wise variation can also be largely alleviated.\n",
      "More importantly, our proposed co-motion pattern possesses both superior\n",
      "interpretability and sufficient robustness against data compression for\n",
      "deep-faked videos. We conduct extensive experiments to empirically demonstrate\n",
      "the superiority and effectiveness of our approach under both classification and\n",
      "anomaly detection evaluation settings against the state-of-the-art deep\n",
      "forensic methods.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Recent deep learning based video synthesis approaches, in particular with\\napplications that can forge identities such as \"DeepFake\", have raised great\\nsecurity concerns. Therefore, corresponding deep forensic methods are proposed\\nto tackle this problem. However, existing methods are either based on\\nunexplainable deep networks which greatly degrades the principal\\ninterpretability factor to media forensic, or rely on fragile image statistics\\nsuch as noise pattern, which in real-world scenarios can be easily deteriorated\\nby data compression. In this paper, we propose an fully-interpretable video\\nforensic method that is designed specifically to expose deep-faked videos. To\\nenhance generalizability on videos with various content, we model the temporal\\nmotion of multiple specific spatial locations in the videos to extract a robust\\nand reliable representation, called Co-Motion Pattern. Such kind of conjoint\\npattern is mined across local motion features which is independent of the video\\ncontents so that the instance-wise variation can also be largely alleviated.\\nMore importantly, our proposed co-motion pattern possesses both superior\\ninterpretability and sufficient robustness against data compression for\\ndeep-faked videos. We conduct extensive experiments to empirically demonstrate\\nthe superiority and effectiveness of our approach under both classification and\\nanomaly detection evaluation settings against the state-of-the-art deep\\nforensic methods.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Gengxing Wang'}, {'name': 'Jiahuan Zhou'}, {'name': 'Ying Wu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ying Wu'}\n",
      "\n",
      "\n",
      "author\n",
      "Ying Wu\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2008.04848v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2008.04848v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2008.04585v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2008.04585v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-11T08:52:17Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=11, tm_hour=8, tm_min=52, tm_sec=17, tm_wday=1, tm_yday=224, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-08-11T08:52:17Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=11, tm_hour=8, tm_min=52, tm_sec=17, tm_wday=1, tm_yday=224, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Sharp Multiple Instance Learning for DeepFake Video Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Sharp Multiple Instance Learning for DeepFake Video Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the rapid development of facial manipulation techniques, face forgery\n",
      "has received considerable attention in multimedia and computer vision community\n",
      "due to security concerns. Existing methods are mostly designed for single-frame\n",
      "detection trained with precise image-level labels or for video-level prediction\n",
      "by only modeling the inter-frame inconsistency, leaving potential high risks\n",
      "for DeepFake attackers. In this paper, we introduce a new problem of partial\n",
      "face attack in DeepFake video, where only video-level labels are provided but\n",
      "not all the faces in the fake videos are manipulated. We address this problem\n",
      "by multiple instance learning framework, treating faces and input video as\n",
      "instances and bag respectively. A sharp MIL (S-MIL) is proposed which builds\n",
      "direct mapping from instance embeddings to bag prediction, rather than from\n",
      "instance embeddings to instance prediction and then to bag prediction in\n",
      "traditional MIL. Theoretical analysis proves that the gradient vanishing in\n",
      "traditional MIL is relieved in S-MIL. To generate instances that can accurately\n",
      "incorporate the partially manipulated faces, spatial-temporal encoded instance\n",
      "is designed to fully model the intra-frame and inter-frame inconsistency, which\n",
      "further helps to promote the detection performance. We also construct a new\n",
      "dataset FFPMS for partially attacked DeepFake video detection, which can\n",
      "benefit the evaluation of different methods at both frame and video levels.\n",
      "Experiments on FFPMS and the widely used DFDC dataset verify that S-MIL is\n",
      "superior to other counterparts for partially attacked DeepFake video detection.\n",
      "In addition, S-MIL can also be adapted to traditional DeepFake image detection\n",
      "tasks and achieve state-of-the-art performance on single-frame datasets.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'With the rapid development of facial manipulation techniques, face forgery\\nhas received considerable attention in multimedia and computer vision community\\ndue to security concerns. Existing methods are mostly designed for single-frame\\ndetection trained with precise image-level labels or for video-level prediction\\nby only modeling the inter-frame inconsistency, leaving potential high risks\\nfor DeepFake attackers. In this paper, we introduce a new problem of partial\\nface attack in DeepFake video, where only video-level labels are provided but\\nnot all the faces in the fake videos are manipulated. We address this problem\\nby multiple instance learning framework, treating faces and input video as\\ninstances and bag respectively. A sharp MIL (S-MIL) is proposed which builds\\ndirect mapping from instance embeddings to bag prediction, rather than from\\ninstance embeddings to instance prediction and then to bag prediction in\\ntraditional MIL. Theoretical analysis proves that the gradient vanishing in\\ntraditional MIL is relieved in S-MIL. To generate instances that can accurately\\nincorporate the partially manipulated faces, spatial-temporal encoded instance\\nis designed to fully model the intra-frame and inter-frame inconsistency, which\\nfurther helps to promote the detection performance. We also construct a new\\ndataset FFPMS for partially attacked DeepFake video detection, which can\\nbenefit the evaluation of different methods at both frame and video levels.\\nExperiments on FFPMS and the widely used DFDC dataset verify that S-MIL is\\nsuperior to other counterparts for partially attacked DeepFake video detection.\\nIn addition, S-MIL can also be adapted to traditional DeepFake image detection\\ntasks and achieve state-of-the-art performance on single-frame datasets.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Xiaodan Li'}, {'name': 'Yining Lang'}, {'name': 'Yuefeng Chen'}, {'name': 'Xiaofeng Mao'}, {'name': 'Yuan He'}, {'name': 'Shuhui Wang'}, {'name': 'Hui Xue'}, {'name': 'Quan Lu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Quan Lu'}\n",
      "\n",
      "\n",
      "author\n",
      "Quan Lu\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1145/3394171.3414034\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3394171.3414034', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2008.04585v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2008.04585v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted at ACM MM 2020. 11 pages, 8 figures, with appendix\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "Proceedings of the 28th ACM International Conference on\n",
      "  Multimedia, 2020\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2008.03412v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2008.03412v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-09-04T01:03:55Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=9, tm_mday=4, tm_hour=1, tm_min=3, tm_sec=55, tm_wday=4, tm_yday=248, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-08-08T01:38:56Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=8, tm_hour=1, tm_min=38, tm_sec=56, tm_wday=5, tm_yday=221, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Two-branch Recurrent Network for Isolating Deepfakes in Videos\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Two-branch Recurrent Network for Isolating Deepfakes in Videos'}\n",
      "\n",
      "\n",
      "summary\n",
      "The current spike of hyper-realistic faces artificially generated using\n",
      "deepfakes calls for media forensics solutions that are tailored to video\n",
      "streams and work reliably with a low false alarm rate at the video level. We\n",
      "present a method for deepfake detection based on a two-branch network structure\n",
      "that isolates digitally manipulated faces by learning to amplify artifacts\n",
      "while suppressing the high-level face content. Unlike current methods that\n",
      "extract spatial frequencies as a preprocessing step, we propose a two-branch\n",
      "structure: one branch propagates the original information, while the other\n",
      "branch suppresses the face content yet amplifies multi-band frequencies using a\n",
      "Laplacian of Gaussian (LoG) as a bottleneck layer. To better isolate\n",
      "manipulated faces, we derive a novel cost function that, unlike regular\n",
      "classification, compresses the variability of natural faces and pushes away the\n",
      "unrealistic facial samples in the feature space. Our two novel components show\n",
      "promising results on the FaceForensics++, Celeb-DF, and Facebook's DFDC preview\n",
      "benchmarks, when compared to prior work. We then offer a full, detailed\n",
      "ablation study of our network architecture and cost function. Finally, although\n",
      "the bar is still high to get very remarkable figures at a very low false alarm\n",
      "rate, our study shows that we can achieve good video-level performance when\n",
      "cross-testing in terms of video-level AUC.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"The current spike of hyper-realistic faces artificially generated using\\ndeepfakes calls for media forensics solutions that are tailored to video\\nstreams and work reliably with a low false alarm rate at the video level. We\\npresent a method for deepfake detection based on a two-branch network structure\\nthat isolates digitally manipulated faces by learning to amplify artifacts\\nwhile suppressing the high-level face content. Unlike current methods that\\nextract spatial frequencies as a preprocessing step, we propose a two-branch\\nstructure: one branch propagates the original information, while the other\\nbranch suppresses the face content yet amplifies multi-band frequencies using a\\nLaplacian of Gaussian (LoG) as a bottleneck layer. To better isolate\\nmanipulated faces, we derive a novel cost function that, unlike regular\\nclassification, compresses the variability of natural faces and pushes away the\\nunrealistic facial samples in the feature space. Our two novel components show\\npromising results on the FaceForensics++, Celeb-DF, and Facebook's DFDC preview\\nbenchmarks, when compared to prior work. We then offer a full, detailed\\nablation study of our network architecture and cost function. Finally, although\\nthe bar is still high to get very remarkable figures at a very low false alarm\\nrate, our study shows that we can achieve good video-level performance when\\ncross-testing in terms of video-level AUC.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Iacopo Masi'}, {'name': 'Aditya Killekar'}, {'name': 'Royston Marian Mascarenhas'}, {'name': 'Shenoy Pratik Gurudatt'}, {'name': 'Wael AbdAlmageed'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Wael AbdAlmageed'}\n",
      "\n",
      "\n",
      "author\n",
      "Wael AbdAlmageed\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To appear in the 16th European Conference on Computer Vision ECCV\n",
      "  2020 (added link to our demo and to the video presentation)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2008.03412v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2008.03412v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2008.04095v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2008.04095v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-07T08:49:23Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=7, tm_hour=8, tm_min=49, tm_sec=23, tm_wday=4, tm_yday=220, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-08-07T08:49:23Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=7, tm_hour=8, tm_min=49, tm_sec=23, tm_wday=4, tm_yday=220, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Fighting Deepfake by Exposing the Convolutional Traces on Images\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Fighting Deepfake by Exposing the Convolutional Traces on Images'}\n",
      "\n",
      "\n",
      "summary\n",
      "Advances in Artificial Intelligence and Image Processing are changing the way\n",
      "people interacts with digital images and video. Widespread mobile apps like\n",
      "FACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to\n",
      "produce extreme transformations on human face photos such gender swap, aging,\n",
      "etc. The results are utterly realistic and extremely easy to be exploited even\n",
      "for non-experienced users. This kind of media object took the name of Deepfake\n",
      "and raised a new challenge in the multimedia forensics field: the Deepfake\n",
      "detection challenge. Indeed, discriminating a Deepfake from a real image could\n",
      "be a difficult task even for human eyes but recent works are trying to apply\n",
      "the same technology used for generating images for discriminating them with\n",
      "preliminary good results but with many limitations: employed Convolutional\n",
      "Neural Networks are not so robust, demonstrate to be specific to the context\n",
      "and tend to extract semantics from images. In this paper, a new approach aimed\n",
      "to extract a Deepfake fingerprint from images is proposed. The method is based\n",
      "on the Expectation-Maximization algorithm trained to detect and extract a\n",
      "fingerprint that represents the Convolutional Traces (CT) left by GANs during\n",
      "image generation. The CT demonstrates to have high discriminative power\n",
      "achieving better results than state-of-the-art in the Deepfake detection task\n",
      "also proving to be robust to different attacks. Achieving an overall\n",
      "classification accuracy of over 98%, considering Deepfakes from 10 different\n",
      "GAN architectures not only involved in images of faces, the CT demonstrates to\n",
      "be reliable and without any dependence on image semantic. Finally, tests\n",
      "carried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the\n",
      "fake detection task, demonstrated the effectiveness of the proposed technique\n",
      "on a real-case scenario.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Advances in Artificial Intelligence and Image Processing are changing the way\\npeople interacts with digital images and video. Widespread mobile apps like\\nFACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to\\nproduce extreme transformations on human face photos such gender swap, aging,\\netc. The results are utterly realistic and extremely easy to be exploited even\\nfor non-experienced users. This kind of media object took the name of Deepfake\\nand raised a new challenge in the multimedia forensics field: the Deepfake\\ndetection challenge. Indeed, discriminating a Deepfake from a real image could\\nbe a difficult task even for human eyes but recent works are trying to apply\\nthe same technology used for generating images for discriminating them with\\npreliminary good results but with many limitations: employed Convolutional\\nNeural Networks are not so robust, demonstrate to be specific to the context\\nand tend to extract semantics from images. In this paper, a new approach aimed\\nto extract a Deepfake fingerprint from images is proposed. The method is based\\non the Expectation-Maximization algorithm trained to detect and extract a\\nfingerprint that represents the Convolutional Traces (CT) left by GANs during\\nimage generation. The CT demonstrates to have high discriminative power\\nachieving better results than state-of-the-art in the Deepfake detection task\\nalso proving to be robust to different attacks. Achieving an overall\\nclassification accuracy of over 98%, considering Deepfakes from 10 different\\nGAN architectures not only involved in images of faces, the CT demonstrates to\\nbe reliable and without any dependence on image semantic. Finally, tests\\ncarried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the\\nfake detection task, demonstrated the effectiveness of the proposed technique\\non a real-case scenario.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Luca Guarnera'}, {'name': 'Oliver Giudice'}, {'name': 'Sebastiano Battiato'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Sebastiano Battiato'}\n",
      "\n",
      "\n",
      "arxiv_affiliation\n",
      "iCTLab s.r.l. - Spin-off of University of Catania\n",
      "\n",
      "\n",
      "author\n",
      "Sebastiano Battiato\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1109/ACCESS.2020.3023037\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1109/ACCESS.2020.3023037', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2008.04095v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2008.04095v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "arXiv admin note: text overlap with arXiv:2004.10448\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "IEEE Access 2020\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2008.00036v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2008.00036v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-06T16:40:26Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=6, tm_hour=16, tm_min=40, tm_sec=26, tm_wday=3, tm_yday=126, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-07-31T19:01:13Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=7, tm_mday=31, tm_hour=19, tm_min=1, tm_sec=13, tm_wday=4, tm_yday=213, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "TweepFake: about Detecting Deepfake Tweets\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'TweepFake: about Detecting Deepfake Tweets'}\n",
      "\n",
      "\n",
      "summary\n",
      "The recent advances in language modeling significantly improved the\n",
      "generative capabilities of deep neural models: in 2019 OpenAI released GPT-2, a\n",
      "pre-trained language model that can autonomously generate coherent, non-trivial\n",
      "and human-like text samples. Since then, ever more powerful text generative\n",
      "models have been developed. Adversaries can exploit these tremendous generative\n",
      "capabilities to enhance social bots that will have the ability to write\n",
      "plausible deepfake messages, hoping to contaminate public debate. To prevent\n",
      "this, it is crucial to develop deepfake social media messages detection\n",
      "systems. However, to the best of our knowledge no one has ever addressed the\n",
      "detection of machine-generated texts on social networks like Twitter or\n",
      "Facebook. With the aim of helping the research in this detection field, we\n",
      "collected the first dataset of \\real deepfake tweets, TweepFake. It is real in\n",
      "the sense that each deepfake tweet was actually posted on Twitter. We collected\n",
      "tweets from a total of 23 bots, imitating 17 human accounts. The bots are based\n",
      "on various generation techniques, i.e., Markov Chains, RNN, RNN+Markov, LSTM,\n",
      "GPT-2. We also randomly selected tweets from the humans imitated by the bots to\n",
      "have an overall balanced dataset of 25,572 tweets (half human and half bots\n",
      "generated). The dataset is publicly available on Kaggle. Lastly, we evaluated\n",
      "13 deepfake text detection methods (based on various state-of-the-art\n",
      "approaches) to both demonstrate the challenges that Tweepfake poses and create\n",
      "a solid baseline of detection techniques. We hope that TweepFake can offer the\n",
      "opportunity to tackle the deepfake detection on social media messages as well.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The recent advances in language modeling significantly improved the\\ngenerative capabilities of deep neural models: in 2019 OpenAI released GPT-2, a\\npre-trained language model that can autonomously generate coherent, non-trivial\\nand human-like text samples. Since then, ever more powerful text generative\\nmodels have been developed. Adversaries can exploit these tremendous generative\\ncapabilities to enhance social bots that will have the ability to write\\nplausible deepfake messages, hoping to contaminate public debate. To prevent\\nthis, it is crucial to develop deepfake social media messages detection\\nsystems. However, to the best of our knowledge no one has ever addressed the\\ndetection of machine-generated texts on social networks like Twitter or\\nFacebook. With the aim of helping the research in this detection field, we\\ncollected the first dataset of \\\\real deepfake tweets, TweepFake. It is real in\\nthe sense that each deepfake tweet was actually posted on Twitter. We collected\\ntweets from a total of 23 bots, imitating 17 human accounts. The bots are based\\non various generation techniques, i.e., Markov Chains, RNN, RNN+Markov, LSTM,\\nGPT-2. We also randomly selected tweets from the humans imitated by the bots to\\nhave an overall balanced dataset of 25,572 tweets (half human and half bots\\ngenerated). The dataset is publicly available on Kaggle. Lastly, we evaluated\\n13 deepfake text detection methods (based on various state-of-the-art\\napproaches) to both demonstrate the challenges that Tweepfake poses and create\\na solid baseline of detection techniques. We hope that TweepFake can offer the\\nopportunity to tackle the deepfake detection on social media messages as well.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Tiziano Fagni'}, {'name': 'Fabrizio Falchi'}, {'name': 'Margherita Gambini'}, {'name': 'Antonio Martella'}, {'name': 'Maurizio Tesconi'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Maurizio Tesconi'}\n",
      "\n",
      "\n",
      "author\n",
      "Maurizio Tesconi\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1371/journal.pone.0251415\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1371/journal.pone.0251415', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2008.00036v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2008.00036v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2007.08457v7\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2007.08457v7\n",
      "\n",
      "\n",
      "updated\n",
      "2022-03-17T20:45:53Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=20, tm_min=45, tm_sec=53, tm_wday=3, tm_yday=76, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-07-16T16:49:55Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=7, tm_mday=16, tm_hour=16, tm_min=49, tm_sec=55, tm_wday=3, tm_yday=198, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Artificial Fingerprinting for Generative Models: Rooting Deepfake\n",
      "  Attribution in Training Data\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Artificial Fingerprinting for Generative Models: Rooting Deepfake\\n  Attribution in Training Data'}\n",
      "\n",
      "\n",
      "summary\n",
      "Photorealistic image generation has reached a new level of quality due to the\n",
      "breakthroughs of generative adversarial networks (GANs). Yet, the dark side of\n",
      "such deepfakes, the malicious use of generated media, raises concerns about\n",
      "visual misinformation. While existing research work on deepfake detection\n",
      "demonstrates high accuracy, it is subject to advances in generation techniques\n",
      "and adversarial iterations on detection countermeasure techniques. Thus, we\n",
      "seek a proactive and sustainable solution on deepfake detection, that is\n",
      "agnostic to the evolution of generative models, by introducing artificial\n",
      "fingerprints into the models.\n",
      "  Our approach is simple and effective. We first embed artificial fingerprints\n",
      "into training data, then validate a surprising discovery on the transferability\n",
      "of such fingerprints from training data to generative models, which in turn\n",
      "appears in the generated deepfakes. Experiments show that our fingerprinting\n",
      "solution (1) holds for a variety of cutting-edge generative models, (2) leads\n",
      "to a negligible side effect on generation quality, (3) stays robust against\n",
      "image-level and model-level perturbations, (4) stays hard to be detected by\n",
      "adversaries, and (5) converts deepfake detection and attribution into trivial\n",
      "tasks and outperforms the recent state-of-the-art baselines. Our solution\n",
      "closes the responsibility loop between publishing pre-trained generative model\n",
      "inventions and their possible misuses, which makes it independent of the\n",
      "current arms race. Code and models are available at\n",
      "https://github.com/ningyu1991/ArtificialGANFingerprints .\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Photorealistic image generation has reached a new level of quality due to the\\nbreakthroughs of generative adversarial networks (GANs). Yet, the dark side of\\nsuch deepfakes, the malicious use of generated media, raises concerns about\\nvisual misinformation. While existing research work on deepfake detection\\ndemonstrates high accuracy, it is subject to advances in generation techniques\\nand adversarial iterations on detection countermeasure techniques. Thus, we\\nseek a proactive and sustainable solution on deepfake detection, that is\\nagnostic to the evolution of generative models, by introducing artificial\\nfingerprints into the models.\\n  Our approach is simple and effective. We first embed artificial fingerprints\\ninto training data, then validate a surprising discovery on the transferability\\nof such fingerprints from training data to generative models, which in turn\\nappears in the generated deepfakes. Experiments show that our fingerprinting\\nsolution (1) holds for a variety of cutting-edge generative models, (2) leads\\nto a negligible side effect on generation quality, (3) stays robust against\\nimage-level and model-level perturbations, (4) stays hard to be detected by\\nadversaries, and (5) converts deepfake detection and attribution into trivial\\ntasks and outperforms the recent state-of-the-art baselines. Our solution\\ncloses the responsibility loop between publishing pre-trained generative model\\ninventions and their possible misuses, which makes it independent of the\\ncurrent arms race. Code and models are available at\\nhttps://github.com/ningyu1991/ArtificialGANFingerprints .'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ning Yu'}, {'name': 'Vladislav Skripniuk'}, {'name': 'Sahar Abdelnabi'}, {'name': 'Mario Fritz'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Mario Fritz'}\n",
      "\n",
      "\n",
      "author\n",
      "Mario Fritz\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to ICCV'21 as Oral\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2007.08457v7', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2007.08457v7', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.GR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2007.08517v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2007.08517v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-07-15T20:36:23Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=7, tm_mday=15, tm_hour=20, tm_min=36, tm_sec=23, tm_wday=2, tm_yday=197, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-07-15T20:36:23Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=7, tm_mday=15, tm_hour=20, tm_min=36, tm_sec=23, tm_wday=2, tm_yday=197, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Detecting Deepfake Videos: An Analysis of Three Techniques\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Detecting Deepfake Videos: An Analysis of Three Techniques'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent advances in deepfake generating algorithms that produce manipulated\n",
      "media have had dangerous implications in privacy, security and mass\n",
      "communication. Efforts to combat this issue have risen in the form of\n",
      "competitions and funding for research to detect deepfakes. This paper presents\n",
      "three techniques and algorithms: convolutional LSTM, eye blink detection and\n",
      "grayscale histograms-pursued while participating in the Deepfake Detection\n",
      "Challenge. We assessed the current knowledge about deepfake videos, a more\n",
      "severe version of manipulated media, and previous methods used, and found\n",
      "relevance in the grayscale histogram technique over others. We discussed the\n",
      "implications of each method developed and provided further steps to improve the\n",
      "given findings.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Recent advances in deepfake generating algorithms that produce manipulated\\nmedia have had dangerous implications in privacy, security and mass\\ncommunication. Efforts to combat this issue have risen in the form of\\ncompetitions and funding for research to detect deepfakes. This paper presents\\nthree techniques and algorithms: convolutional LSTM, eye blink detection and\\ngrayscale histograms-pursued while participating in the Deepfake Detection\\nChallenge. We assessed the current knowledge about deepfake videos, a more\\nsevere version of manipulated media, and previous methods used, and found\\nrelevance in the grayscale histogram technique over others. We discussed the\\nimplications of each method developed and provided further steps to improve the\\ngiven findings.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Armaan Pishori'}, {'name': 'Brittany Rollins'}, {'name': 'Nicolas van Houten'}, {'name': 'Nisha Chatwani'}, {'name': 'Omar Uraimov'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Omar Uraimov'}\n",
      "\n",
      "\n",
      "author\n",
      "Omar Uraimov\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "11 pages, 8 figures, 2 tables\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2007.08517v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2007.08517v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.15473v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.15473v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-01-15T02:13:45Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=1, tm_mday=15, tm_hour=2, tm_min=13, tm_sec=45, tm_wday=4, tm_yday=15, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-28T00:25:34Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=28, tm_hour=0, tm_min=25, tm_sec=34, tm_wday=6, tm_yday=180, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Interpretable and Trustworthy Deepfake Detection via Dynamic Prototypes\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Interpretable and Trustworthy Deepfake Detection via Dynamic Prototypes'}\n",
      "\n",
      "\n",
      "summary\n",
      "In this paper we propose a novel human-centered approach for detecting\n",
      "forgery in face images, using dynamic prototypes as a form of visual\n",
      "explanations. Currently, most state-of-the-art deepfake detections are based on\n",
      "black-box models that process videos frame-by-frame for inference, and few\n",
      "closely examine their temporal inconsistencies. However, the existence of such\n",
      "temporal artifacts within deepfake videos is key in detecting and explaining\n",
      "deepfakes to a supervising human. To this end, we propose Dynamic Prototype\n",
      "Network (DPNet) -- an interpretable and effective solution that utilizes\n",
      "dynamic representations (i.e., prototypes) to explain deepfake temporal\n",
      "artifacts. Extensive experimental results show that DPNet achieves competitive\n",
      "predictive performance, even on unseen testing datasets such as Google's\n",
      "DeepFakeDetection, DeeperForensics, and Celeb-DF, while providing easy\n",
      "referential explanations of deepfake dynamics. On top of DPNet's prototypical\n",
      "framework, we further formulate temporal logic specifications based on these\n",
      "dynamics to check our model's compliance to desired temporal behaviors, hence\n",
      "providing trustworthiness for such critical detection systems.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"In this paper we propose a novel human-centered approach for detecting\\nforgery in face images, using dynamic prototypes as a form of visual\\nexplanations. Currently, most state-of-the-art deepfake detections are based on\\nblack-box models that process videos frame-by-frame for inference, and few\\nclosely examine their temporal inconsistencies. However, the existence of such\\ntemporal artifacts within deepfake videos is key in detecting and explaining\\ndeepfakes to a supervising human. To this end, we propose Dynamic Prototype\\nNetwork (DPNet) -- an interpretable and effective solution that utilizes\\ndynamic representations (i.e., prototypes) to explain deepfake temporal\\nartifacts. Extensive experimental results show that DPNet achieves competitive\\npredictive performance, even on unseen testing datasets such as Google's\\nDeepFakeDetection, DeeperForensics, and Celeb-DF, while providing easy\\nreferential explanations of deepfake dynamics. On top of DPNet's prototypical\\nframework, we further formulate temporal logic specifications based on these\\ndynamics to check our model's compliance to desired temporal behaviors, hence\\nproviding trustworthiness for such critical detection systems.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Loc Trinh'}, {'name': 'Michael Tsang'}, {'name': 'Sirisha Rambhatla'}, {'name': 'Yan Liu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yan Liu'}\n",
      "\n",
      "\n",
      "author\n",
      "Yan Liu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To appear in the 2021 IEEE Winter Conference on Applications of\n",
      "  Computer Vision (WACV 21')\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.15473v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.15473v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.14749v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.14749v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-06-26T01:32:31Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=26, tm_hour=1, tm_min=32, tm_sec=31, tm_wday=4, tm_yday=178, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-26T01:32:31Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=26, tm_hour=1, tm_min=32, tm_sec=31, tm_wday=4, tm_yday=178, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Detection using Spatiotemporal Convolutional Networks\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake Detection using Spatiotemporal Convolutional Networks'}\n",
      "\n",
      "\n",
      "summary\n",
      "Better generative models and larger datasets have led to more realistic fake\n",
      "videos that can fool the human eye but produce temporal and spatial artifacts\n",
      "that deep learning approaches can detect. Most current Deepfake detection\n",
      "methods only use individual video frames and therefore fail to learn from\n",
      "temporal information. We created a benchmark of the performance of\n",
      "spatiotemporal convolutional methods using the Celeb-DF dataset. Our methods\n",
      "outperformed state-of-the-art frame-based detection methods. Code for our paper\n",
      "is publicly available at https://github.com/oidelima/Deepfake-Detection.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Better generative models and larger datasets have led to more realistic fake\\nvideos that can fool the human eye but produce temporal and spatial artifacts\\nthat deep learning approaches can detect. Most current Deepfake detection\\nmethods only use individual video frames and therefore fail to learn from\\ntemporal information. We created a benchmark of the performance of\\nspatiotemporal convolutional methods using the Celeb-DF dataset. Our methods\\noutperformed state-of-the-art frame-based detection methods. Code for our paper\\nis publicly available at https://github.com/oidelima/Deepfake-Detection.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Oscar de Lima'}, {'name': 'Sean Franklin'}, {'name': 'Shreshtha Basu'}, {'name': 'Blake Karwoski'}, {'name': 'Annet George'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Annet George'}\n",
      "\n",
      "\n",
      "author\n",
      "Annet George\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.14749v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.14749v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.12247v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.12247v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-11-25T11:47:30Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=25, tm_hour=11, tm_min=47, tm_sec=30, tm_wday=2, tm_yday=330, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-17T17:18:29Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=17, tm_hour=17, tm_min=18, tm_sec=29, tm_wday=2, tm_yday=169, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "OGAN: Disrupting Deepfakes with an Adversarial Attack that Survives\n",
      "  Training\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'OGAN: Disrupting Deepfakes with an Adversarial Attack that Survives\\n  Training'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent advances in autoencoders and generative models have given rise to\n",
      "effective video forgery methods, used for generating so-called \"deepfakes\".\n",
      "Mitigation research is mostly focused on post-factum deepfake detection and not\n",
      "on prevention. We complement these efforts by introducing a novel class of\n",
      "adversarial attacks---training-resistant attacks---which can disrupt\n",
      "face-swapping autoencoders whether or not its adversarial images have been\n",
      "included in the training set of said autoencoders. We propose the Oscillating\n",
      "GAN (OGAN) attack, a novel attack optimized to be training-resistant, which\n",
      "introduces spatial-temporal distortions to the output of face-swapping\n",
      "autoencoders. To implement OGAN, we construct a bilevel optimization problem,\n",
      "where we train a generator and a face-swapping model instance against each\n",
      "other. Specifically, we pair each input image with a target distortion, and\n",
      "feed them into a generator that produces an adversarial image. This image will\n",
      "exhibit the distortion when a face-swapping autoencoder is applied to it. We\n",
      "solve the optimization problem by training the generator and the face-swapping\n",
      "model simultaneously using an iterative process of alternating optimization.\n",
      "Next, we analyze the previously published Distorting Attack and show it is\n",
      "training-resistant, though it is outperformed by our suggested OGAN. Finally,\n",
      "we validate both attacks using a popular implementation of FaceSwap, and show\n",
      "that they transfer across different target models and target faces, including\n",
      "faces the adversarial attacks were not trained on. More broadly, these results\n",
      "demonstrate the existence of training-resistant adversarial attacks,\n",
      "potentially applicable to a wide range of domains.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Recent advances in autoencoders and generative models have given rise to\\neffective video forgery methods, used for generating so-called \"deepfakes\".\\nMitigation research is mostly focused on post-factum deepfake detection and not\\non prevention. We complement these efforts by introducing a novel class of\\nadversarial attacks---training-resistant attacks---which can disrupt\\nface-swapping autoencoders whether or not its adversarial images have been\\nincluded in the training set of said autoencoders. We propose the Oscillating\\nGAN (OGAN) attack, a novel attack optimized to be training-resistant, which\\nintroduces spatial-temporal distortions to the output of face-swapping\\nautoencoders. To implement OGAN, we construct a bilevel optimization problem,\\nwhere we train a generator and a face-swapping model instance against each\\nother. Specifically, we pair each input image with a target distortion, and\\nfeed them into a generator that produces an adversarial image. This image will\\nexhibit the distortion when a face-swapping autoencoder is applied to it. We\\nsolve the optimization problem by training the generator and the face-swapping\\nmodel simultaneously using an iterative process of alternating optimization.\\nNext, we analyze the previously published Distorting Attack and show it is\\ntraining-resistant, though it is outperformed by our suggested OGAN. Finally,\\nwe validate both attacks using a popular implementation of FaceSwap, and show\\nthat they transfer across different target models and target faces, including\\nfaces the adversarial attacks were not trained on. More broadly, these results\\ndemonstrate the existence of training-resistant adversarial attacks,\\npotentially applicable to a wide range of domains.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Eran Segalis'}, {'name': 'Eran Galili'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Eran Galili'}\n",
      "\n",
      "\n",
      "author\n",
      "Eran Galili\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "10 pages\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.12247v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.12247v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.07634v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.07634v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-26T06:45:10Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=26, tm_hour=6, tm_min=45, tm_sec=10, tm_wday=2, tm_yday=239, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-13T12:56:46Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=13, tm_hour=12, tm_min=56, tm_sec=46, tm_wday=5, tm_yday=165, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms'}\n",
      "\n",
      "\n",
      "summary\n",
      "As the GAN-based face image and video generation techniques, widely known as\n",
      "DeepFakes, have become more and more matured and realistic, there comes a\n",
      "pressing and urgent demand for effective DeepFakes detectors. Motivated by the\n",
      "fact that remote visual photoplethysmography (PPG) is made possible by\n",
      "monitoring the minuscule periodic changes of skin color due to blood pumping\n",
      "through the face, we conjecture that normal heartbeat rhythms found in the real\n",
      "face videos will be disrupted or even entirely broken in a DeepFake video,\n",
      "making it a potentially powerful indicator for DeepFake detection. In this\n",
      "work, we propose DeepRhythm, a DeepFake detection technique that exposes\n",
      "DeepFakes by monitoring the heartbeat rhythms. DeepRhythm utilizes\n",
      "dual-spatial-temporal attention to adapt to dynamically changing face and fake\n",
      "types. Extensive experiments on FaceForensics++ and DFDC-preview datasets have\n",
      "confirmed our conjecture and demonstrated not only the effectiveness, but also\n",
      "the generalization capability of \\emph{DeepRhythm} over different datasets by\n",
      "various DeepFakes generation techniques and multifarious challenging\n",
      "degradations.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'As the GAN-based face image and video generation techniques, widely known as\\nDeepFakes, have become more and more matured and realistic, there comes a\\npressing and urgent demand for effective DeepFakes detectors. Motivated by the\\nfact that remote visual photoplethysmography (PPG) is made possible by\\nmonitoring the minuscule periodic changes of skin color due to blood pumping\\nthrough the face, we conjecture that normal heartbeat rhythms found in the real\\nface videos will be disrupted or even entirely broken in a DeepFake video,\\nmaking it a potentially powerful indicator for DeepFake detection. In this\\nwork, we propose DeepRhythm, a DeepFake detection technique that exposes\\nDeepFakes by monitoring the heartbeat rhythms. DeepRhythm utilizes\\ndual-spatial-temporal attention to adapt to dynamically changing face and fake\\ntypes. Extensive experiments on FaceForensics++ and DFDC-preview datasets have\\nconfirmed our conjecture and demonstrated not only the effectiveness, but also\\nthe generalization capability of \\\\emph{DeepRhythm} over different datasets by\\nvarious DeepFakes generation techniques and multifarious challenging\\ndegradations.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hua Qi'}, {'name': 'Qing Guo'}, {'name': 'Felix Juefei-Xu'}, {'name': 'Xiaofei Xie'}, {'name': 'Lei Ma'}, {'name': 'Wei Feng'}, {'name': 'Yang Liu'}, {'name': 'Jianjun Zhao'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Jianjun Zhao'}\n",
      "\n",
      "\n",
      "author\n",
      "Jianjun Zhao\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "11 pages, 7 figures; This paper has been accepted to ACM-MM 2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.07634v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.07634v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.07533v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.07533v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-17T07:27:28Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=17, tm_hour=7, tm_min=27, tm_sec=28, tm_wday=0, tm_yday=230, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-13T01:48:15Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=13, tm_hour=1, tm_min=48, tm_sec=15, tm_wday=5, tm_yday=165, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FakePolisher: Making DeepFakes More Detection-Evasive by Shallow\n",
      "  Reconstruction\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'FakePolisher: Making DeepFakes More Detection-Evasive by Shallow\\n  Reconstruction'}\n",
      "\n",
      "\n",
      "summary\n",
      "At this moment, GAN-based image generation methods are still imperfect, whose\n",
      "upsampling design has limitations in leaving some certain artifact patterns in\n",
      "the synthesized image. Such artifact patterns can be easily exploited (by\n",
      "recent methods) for difference detection of real and GAN-synthesized images.\n",
      "However, the existing detection methods put much emphasis on the artifact\n",
      "patterns, which can become futile if such artifact patterns were reduced.\n",
      "Towards reducing the artifacts in the synthesized images, in this paper, we\n",
      "devise a simple yet powerful approach termed FakePolisher that performs shallow\n",
      "reconstruction of fake images through a learned linear dictionary, intending to\n",
      "effectively and efficiently reduce the artifacts introduced during image\n",
      "synthesis. The comprehensive evaluation on 3 state-of-the-art DeepFake\n",
      "detection methods and fake images generated by 16 popular GAN-based fake image\n",
      "generation techniques, demonstrates the effectiveness of our technique.Overall,\n",
      "through reducing artifact patterns, our technique significantly reduces the\n",
      "accuracy of the 3 state-of-the-art fake image detection methods, i.e., 47% on\n",
      "average and up to 93% in the worst case.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'At this moment, GAN-based image generation methods are still imperfect, whose\\nupsampling design has limitations in leaving some certain artifact patterns in\\nthe synthesized image. Such artifact patterns can be easily exploited (by\\nrecent methods) for difference detection of real and GAN-synthesized images.\\nHowever, the existing detection methods put much emphasis on the artifact\\npatterns, which can become futile if such artifact patterns were reduced.\\nTowards reducing the artifacts in the synthesized images, in this paper, we\\ndevise a simple yet powerful approach termed FakePolisher that performs shallow\\nreconstruction of fake images through a learned linear dictionary, intending to\\neffectively and efficiently reduce the artifacts introduced during image\\nsynthesis. The comprehensive evaluation on 3 state-of-the-art DeepFake\\ndetection methods and fake images generated by 16 popular GAN-based fake image\\ngeneration techniques, demonstrates the effectiveness of our technique.Overall,\\nthrough reducing artifact patterns, our technique significantly reduces the\\naccuracy of the 3 state-of-the-art fake image detection methods, i.e., 47% on\\naverage and up to 93% in the worst case.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yihao Huang'}, {'name': 'Felix Juefei-Xu'}, {'name': 'Run Wang'}, {'name': 'Qing Guo'}, {'name': 'Lei Ma'}, {'name': 'Xiaofei Xie'}, {'name': 'Jianwen Li'}, {'name': 'Weikai Miao'}, {'name': 'Yang Liu'}, {'name': 'Geguang Pu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Geguang Pu'}\n",
      "\n",
      "\n",
      "author\n",
      "Geguang Pu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "9 pages, accepted by ACM MM 2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.07533v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.07533v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.07421v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.07421v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-06-12T18:51:57Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=12, tm_hour=18, tm_min=51, tm_sec=57, tm_wday=4, tm_yday=164, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-12T18:51:57Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=12, tm_hour=18, tm_min=51, tm_sec=57, tm_wday=4, tm_yday=164, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Defending against GAN-based Deepfake Attacks via Transformation-aware\n",
      "  Adversarial Faces\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Defending against GAN-based Deepfake Attacks via Transformation-aware\\n  Adversarial Faces'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake represents a category of face-swapping attacks that leverage machine\n",
      "learning models such as autoencoders or generative adversarial networks.\n",
      "Although the concept of the face-swapping is not new, its recent technical\n",
      "advances make fake content (e.g., images, videos) more realistic and\n",
      "imperceptible to Humans. Various detection techniques for Deepfake attacks have\n",
      "been explored. These methods, however, are passive measures against Deepfakes\n",
      "as they are mitigation strategies after the high-quality fake content is\n",
      "generated. More importantly, we would like to think ahead of the attackers with\n",
      "robust defenses. This work aims to take an offensive measure to impede the\n",
      "generation of high-quality fake images or videos. Specifically, we propose to\n",
      "use novel transformation-aware adversarially perturbed faces as a defense\n",
      "against GAN-based Deepfake attacks. Different from the naive adversarial faces,\n",
      "our proposed approach leverages differentiable random image transformations\n",
      "during the generation. We also propose to use an ensemble-based approach to\n",
      "enhance the defense robustness against GAN-based Deepfake variants under the\n",
      "black-box setting. We show that training a Deepfake model with adversarial\n",
      "faces can lead to a significant degradation in the quality of synthesized\n",
      "faces. This degradation is twofold. On the one hand, the quality of the\n",
      "synthesized faces is reduced with more visual artifacts such that the\n",
      "synthesized faces are more obviously fake or less convincing to human\n",
      "observers. On the other hand, the synthesized faces can easily be detected\n",
      "based on various metrics.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake represents a category of face-swapping attacks that leverage machine\\nlearning models such as autoencoders or generative adversarial networks.\\nAlthough the concept of the face-swapping is not new, its recent technical\\nadvances make fake content (e.g., images, videos) more realistic and\\nimperceptible to Humans. Various detection techniques for Deepfake attacks have\\nbeen explored. These methods, however, are passive measures against Deepfakes\\nas they are mitigation strategies after the high-quality fake content is\\ngenerated. More importantly, we would like to think ahead of the attackers with\\nrobust defenses. This work aims to take an offensive measure to impede the\\ngeneration of high-quality fake images or videos. Specifically, we propose to\\nuse novel transformation-aware adversarially perturbed faces as a defense\\nagainst GAN-based Deepfake attacks. Different from the naive adversarial faces,\\nour proposed approach leverages differentiable random image transformations\\nduring the generation. We also propose to use an ensemble-based approach to\\nenhance the defense robustness against GAN-based Deepfake variants under the\\nblack-box setting. We show that training a Deepfake model with adversarial\\nfaces can lead to a significant degradation in the quality of synthesized\\nfaces. This degradation is twofold. On the one hand, the quality of the\\nsynthesized faces is reduced with more visual artifacts such that the\\nsynthesized faces are more obviously fake or less convincing to human\\nobservers. On the other hand, the synthesized faces can easily be detected\\nbased on various metrics.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Chaofei Yang'}, {'name': 'Lei Ding'}, {'name': 'Yiran Chen'}, {'name': 'Hai Li'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Hai Li'}\n",
      "\n",
      "\n",
      "author\n",
      "Hai Li\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.07421v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.07421v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.07397v4\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.07397v4\n",
      "\n",
      "\n",
      "updated\n",
      "2020-10-28T03:48:28Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=10, tm_mday=28, tm_hour=3, tm_min=48, tm_sec=28, tm_wday=2, tm_yday=302, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-12T18:15:55Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=12, tm_hour=18, tm_min=15, tm_sec=55, tm_wday=4, tm_yday=164, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "The DeepFake Detection Challenge (DFDC) Dataset\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The DeepFake Detection Challenge (DFDC) Dataset'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes are a recent off-the-shelf manipulation technique that allows\n",
      "anyone to swap two identities in a single video. In addition to Deepfakes, a\n",
      "variety of GAN-based face swapping methods have also been published with\n",
      "accompanying code. To counter this emerging threat, we have constructed an\n",
      "extremely large face swap video dataset to enable the training of detection\n",
      "models, and organized the accompanying DeepFake Detection Challenge (DFDC)\n",
      "Kaggle competition. Importantly, all recorded subjects agreed to participate in\n",
      "and have their likenesses modified during the construction of the face-swapped\n",
      "dataset. The DFDC dataset is by far the largest currently and publicly\n",
      "available face swap video dataset, with over 100,000 total clips sourced from\n",
      "3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned\n",
      "methods. In addition to describing the methods used to construct the dataset,\n",
      "we provide a detailed analysis of the top submissions from the Kaggle contest.\n",
      "We show although Deepfake detection is extremely difficult and still an\n",
      "unsolved problem, a Deepfake detection model trained only on the DFDC can\n",
      "generalize to real \"in-the-wild\" Deepfake videos, and such a model can be a\n",
      "valuable analysis tool when analyzing potentially Deepfaked videos. Training,\n",
      "validation and testing corpuses can be downloaded from\n",
      "https://ai.facebook.com/datasets/dfdc.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfakes are a recent off-the-shelf manipulation technique that allows\\nanyone to swap two identities in a single video. In addition to Deepfakes, a\\nvariety of GAN-based face swapping methods have also been published with\\naccompanying code. To counter this emerging threat, we have constructed an\\nextremely large face swap video dataset to enable the training of detection\\nmodels, and organized the accompanying DeepFake Detection Challenge (DFDC)\\nKaggle competition. Importantly, all recorded subjects agreed to participate in\\nand have their likenesses modified during the construction of the face-swapped\\ndataset. The DFDC dataset is by far the largest currently and publicly\\navailable face swap video dataset, with over 100,000 total clips sourced from\\n3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned\\nmethods. In addition to describing the methods used to construct the dataset,\\nwe provide a detailed analysis of the top submissions from the Kaggle contest.\\nWe show although Deepfake detection is extremely difficult and still an\\nunsolved problem, a Deepfake detection model trained only on the DFDC can\\ngeneralize to real \"in-the-wild\" Deepfake videos, and such a model can be a\\nvaluable analysis tool when analyzing potentially Deepfaked videos. Training,\\nvalidation and testing corpuses can be downloaded from\\nhttps://ai.facebook.com/datasets/dfdc.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Brian Dolhansky'}, {'name': 'Joanna Bitton'}, {'name': 'Ben Pflaum'}, {'name': 'Jikuo Lu'}, {'name': 'Russ Howes'}, {'name': 'Menglin Wang'}, {'name': 'Cristian Canton Ferrer'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Cristian Canton Ferrer'}\n",
      "\n",
      "\n",
      "author\n",
      "Cristian Canton Ferrer\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.07397v4', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.07397v4', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.07084v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.07084v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-10-19T10:22:15Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=10, tm_mday=19, tm_hour=10, tm_min=22, tm_sec=15, tm_wday=0, tm_yday=293, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-12T11:16:02Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=12, tm_hour=11, tm_min=16, tm_sec=2, tm_wday=4, tm_yday=164, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Investigating the Impact of Pre-processing and Prediction Aggregation on\n",
      "  the DeepFake Detection Task\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Investigating the Impact of Pre-processing and Prediction Aggregation on\\n  the DeepFake Detection Task'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent advances in content generation technologies (widely known as\n",
      "DeepFakes) along with the online proliferation of manipulated media content\n",
      "render the detection of such manipulations a task of increasing importance.\n",
      "Even though there are many DeepFake detection methods, only a few focus on the\n",
      "impact of dataset preprocessing and the aggregation of frame-level to\n",
      "video-level prediction on model performance. In this paper, we propose a\n",
      "pre-processing step to improve the training data quality and examine its effect\n",
      "on the performance of DeepFake detection. We also propose and evaluate the\n",
      "effect of video-level prediction aggregation approaches. Experimental results\n",
      "show that the proposed pre-processing approach leads to considerable\n",
      "improvements in the performance of detection models, and the proposed\n",
      "prediction aggregation scheme further boosts the detection efficiency in cases\n",
      "where there are multiple faces in a video.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Recent advances in content generation technologies (widely known as\\nDeepFakes) along with the online proliferation of manipulated media content\\nrender the detection of such manipulations a task of increasing importance.\\nEven though there are many DeepFake detection methods, only a few focus on the\\nimpact of dataset preprocessing and the aggregation of frame-level to\\nvideo-level prediction on model performance. In this paper, we propose a\\npre-processing step to improve the training data quality and examine its effect\\non the performance of DeepFake detection. We also propose and evaluate the\\neffect of video-level prediction aggregation approaches. Experimental results\\nshow that the proposed pre-processing approach leads to considerable\\nimprovements in the performance of detection models, and the proposed\\nprediction aggregation scheme further boosts the detection efficiency in cases\\nwhere there are multiple faces in a video.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Polychronis Charitidis'}, {'name': 'Giorgos Kordopatis-Zilos'}, {'name': 'Symeon Papadopoulos'}, {'name': 'Ioannis Kompatsiaris'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ioannis Kompatsiaris'}\n",
      "\n",
      "\n",
      "author\n",
      "Ioannis Kompatsiaris\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.07084v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.07084v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.06961v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.06961v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-06-18T22:02:13Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=18, tm_hour=22, tm_min=2, tm_sec=13, tm_wday=3, tm_yday=170, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-12T06:14:49Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=12, tm_hour=6, tm_min=14, tm_sec=49, tm_wday=4, tm_yday=164, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "The eyes know it: FakeET -- An Eye-tracking Database to Understand\n",
      "  Deepfake Perception\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The eyes know it: FakeET -- An Eye-tracking Database to Understand\\n  Deepfake Perception'}\n",
      "\n",
      "\n",
      "summary\n",
      "We present \\textbf{FakeET}-- an eye-tracking database to understand human\n",
      "visual perception of \\emph{deepfake} videos. Given that the principal purpose\n",
      "of deepfakes is to deceive human observers, FakeET is designed to understand\n",
      "and evaluate the ease with which viewers can detect synthetic video artifacts.\n",
      "FakeET contains viewing patterns compiled from 40 users via the \\emph{Tobii}\n",
      "desktop eye-tracker for 811 videos from the \\textit{Google Deepfake} dataset,\n",
      "with a minimum of two viewings per video. Additionally, EEG responses acquired\n",
      "via the \\emph{Emotiv} sensor are also available. The compiled data confirms (a)\n",
      "distinct eye movement characteristics for \\emph{real} vs \\emph{fake} videos;\n",
      "(b) utility of the eye-track saliency maps for spatial forgery localization and\n",
      "detection, and (c) Error Related Negativity (ERN) triggers in the EEG\n",
      "responses, and the ability of the \\emph{raw} EEG signal to distinguish between\n",
      "\\emph{real} and \\emph{fake} videos.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'We present \\\\textbf{FakeET}-- an eye-tracking database to understand human\\nvisual perception of \\\\emph{deepfake} videos. Given that the principal purpose\\nof deepfakes is to deceive human observers, FakeET is designed to understand\\nand evaluate the ease with which viewers can detect synthetic video artifacts.\\nFakeET contains viewing patterns compiled from 40 users via the \\\\emph{Tobii}\\ndesktop eye-tracker for 811 videos from the \\\\textit{Google Deepfake} dataset,\\nwith a minimum of two viewings per video. Additionally, EEG responses acquired\\nvia the \\\\emph{Emotiv} sensor are also available. The compiled data confirms (a)\\ndistinct eye movement characteristics for \\\\emph{real} vs \\\\emph{fake} videos;\\n(b) utility of the eye-track saliency maps for spatial forgery localization and\\ndetection, and (c) Error Related Negativity (ERN) triggers in the EEG\\nresponses, and the ability of the \\\\emph{raw} EEG signal to distinguish between\\n\\\\emph{real} and \\\\emph{fake} videos.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Parul Gupta'}, {'name': 'Komal Chugh'}, {'name': 'Abhinav Dhall'}, {'name': 'Ramanathan Subramanian'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ramanathan Subramanian'}\n",
      "\n",
      "\n",
      "author\n",
      "Ramanathan Subramanian\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "8 pages\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.06961v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.06961v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.06493v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.06493v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-06-11T15:02:27Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=11, tm_hour=15, tm_min=2, tm_sec=27, tm_wday=3, tm_yday=163, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-11T15:02:27Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=11, tm_hour=15, tm_min=2, tm_sec=27, tm_wday=3, tm_yday=163, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Protecting Against Image Translation Deepfakes by Leaking Universal\n",
      "  Perturbations from Black-Box Neural Networks\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Protecting Against Image Translation Deepfakes by Leaking Universal\\n  Perturbations from Black-Box Neural Networks'}\n",
      "\n",
      "\n",
      "summary\n",
      "In this work, we develop efficient disruptions of black-box image translation\n",
      "deepfake generation systems. We are the first to demonstrate black-box deepfake\n",
      "generation disruption by presenting image translation formulations of attacks\n",
      "initially proposed for classification models. Nevertheless, a naive adaptation\n",
      "of classification black-box attacks results in a prohibitive number of queries\n",
      "for image translation systems in the real-world. We present a frustratingly\n",
      "simple yet highly effective algorithm Leaking Universal Perturbations (LUP),\n",
      "that significantly reduces the number of queries needed to attack an image. LUP\n",
      "consists of two phases: (1) a short leaking phase where we attack the network\n",
      "using traditional black-box attacks and gather information on successful\n",
      "attacks on a small dataset and (2) and an exploitation phase where we leverage\n",
      "said information to subsequently attack the network with improved efficiency.\n",
      "Our attack reduces the total number of queries necessary to attack GANimation\n",
      "and StarGAN by 30%.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In this work, we develop efficient disruptions of black-box image translation\\ndeepfake generation systems. We are the first to demonstrate black-box deepfake\\ngeneration disruption by presenting image translation formulations of attacks\\ninitially proposed for classification models. Nevertheless, a naive adaptation\\nof classification black-box attacks results in a prohibitive number of queries\\nfor image translation systems in the real-world. We present a frustratingly\\nsimple yet highly effective algorithm Leaking Universal Perturbations (LUP),\\nthat significantly reduces the number of queries needed to attack an image. LUP\\nconsists of two phases: (1) a short leaking phase where we attack the network\\nusing traditional black-box attacks and gather information on successful\\nattacks on a small dataset and (2) and an exploitation phase where we leverage\\nsaid information to subsequently attack the network with improved efficiency.\\nOur attack reduces the total number of queries necessary to attack GANimation\\nand StarGAN by 30%.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Nataniel Ruiz'}, {'name': 'Sarah Adel Bargal'}, {'name': 'Stan Sclaroff'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Stan Sclaroff'}\n",
      "\n",
      "\n",
      "author\n",
      "Stan Sclaroff\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.06493v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.06493v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2006.05183v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2006.05183v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-06-09T11:07:08Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=9, tm_hour=11, tm_min=7, tm_sec=8, tm_wday=1, tm_yday=161, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-06-09T11:07:08Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=9, tm_hour=11, tm_min=7, tm_sec=8, tm_wday=1, tm_yday=161, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "A Note on Deepfake Detection with Low-Resources\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'A Note on Deepfake Detection with Low-Resources'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfakes are videos that include changes, quite often substituting face of a\n",
      "portrayed individual with a different face using neural networks. Even though\n",
      "the technology gained its popularity as a carrier of jokes and parodies it\n",
      "raises a serious threat to ones security - via biometric impersonation or\n",
      "besmearing. In this paper we present two methods that allow detecting Deepfakes\n",
      "for a user without significant computational power. In particular, we enhance\n",
      "MesoNet by replacing the original activation functions allowing a nearly 1%\n",
      "improvement as well as increasing the consistency of the results. Moreover, we\n",
      "introduced and verified a new activation function - Pish that at the cost of\n",
      "slight time overhead allows even higher consistency.\n",
      "  Additionally, we present a preliminary results of Deepfake detection method\n",
      "based on Local Feature Descriptors (LFD), that allows setting up the system\n",
      "even faster and without resorting to GPU computation. Our method achieved Equal\n",
      "Error Rate of 0.28, with both accuracy and recall exceeding 0.7.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfakes are videos that include changes, quite often substituting face of a\\nportrayed individual with a different face using neural networks. Even though\\nthe technology gained its popularity as a carrier of jokes and parodies it\\nraises a serious threat to ones security - via biometric impersonation or\\nbesmearing. In this paper we present two methods that allow detecting Deepfakes\\nfor a user without significant computational power. In particular, we enhance\\nMesoNet by replacing the original activation functions allowing a nearly 1%\\nimprovement as well as increasing the consistency of the results. Moreover, we\\nintroduced and verified a new activation function - Pish that at the cost of\\nslight time overhead allows even higher consistency.\\n  Additionally, we present a preliminary results of Deepfake detection method\\nbased on Local Feature Descriptors (LFD), that allows setting up the system\\neven faster and without resorting to GPU computation. Our method achieved Equal\\nError Rate of 0.28, with both accuracy and recall exceeding 0.7.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Piotr Kawa'}, {'name': 'Piotr Syga'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Piotr Syga'}\n",
      "\n",
      "\n",
      "author\n",
      "Piotr Syga\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2006.05183v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2006.05183v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2005.14405v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2005.14405v3\n",
      "\n",
      "\n",
      "updated\n",
      "2021-03-20T15:09:49Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=3, tm_mday=20, tm_hour=15, tm_min=9, tm_sec=49, tm_wday=5, tm_yday=79, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-05-29T06:09:33Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=5, tm_mday=29, tm_hour=6, tm_min=9, tm_sec=33, tm_wday=4, tm_yday=150, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Not made for each other- Audio-Visual Dissonance-based Deepfake\n",
      "  Detection and Localization\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Not made for each other- Audio-Visual Dissonance-based Deepfake\\n  Detection and Localization'}\n",
      "\n",
      "\n",
      "summary\n",
      "We propose detection of deepfake videos based on the dissimilarity between\n",
      "the audio and visual modalities, termed as the Modality Dissonance Score (MDS).\n",
      "We hypothesize that manipulation of either modality will lead to dis-harmony\n",
      "between the two modalities, eg, loss of lip-sync, unnatural facial and lip\n",
      "movements, etc. MDS is computed as an aggregate of dissimilarity scores between\n",
      "audio and visual segments in a video. Discriminative features are learnt for\n",
      "the audio and visual channels in a chunk-wise manner, employing the\n",
      "cross-entropy loss for individual modalities, and a contrastive loss that\n",
      "models inter-modality similarity. Extensive experiments on the DFDC and\n",
      "DeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art\n",
      "by up to 7%. We also demonstrate temporal forgery localization, and show how\n",
      "our technique identifies the manipulated video segments.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'We propose detection of deepfake videos based on the dissimilarity between\\nthe audio and visual modalities, termed as the Modality Dissonance Score (MDS).\\nWe hypothesize that manipulation of either modality will lead to dis-harmony\\nbetween the two modalities, eg, loss of lip-sync, unnatural facial and lip\\nmovements, etc. MDS is computed as an aggregate of dissimilarity scores between\\naudio and visual segments in a video. Discriminative features are learnt for\\nthe audio and visual channels in a chunk-wise manner, employing the\\ncross-entropy loss for individual modalities, and a contrastive loss that\\nmodels inter-modality similarity. Extensive experiments on the DFDC and\\nDeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art\\nby up to 7%. We also demonstrate temporal forgery localization, and show how\\nour technique identifies the manipulated video segments.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Komal Chugh'}, {'name': 'Parul Gupta'}, {'name': 'Abhinav Dhall'}, {'name': 'Ramanathan Subramanian'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ramanathan Subramanian'}\n",
      "\n",
      "\n",
      "author\n",
      "Ramanathan Subramanian\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2005.14405v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2005.14405v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2005.13770v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2005.13770v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-15T13:37:57Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=15, tm_hour=13, tm_min=37, tm_sec=57, tm_wday=5, tm_yday=228, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-05-28T04:02:52Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=5, tm_mday=28, tm_hour=4, tm_min=2, tm_sec=52, tm_wday=3, tm_yday=149, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake\n",
      "  Voices\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake\\n  Voices'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the recent advances in voice synthesis, AI-synthesized fake voices are\n",
      "indistinguishable to human ears and widely are applied to produce realistic and\n",
      "natural DeepFakes, exhibiting real threats to our society. However, effective\n",
      "and robust detectors for synthesized fake voices are still in their infancy and\n",
      "are not ready to fully tackle this emerging threat. In this paper, we devise a\n",
      "novel approach, named \\emph{DeepSonar}, based on monitoring neuron behaviors of\n",
      "speaker recognition (SR) system, \\ie, a deep neural network (DNN), to discern\n",
      "AI-synthesized fake voices. Layer-wise neuron behaviors provide an important\n",
      "insight to meticulously catch the differences among inputs, which are widely\n",
      "employed for building safety, robust, and interpretable DNNs. In this work, we\n",
      "leverage the power of layer-wise neuron activation patterns with a conjecture\n",
      "that they can capture the subtle differences between real and AI-synthesized\n",
      "fake voices, in providing a cleaner signal to classifiers than raw inputs.\n",
      "Experiments are conducted on three datasets (including commercial products from\n",
      "Google, Baidu, \\etc) containing both English and Chinese languages to\n",
      "corroborate the high detection rates (98.1\\% average accuracy) and low false\n",
      "alarm rates (about 2\\% error rate) of DeepSonar in discerning fake voices.\n",
      "Furthermore, extensive experimental results also demonstrate its robustness\n",
      "against manipulation attacks (\\eg, voice conversion and additive real-world\n",
      "noises). Our work further poses a new insight into adopting neuron behaviors\n",
      "for effective and robust AI aided multimedia fakes forensics as an inside-out\n",
      "approach instead of being motivated and swayed by various artifacts introduced\n",
      "in synthesizing fakes.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'With the recent advances in voice synthesis, AI-synthesized fake voices are\\nindistinguishable to human ears and widely are applied to produce realistic and\\nnatural DeepFakes, exhibiting real threats to our society. However, effective\\nand robust detectors for synthesized fake voices are still in their infancy and\\nare not ready to fully tackle this emerging threat. In this paper, we devise a\\nnovel approach, named \\\\emph{DeepSonar}, based on monitoring neuron behaviors of\\nspeaker recognition (SR) system, \\\\ie, a deep neural network (DNN), to discern\\nAI-synthesized fake voices. Layer-wise neuron behaviors provide an important\\ninsight to meticulously catch the differences among inputs, which are widely\\nemployed for building safety, robust, and interpretable DNNs. In this work, we\\nleverage the power of layer-wise neuron activation patterns with a conjecture\\nthat they can capture the subtle differences between real and AI-synthesized\\nfake voices, in providing a cleaner signal to classifiers than raw inputs.\\nExperiments are conducted on three datasets (including commercial products from\\nGoogle, Baidu, \\\\etc) containing both English and Chinese languages to\\ncorroborate the high detection rates (98.1\\\\% average accuracy) and low false\\nalarm rates (about 2\\\\% error rate) of DeepSonar in discerning fake voices.\\nFurthermore, extensive experimental results also demonstrate its robustness\\nagainst manipulation attacks (\\\\eg, voice conversion and additive real-world\\nnoises). Our work further poses a new insight into adopting neuron behaviors\\nfor effective and robust AI aided multimedia fakes forensics as an inside-out\\napproach instead of being motivated and swayed by various artifacts introduced\\nin synthesizing fakes.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Run Wang'}, {'name': 'Felix Juefei-Xu'}, {'name': 'Yihao Huang'}, {'name': 'Qing Guo'}, {'name': 'Xiaofei Xie'}, {'name': 'Lei Ma'}, {'name': 'Yang Liu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Yang Liu'}\n",
      "\n",
      "\n",
      "author\n",
      "Yang Liu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted by ACM MM'20\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2005.13770v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2005.13770v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2005.07034v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2005.07034v2\n",
      "\n",
      "\n",
      "updated\n",
      "2021-05-04T05:59:16Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=5, tm_mday=4, tm_hour=5, tm_min=59, tm_sec=16, tm_wday=1, tm_yday=124, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-05-13T00:37:43Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=5, tm_mday=13, tm_hour=0, tm_min=37, tm_sec=43, tm_wday=2, tm_yday=134, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFake: Deep Dueling-based Deception Strategy to Defeat Reactive\n",
      "  Jammers\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFake: Deep Dueling-based Deception Strategy to Defeat Reactive\\n  Jammers'}\n",
      "\n",
      "\n",
      "summary\n",
      "In this paper, we introduce DeepFake, a novel deep reinforcement\n",
      "learning-based deception strategy to deal with reactive jamming attacks. In\n",
      "particular, for a smart and reactive jamming attack, the jammer is able to\n",
      "sense the channel and attack the channel if it detects communications from the\n",
      "legitimate transmitter. To deal with such attacks, we propose an intelligent\n",
      "deception strategy which allows the legitimate transmitter to transmit \"fake\"\n",
      "signals to attract the jammer. Then, if the jammer attacks the channel, the\n",
      "transmitter can leverage the strong jamming signals to transmit data by using\n",
      "ambient backscatter communication technology or harvest energy from the strong\n",
      "jamming signals for future use. By doing so, we can not only undermine the\n",
      "attack ability of the jammer, but also utilize jamming signals to improve the\n",
      "system performance. To effectively learn from and adapt to the dynamic and\n",
      "uncertainty of jamming attacks, we develop a novel deep reinforcement learning\n",
      "algorithm using the deep dueling neural network architecture to obtain the\n",
      "optimal policy with thousand times faster than those of the conventional\n",
      "reinforcement algorithms. Extensive simulation results reveal that our proposed\n",
      "DeepFake framework is superior to other anti-jamming strategies in terms of\n",
      "throughput, packet loss, and learning rate.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In this paper, we introduce DeepFake, a novel deep reinforcement\\nlearning-based deception strategy to deal with reactive jamming attacks. In\\nparticular, for a smart and reactive jamming attack, the jammer is able to\\nsense the channel and attack the channel if it detects communications from the\\nlegitimate transmitter. To deal with such attacks, we propose an intelligent\\ndeception strategy which allows the legitimate transmitter to transmit \"fake\"\\nsignals to attract the jammer. Then, if the jammer attacks the channel, the\\ntransmitter can leverage the strong jamming signals to transmit data by using\\nambient backscatter communication technology or harvest energy from the strong\\njamming signals for future use. By doing so, we can not only undermine the\\nattack ability of the jammer, but also utilize jamming signals to improve the\\nsystem performance. To effectively learn from and adapt to the dynamic and\\nuncertainty of jamming attacks, we develop a novel deep reinforcement learning\\nalgorithm using the deep dueling neural network architecture to obtain the\\noptimal policy with thousand times faster than those of the conventional\\nreinforcement algorithms. Extensive simulation results reveal that our proposed\\nDeepFake framework is superior to other anti-jamming strategies in terms of\\nthroughput, packet loss, and learning rate.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Nguyen Van Huynh'}, {'name': 'Dinh Thai Hoang'}, {'name': 'Diep N. Nguyen'}, {'name': 'Eryk Dutkiewicz'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Eryk Dutkiewicz'}\n",
      "\n",
      "\n",
      "author\n",
      "Eryk Dutkiewicz\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "30 pages\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2005.07034v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2005.07034v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.NI', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.NI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.IT', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.SP', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'math.IT', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2005.05535v5\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2005.05535v5\n",
      "\n",
      "\n",
      "updated\n",
      "2021-06-29T07:07:57Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=6, tm_mday=29, tm_hour=7, tm_min=7, tm_sec=57, tm_wday=1, tm_yday=180, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-05-12T03:26:55Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=5, tm_mday=12, tm_hour=3, tm_min=26, tm_sec=55, tm_wday=1, tm_yday=133, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFaceLab: Integrated, flexible and extensible face-swapping framework\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFaceLab: Integrated, flexible and extensible face-swapping framework'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deepfake defense not only requires the research of detection but also\n",
      "requires the efforts of generation methods. However, current deepfake methods\n",
      "suffer the effects of obscure workflow and poor performance. To solve this\n",
      "problem, we present DeepFaceLab, the current dominant deepfake framework for\n",
      "face-swapping. It provides the necessary tools as well as an easy-to-use way to\n",
      "conduct high-quality face-swapping. It also offers a flexible and loose\n",
      "coupling structure for people who need to strengthen their pipeline with other\n",
      "features without writing complicated boilerplate code. We detail the principles\n",
      "that drive the implementation of DeepFaceLab and introduce its pipeline,\n",
      "through which every aspect of the pipeline can be modified painlessly by users\n",
      "to achieve their customization purpose. It is noteworthy that DeepFaceLab could\n",
      "achieve cinema-quality results with high fidelity. We demonstrate the advantage\n",
      "of our system by comparing our approach with other face-swapping methods.For\n",
      "more information, please visit:https://github.com/iperov/DeepFaceLab/.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake defense not only requires the research of detection but also\\nrequires the efforts of generation methods. However, current deepfake methods\\nsuffer the effects of obscure workflow and poor performance. To solve this\\nproblem, we present DeepFaceLab, the current dominant deepfake framework for\\nface-swapping. It provides the necessary tools as well as an easy-to-use way to\\nconduct high-quality face-swapping. It also offers a flexible and loose\\ncoupling structure for people who need to strengthen their pipeline with other\\nfeatures without writing complicated boilerplate code. We detail the principles\\nthat drive the implementation of DeepFaceLab and introduce its pipeline,\\nthrough which every aspect of the pipeline can be modified painlessly by users\\nto achieve their customization purpose. It is noteworthy that DeepFaceLab could\\nachieve cinema-quality results with high fidelity. We demonstrate the advantage\\nof our system by comparing our approach with other face-swapping methods.For\\nmore information, please visit:https://github.com/iperov/DeepFaceLab/.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ivan Perov'}, {'name': 'Daiheng Gao'}, {'name': 'Nikolay Chervoniy'}, {'name': 'Kunlin Liu'}, {'name': 'Sugasa Marangonda'}, {'name': 'Chris Umé'}, {'name': 'Mr. Dpfks'}, {'name': 'Carl Shift Facenheim'}, {'name': 'Luis RP'}, {'name': 'Jian Jiang'}, {'name': 'Sheng Zhang'}, {'name': 'Pingyu Wu'}, {'name': 'Bo Zhou'}, {'name': 'Weiming Zhang'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Weiming Zhang'}\n",
      "\n",
      "\n",
      "author\n",
      "Weiming Zhang\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2005.05535v5', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2005.05535v5', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2005.04945v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2005.04945v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-12-16T06:18:36Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=12, tm_mday=16, tm_hour=6, tm_min=18, tm_sec=36, tm_wday=2, tm_yday=351, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-05-11T09:16:39Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=5, tm_mday=11, tm_hour=9, tm_min=16, tm_sec=39, tm_wday=0, tm_yday=132, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Fake face detection via adaptive manipulation traces extraction network\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Fake face detection via adaptive manipulation traces extraction network'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the proliferation of face image manipulation (FIM) techniques such as\n",
      "Face2Face and Deepfake, more fake face images are spreading over the internet,\n",
      "which brings serious challenges to public confidence. Face image forgery\n",
      "detection has made considerable progresses in exposing specific FIM, but it is\n",
      "still in scarcity of a robust fake face detector to expose face image forgeries\n",
      "under complex scenarios such as with further compression, blurring, scaling,\n",
      "etc. Due to the relatively fixed structure, convolutional neural network (CNN)\n",
      "tends to learn image content representations. However, CNN should learn subtle\n",
      "manipulation traces for image forensics tasks. Thus, we propose an adaptive\n",
      "manipulation traces extraction network (AMTEN), which serves as pre-processing\n",
      "to suppress image content and highlight manipulation traces. AMTEN exploits an\n",
      "adaptive convolution layer to predict manipulation traces in the image, which\n",
      "are reused in subsequent layers to maximize manipulation artifacts by updating\n",
      "weights during the back-propagation pass. A fake face detector, namely\n",
      "AMTENnet, is constructed by integrating AMTEN with CNN. Experimental results\n",
      "prove that the proposed AMTEN achieves desirable pre-processing. When detecting\n",
      "fake face images generated by various FIM techniques, AMTENnet achieves an\n",
      "average accuracy up to 98.52%, which outperforms the state-of-the-art works.\n",
      "When detecting face images with unknown post-processing operations, the\n",
      "detector also achieves an average accuracy of 95.17%.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'With the proliferation of face image manipulation (FIM) techniques such as\\nFace2Face and Deepfake, more fake face images are spreading over the internet,\\nwhich brings serious challenges to public confidence. Face image forgery\\ndetection has made considerable progresses in exposing specific FIM, but it is\\nstill in scarcity of a robust fake face detector to expose face image forgeries\\nunder complex scenarios such as with further compression, blurring, scaling,\\netc. Due to the relatively fixed structure, convolutional neural network (CNN)\\ntends to learn image content representations. However, CNN should learn subtle\\nmanipulation traces for image forensics tasks. Thus, we propose an adaptive\\nmanipulation traces extraction network (AMTEN), which serves as pre-processing\\nto suppress image content and highlight manipulation traces. AMTEN exploits an\\nadaptive convolution layer to predict manipulation traces in the image, which\\nare reused in subsequent layers to maximize manipulation artifacts by updating\\nweights during the back-propagation pass. A fake face detector, namely\\nAMTENnet, is constructed by integrating AMTEN with CNN. Experimental results\\nprove that the proposed AMTEN achieves desirable pre-processing. When detecting\\nfake face images generated by various FIM techniques, AMTENnet achieves an\\naverage accuracy up to 98.52%, which outperforms the state-of-the-art works.\\nWhen detecting face images with unknown post-processing operations, the\\ndetector also achieves an average accuracy of 95.17%.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Zhiqing Guo'}, {'name': 'Gaobo Yang'}, {'name': 'Jiyou Chen'}, {'name': 'Xingming Sun'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Xingming Sun'}\n",
      "\n",
      "\n",
      "author\n",
      "Xingming Sun\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2005.04945v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2005.04945v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2005.00229v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2005.00229v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-05-01T05:27:16Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=5, tm_mday=1, tm_hour=5, tm_min=27, tm_sec=16, tm_wday=4, tm_yday=122, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-05-01T05:27:16Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=5, tm_mday=1, tm_hour=5, tm_min=27, tm_sec=16, tm_wday=4, tm_yday=122, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Forensics Using Recurrent Neural Networks\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake Forensics Using Recurrent Neural Networks'}\n",
      "\n",
      "\n",
      "summary\n",
      "As of late an AI based free programming device has made it simple to make\n",
      "authentic face swaps in recordings that leaves barely any hints of control, in\n",
      "what are known as \"deepfake\" recordings. Situations where these genuine istic\n",
      "counterfeit recordings are utilized to make political pain, extort somebody or\n",
      "phony fear based oppression occasions are effectively imagined. This paper\n",
      "proposes a transient mindful pipeline to automat-ically recognize deepfake\n",
      "recordings. Our framework utilizes a convolutional neural system (CNN) to\n",
      "remove outline level highlights. These highlights are then used to prepare a\n",
      "repetitive neural net-work (RNN) that figures out how to characterize if a\n",
      "video has been sub-ject to control or not. We assess our technique against a\n",
      "huge arrangement of deepfake recordings gathered from different video sites. We\n",
      "show how our framework can accomplish aggressive outcomes in this assignment\n",
      "while utilizing a basic design.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'As of late an AI based free programming device has made it simple to make\\nauthentic face swaps in recordings that leaves barely any hints of control, in\\nwhat are known as \"deepfake\" recordings. Situations where these genuine istic\\ncounterfeit recordings are utilized to make political pain, extort somebody or\\nphony fear based oppression occasions are effectively imagined. This paper\\nproposes a transient mindful pipeline to automat-ically recognize deepfake\\nrecordings. Our framework utilizes a convolutional neural system (CNN) to\\nremove outline level highlights. These highlights are then used to prepare a\\nrepetitive neural net-work (RNN) that figures out how to characterize if a\\nvideo has been sub-ject to control or not. We assess our technique against a\\nhuge arrangement of deepfake recordings gathered from different video sites. We\\nshow how our framework can accomplish aggressive outcomes in this assignment\\nwhile utilizing a basic design.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Rahul U'}, {'name': 'Ragul M'}, {'name': 'Raja Vignesh K'}, {'name': 'Tejeswinee K'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Tejeswinee K'}\n",
      "\n",
      "\n",
      "author\n",
      "Tejeswinee K\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1729/Journal.22894\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1729/Journal.22894', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2005.00229v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2005.00229v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "This submission has been removed by arXiv administrators due to\n",
      "  copyright infringement\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2004.14178v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2004.14178v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-04-29T13:21:28Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=29, tm_hour=13, tm_min=21, tm_sec=28, tm_wday=2, tm_yday=120, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-04-29T13:21:28Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=29, tm_hour=13, tm_min=21, tm_sec=28, tm_wday=2, tm_yday=120, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfake Video Forensics based on Transfer Learning\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfake Video Forensics based on Transfer Learning'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deeplearning has been used to solve complex problems in various domains. As\n",
      "it advances, it also creates applications which become a major threat to our\n",
      "privacy, security and even to our Democracy. Such an application which is being\n",
      "developed recently is the \"Deepfake\". Deepfake models can create fake images\n",
      "and videos that humans cannot differentiate them from the genuine ones.\n",
      "Therefore, the counter application to automatically detect and analyze the\n",
      "digital visual media is necessary in today world. This paper details retraining\n",
      "the image classification models to apprehend the features from each deepfake\n",
      "video frames. After feeding different sets of deepfake clips of video fringes\n",
      "through a pretrained layer of bottleneck in the neural network is made for\n",
      "every video frame, already stated layer contains condense data for all images\n",
      "and exposes artificial manipulations in Deepfake videos. When checking Deepfake\n",
      "videos, this technique received more than 87 per cent accuracy. This technique\n",
      "has been tested on the Face Forensics dataset and obtained good accuracy in\n",
      "detection.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deeplearning has been used to solve complex problems in various domains. As\\nit advances, it also creates applications which become a major threat to our\\nprivacy, security and even to our Democracy. Such an application which is being\\ndeveloped recently is the \"Deepfake\". Deepfake models can create fake images\\nand videos that humans cannot differentiate them from the genuine ones.\\nTherefore, the counter application to automatically detect and analyze the\\ndigital visual media is necessary in today world. This paper details retraining\\nthe image classification models to apprehend the features from each deepfake\\nvideo frames. After feeding different sets of deepfake clips of video fringes\\nthrough a pretrained layer of bottleneck in the neural network is made for\\nevery video frame, already stated layer contains condense data for all images\\nand exposes artificial manipulations in Deepfake videos. When checking Deepfake\\nvideos, this technique received more than 87 per cent accuracy. This technique\\nhas been tested on the Face Forensics dataset and obtained good accuracy in\\ndetection.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Rahul U'}, {'name': 'Ragul M'}, {'name': 'Raja Vignesh K'}, {'name': 'Tejeswinee K'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Tejeswinee K'}\n",
      "\n",
      "\n",
      "author\n",
      "Tejeswinee K\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.35940/ijrte.F9747.038620\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.35940/ijrte.F9747.038620', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2004.14178v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2004.14178v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "This submission has been removed by arXiv administrators due to\n",
      "  copyright infringement\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2004.12626v5\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2004.12626v5\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-04T09:27:14Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=4, tm_hour=9, tm_min=27, tm_sec=14, tm_wday=1, tm_yday=217, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-04-27T08:09:06Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=27, tm_hour=8, tm_min=9, tm_sec=6, tm_wday=0, tm_yday=118, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Preliminary Forensics Analysis of DeepFake Images\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Preliminary Forensics Analysis of DeepFake Images'}\n",
      "\n",
      "\n",
      "summary\n",
      "One of the most terrifying phenomenon nowadays is the DeepFake: the\n",
      "possibility to automatically replace a person's face in images and videos by\n",
      "exploiting algorithms based on deep learning. This paper will present a brief\n",
      "overview of technologies able to produce DeepFake images of faces. A forensics\n",
      "analysis of those images with standard methods will be presented: not\n",
      "surprisingly state of the art techniques are not completely able to detect the\n",
      "fakeness. To solve this, a preliminary idea on how to fight DeepFake images of\n",
      "faces will be presented by analysing anomalies in the frequency domain.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"One of the most terrifying phenomenon nowadays is the DeepFake: the\\npossibility to automatically replace a person's face in images and videos by\\nexploiting algorithms based on deep learning. This paper will present a brief\\noverview of technologies able to produce DeepFake images of faces. A forensics\\nanalysis of those images with standard methods will be presented: not\\nsurprisingly state of the art techniques are not completely able to detect the\\nfakeness. To solve this, a preliminary idea on how to fight DeepFake images of\\nfaces will be presented by analysing anomalies in the frequency domain.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Luca Guarnera'}, {'name': 'Oliver Giudice'}, {'name': 'Cristina Nastasi'}, {'name': 'Sebastiano Battiato'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Sebastiano Battiato'}\n",
      "\n",
      "\n",
      "arxiv_affiliation\n",
      "iCTLab s.r.l. - Spin-off of University of Catania\n",
      "\n",
      "\n",
      "author\n",
      "Sebastiano Battiato\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.23919/AEIT50178.2020.9241108\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.23919/AEIT50178.2020.9241108', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2004.12626v5', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2004.12626v5', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted at IEEE AEIT International Annual Conference 2020\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "2020 AEIT International Annual Conference (AEIT)\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2004.12027v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2004.12027v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-05-04T19:44:49Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=5, tm_mday=4, tm_hour=19, tm_min=44, tm_sec=49, tm_wday=0, tm_yday=125, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-04-25T00:47:42Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=25, tm_hour=0, tm_min=47, tm_sec=42, tm_wday=5, tm_yday=116, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfakes Detection with Automatic Face Weighting\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfakes Detection with Automatic Face Weighting'}\n",
      "\n",
      "\n",
      "summary\n",
      "Altered and manipulated multimedia is increasingly present and widely\n",
      "distributed via social media platforms. Advanced video manipulation tools\n",
      "enable the generation of highly realistic-looking altered multimedia. While\n",
      "many methods have been presented to detect manipulations, most of them fail\n",
      "when evaluated with data outside of the datasets used in research environments.\n",
      "In order to address this problem, the Deepfake Detection Challenge (DFDC)\n",
      "provides a large dataset of videos containing realistic manipulations and an\n",
      "evaluation system that ensures that methods work quickly and accurately, even\n",
      "when faced with challenging data. In this paper, we introduce a method based on\n",
      "convolutional neural networks (CNNs) and recurrent neural networks (RNNs) that\n",
      "extracts visual and temporal features from faces present in videos to\n",
      "accurately detect manipulations. The method is evaluated with the DFDC dataset,\n",
      "providing competitive results compared to other techniques.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Altered and manipulated multimedia is increasingly present and widely\\ndistributed via social media platforms. Advanced video manipulation tools\\nenable the generation of highly realistic-looking altered multimedia. While\\nmany methods have been presented to detect manipulations, most of them fail\\nwhen evaluated with data outside of the datasets used in research environments.\\nIn order to address this problem, the Deepfake Detection Challenge (DFDC)\\nprovides a large dataset of videos containing realistic manipulations and an\\nevaluation system that ensures that methods work quickly and accurately, even\\nwhen faced with challenging data. In this paper, we introduce a method based on\\nconvolutional neural networks (CNNs) and recurrent neural networks (RNNs) that\\nextracts visual and temporal features from faces present in videos to\\naccurately detect manipulations. The method is evaluated with the DFDC dataset,\\nproviding competitive results compared to other techniques.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Daniel Mas Montserrat'}, {'name': 'Hanxiang Hao'}, {'name': 'S. K. Yarlagadda'}, {'name': 'Sriram Baireddy'}, {'name': 'Ruiting Shao'}, {'name': 'János Horváth'}, {'name': 'Emily Bartusiak'}, {'name': 'Justin Yang'}, {'name': 'David Güera'}, {'name': 'Fengqing Zhu'}, {'name': 'Edward J. Delp'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Edward J. Delp'}\n",
      "\n",
      "\n",
      "author\n",
      "Edward J. Delp\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2004.12027v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2004.12027v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2004.11138v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2004.11138v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-09-13T22:44:33Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=9, tm_mday=13, tm_hour=22, tm_min=44, tm_sec=33, tm_wday=6, tm_yday=257, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-04-23T13:35:49Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=23, tm_hour=13, tm_min=35, tm_sec=49, tm_wday=3, tm_yday=114, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "The Creation and Detection of Deepfakes: A Survey\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The Creation and Detection of Deepfakes: A Survey'}\n",
      "\n",
      "\n",
      "summary\n",
      "Generative deep learning algorithms have progressed to a point where it is\n",
      "difficult to tell the difference between what is real and what is fake. In\n",
      "2018, it was discovered how easy it is to use this technology for unethical and\n",
      "malicious applications, such as the spread of misinformation, impersonation of\n",
      "political leaders, and the defamation of innocent individuals. Since then,\n",
      "these `deepfakes' have advanced significantly.\n",
      "  In this paper, we explore the creation and detection of deepfakes and provide\n",
      "an in-depth view of how these architectures work. The purpose of this survey is\n",
      "to provide the reader with a deeper understanding of (1) how deepfakes are\n",
      "created and detected, (2) the current trends and advancements in this domain,\n",
      "(3) the shortcomings of the current defense solutions, and (4) the areas which\n",
      "require further research and attention.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"Generative deep learning algorithms have progressed to a point where it is\\ndifficult to tell the difference between what is real and what is fake. In\\n2018, it was discovered how easy it is to use this technology for unethical and\\nmalicious applications, such as the spread of misinformation, impersonation of\\npolitical leaders, and the defamation of innocent individuals. Since then,\\nthese `deepfakes' have advanced significantly.\\n  In this paper, we explore the creation and detection of deepfakes and provide\\nan in-depth view of how these architectures work. The purpose of this survey is\\nto provide the reader with a deeper understanding of (1) how deepfakes are\\ncreated and detected, (2) the current trends and advancements in this domain,\\n(3) the shortcomings of the current defense solutions, and (4) the areas which\\nrequire further research and attention.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yisroel Mirsky'}, {'name': 'Wenke Lee'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Wenke Lee'}\n",
      "\n",
      "\n",
      "author\n",
      "Wenke Lee\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1145/3425780\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3425780', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2004.11138v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2004.11138v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "ACM Computing Surveys (CSUR), 2020, preprint\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2004.10448v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2004.10448v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-04-22T09:02:55Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=22, tm_hour=9, tm_min=2, tm_sec=55, tm_wday=2, tm_yday=113, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-04-22T09:02:55Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=22, tm_hour=9, tm_min=2, tm_sec=55, tm_wday=2, tm_yday=113, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFake Detection by Analyzing Convolutional Traces\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFake Detection by Analyzing Convolutional Traces'}\n",
      "\n",
      "\n",
      "summary\n",
      "The Deepfake phenomenon has become very popular nowadays thanks to the\n",
      "possibility to create incredibly realistic images using deep learning tools,\n",
      "based mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we\n",
      "focus on the analysis of Deepfakes of human faces with the objective of\n",
      "creating a new detection method able to detect a forensics trace hidden in\n",
      "images: a sort of fingerprint left in the image generation process. The\n",
      "proposed technique, by means of an Expectation Maximization (EM) algorithm,\n",
      "extracts a set of local features specifically addressed to model the underlying\n",
      "convolutional generative process. Ad-hoc validation has been employed through\n",
      "experimental tests with naive classifiers on five different architectures\n",
      "(GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as\n",
      "ground-truth for non-fakes. Results demonstrated the effectiveness of the\n",
      "technique in distinguishing the different architectures and the corresponding\n",
      "generation process.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The Deepfake phenomenon has become very popular nowadays thanks to the\\npossibility to create incredibly realistic images using deep learning tools,\\nbased mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we\\nfocus on the analysis of Deepfakes of human faces with the objective of\\ncreating a new detection method able to detect a forensics trace hidden in\\nimages: a sort of fingerprint left in the image generation process. The\\nproposed technique, by means of an Expectation Maximization (EM) algorithm,\\nextracts a set of local features specifically addressed to model the underlying\\nconvolutional generative process. Ad-hoc validation has been employed through\\nexperimental tests with naive classifiers on five different architectures\\n(GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as\\nground-truth for non-fakes. Results demonstrated the effectiveness of the\\ntechnique in distinguishing the different architectures and the corresponding\\ngeneration process.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Luca Guarnera'}, {'name': 'Oliver Giudice'}, {'name': 'Sebastiano Battiato'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Sebastiano Battiato'}\n",
      "\n",
      "\n",
      "arxiv_affiliation\n",
      "iCTLab s.r.l. - Spin-off of University of Catania\n",
      "\n",
      "\n",
      "author\n",
      "Sebastiano Battiato\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "IEEE Conference on Computer Vision and Pattern Recognition\n",
      "  Workshops 2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2004.10448v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2004.10448v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2004.07676v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2004.07676v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-04-16T14:19:40Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=16, tm_hour=14, tm_min=19, tm_sec=40, tm_wday=3, tm_yday=107, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-04-16T14:19:40Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=16, tm_hour=14, tm_min=19, tm_sec=40, tm_wday=3, tm_yday=107, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Video Face Manipulation Detection Through Ensemble of CNNs\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Video Face Manipulation Detection Through Ensemble of CNNs'}\n",
      "\n",
      "\n",
      "summary\n",
      "In the last few years, several techniques for facial manipulation in videos\n",
      "have been successfully developed and made available to the masses (i.e.,\n",
      "FaceSwap, deepfake, etc.). These methods enable anyone to easily edit faces in\n",
      "video sequences with incredibly realistic results and a very little effort.\n",
      "Despite the usefulness of these tools in many fields, if used maliciously, they\n",
      "can have a significantly bad impact on society (e.g., fake news spreading,\n",
      "cyber bullying through fake revenge porn). The ability of objectively detecting\n",
      "whether a face has been manipulated in a video sequence is then a task of\n",
      "utmost importance. In this paper, we tackle the problem of face manipulation\n",
      "detection in video sequences targeting modern facial manipulation techniques.\n",
      "In particular, we study the ensembling of different trained Convolutional\n",
      "Neural Network (CNN) models. In the proposed solution, different models are\n",
      "obtained starting from a base network (i.e., EfficientNetB4) making use of two\n",
      "different concepts: (i) attention layers; (ii) siamese training. We show that\n",
      "combining these networks leads to promising face manipulation detection results\n",
      "on two publicly available datasets with more than 119000 videos.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In the last few years, several techniques for facial manipulation in videos\\nhave been successfully developed and made available to the masses (i.e.,\\nFaceSwap, deepfake, etc.). These methods enable anyone to easily edit faces in\\nvideo sequences with incredibly realistic results and a very little effort.\\nDespite the usefulness of these tools in many fields, if used maliciously, they\\ncan have a significantly bad impact on society (e.g., fake news spreading,\\ncyber bullying through fake revenge porn). The ability of objectively detecting\\nwhether a face has been manipulated in a video sequence is then a task of\\nutmost importance. In this paper, we tackle the problem of face manipulation\\ndetection in video sequences targeting modern facial manipulation techniques.\\nIn particular, we study the ensembling of different trained Convolutional\\nNeural Network (CNN) models. In the proposed solution, different models are\\nobtained starting from a base network (i.e., EfficientNetB4) making use of two\\ndifferent concepts: (i) attention layers; (ii) siamese training. We show that\\ncombining these networks leads to promising face manipulation detection results\\non two publicly available datasets with more than 119000 videos.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Nicolò Bonettini'}, {'name': 'Edoardo Daniele Cannas'}, {'name': 'Sara Mandelli'}, {'name': 'Luca Bondi'}, {'name': 'Paolo Bestagini'}, {'name': 'Stefano Tubaro'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Stefano Tubaro'}\n",
      "\n",
      "\n",
      "author\n",
      "Stefano Tubaro\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2004.07676v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2004.07676v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2004.07532v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2004.07532v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-07-02T16:24:22Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=7, tm_mday=2, tm_hour=16, tm_min=24, tm_sec=22, tm_wday=3, tm_yday=184, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-04-16T08:49:32Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=16, tm_hour=8, tm_min=49, tm_sec=32, tm_wday=3, tm_yday=107, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFakes Evolution: Analysis of Facial Regions and Fake Detection\n",
      "  Performance\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFakes Evolution: Analysis of Facial Regions and Fake Detection\\n  Performance'}\n",
      "\n",
      "\n",
      "summary\n",
      "Media forensics has attracted a lot of attention in the last years in part\n",
      "due to the increasing concerns around DeepFakes. Since the initial DeepFake\n",
      "databases from the 1st generation such as UADFV and FaceForensics++ up to the\n",
      "latest databases of the 2nd generation such as Celeb-DF and DFDC, many visual\n",
      "improvements have been carried out, making fake videos almost indistinguishable\n",
      "to the human eye. This study provides an exhaustive analysis of both 1st and\n",
      "2nd DeepFake generations in terms of facial regions and fake detection\n",
      "performance. Two different methods are considered in our experimental\n",
      "framework: i) the traditional one followed in the literature and based on\n",
      "selecting the entire face as input to the fake detection system, and ii) a\n",
      "novel approach based on the selection of specific facial regions as input to\n",
      "the fake detection system.\n",
      "  Among all the findings resulting from our experiments, we highlight the poor\n",
      "fake detection results achieved even by the strongest state-of-the-art fake\n",
      "detectors in the latest DeepFake databases of the 2nd generation, with Equal\n",
      "Error Rate results ranging from 15% to 30%. These results remark the necessity\n",
      "of further research to develop more sophisticated fake detectors.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Media forensics has attracted a lot of attention in the last years in part\\ndue to the increasing concerns around DeepFakes. Since the initial DeepFake\\ndatabases from the 1st generation such as UADFV and FaceForensics++ up to the\\nlatest databases of the 2nd generation such as Celeb-DF and DFDC, many visual\\nimprovements have been carried out, making fake videos almost indistinguishable\\nto the human eye. This study provides an exhaustive analysis of both 1st and\\n2nd DeepFake generations in terms of facial regions and fake detection\\nperformance. Two different methods are considered in our experimental\\nframework: i) the traditional one followed in the literature and based on\\nselecting the entire face as input to the fake detection system, and ii) a\\nnovel approach based on the selection of specific facial regions as input to\\nthe fake detection system.\\n  Among all the findings resulting from our experiments, we highlight the poor\\nfake detection results achieved even by the strongest state-of-the-art fake\\ndetectors in the latest DeepFake databases of the 2nd generation, with Equal\\nError Rate results ranging from 15% to 30%. These results remark the necessity\\nof further research to develop more sophisticated fake detectors.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ruben Tolosana'}, {'name': 'Sergio Romero-Tapiador'}, {'name': 'Julian Fierrez'}, {'name': 'Ruben Vera-Rodriguez'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Ruben Vera-Rodriguez'}\n",
      "\n",
      "\n",
      "author\n",
      "Ruben Vera-Rodriguez\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "Proc. International Conference on Pattern Recognition Workshops\n",
      "  2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2004.07532v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2004.07532v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2004.00622v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2004.00622v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-04-01T17:59:59Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=1, tm_hour=17, tm_min=59, tm_sec=59, tm_wday=2, tm_yday=92, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-04-01T17:59:59Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=1, tm_hour=17, tm_min=59, tm_sec=59, tm_wday=2, tm_yday=92, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Evading Deepfake-Image Detectors with White- and Black-Box Attacks\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Evading Deepfake-Image Detectors with White- and Black-Box Attacks'}\n",
      "\n",
      "\n",
      "summary\n",
      "It is now possible to synthesize highly realistic images of people who don't\n",
      "exist. Such content has, for example, been implicated in the creation of\n",
      "fraudulent social-media profiles responsible for dis-information campaigns.\n",
      "Significant efforts are, therefore, being deployed to detect\n",
      "synthetically-generated content. One popular forensic approach trains a neural\n",
      "network to distinguish real from synthetic content.\n",
      "  We show that such forensic classifiers are vulnerable to a range of attacks\n",
      "that reduce the classifier to near-0% accuracy. We develop five attack case\n",
      "studies on a state-of-the-art classifier that achieves an area under the ROC\n",
      "curve (AUC) of 0.95 on almost all existing image generators, when only trained\n",
      "on one generator. With full access to the classifier, we can flip the lowest\n",
      "bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb\n",
      "1% of the image area to reduce the classifier's AUC to 0.08; or add a single\n",
      "noise pattern in the synthesizer's latent space to reduce the classifier's AUC\n",
      "to 0.17. We also develop a black-box attack that, with no access to the target\n",
      "classifier, reduces the AUC to 0.22. These attacks reveal significant\n",
      "vulnerabilities of certain image-forensic classifiers.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"It is now possible to synthesize highly realistic images of people who don't\\nexist. Such content has, for example, been implicated in the creation of\\nfraudulent social-media profiles responsible for dis-information campaigns.\\nSignificant efforts are, therefore, being deployed to detect\\nsynthetically-generated content. One popular forensic approach trains a neural\\nnetwork to distinguish real from synthetic content.\\n  We show that such forensic classifiers are vulnerable to a range of attacks\\nthat reduce the classifier to near-0% accuracy. We develop five attack case\\nstudies on a state-of-the-art classifier that achieves an area under the ROC\\ncurve (AUC) of 0.95 on almost all existing image generators, when only trained\\non one generator. With full access to the classifier, we can flip the lowest\\nbit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb\\n1% of the image area to reduce the classifier's AUC to 0.08; or add a single\\nnoise pattern in the synthesizer's latent space to reduce the classifier's AUC\\nto 0.17. We also develop a black-box attack that, with no access to the target\\nclassifier, reduces the AUC to 0.22. These attacks reveal significant\\nvulnerabilities of certain image-forensic classifiers.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Nicholas Carlini'}, {'name': 'Hany Farid'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Hany Farid'}\n",
      "\n",
      "\n",
      "author\n",
      "Hany Farid\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2004.00622v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2004.00622v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2003.10596v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2003.10596v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-05-15T05:41:32Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=5, tm_mday=15, tm_hour=5, tm_min=41, tm_sec=32, tm_wday=4, tm_yday=136, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-03-24T00:54:02Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=3, tm_mday=24, tm_hour=0, tm_min=54, tm_sec=2, tm_wday=1, tm_yday=84, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Adversarial Perturbations Fool Deepfake Detectors\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Adversarial Perturbations Fool Deepfake Detectors'}\n",
      "\n",
      "\n",
      "summary\n",
      "This work uses adversarial perturbations to enhance deepfake images and fool\n",
      "common deepfake detectors. We created adversarial perturbations using the Fast\n",
      "Gradient Sign Method and the Carlini and Wagner L2 norm attack in both blackbox\n",
      "and whitebox settings. Detectors achieved over 95% accuracy on unperturbed\n",
      "deepfakes, but less than 27% accuracy on perturbed deepfakes. We also explore\n",
      "two improvements to deepfake detectors: (i) Lipschitz regularization, and (ii)\n",
      "Deep Image Prior (DIP). Lipschitz regularization constrains the gradient of the\n",
      "detector with respect to the input in order to increase robustness to input\n",
      "perturbations. The DIP defense removes perturbations using generative\n",
      "convolutional neural networks in an unsupervised manner. Regularization\n",
      "improved the detection of perturbed deepfakes on average, including a 10%\n",
      "accuracy boost in the blackbox case. The DIP defense achieved 95% accuracy on\n",
      "perturbed deepfakes that fooled the original detector, while retaining 98%\n",
      "accuracy in other cases on a 100 image subsample.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'This work uses adversarial perturbations to enhance deepfake images and fool\\ncommon deepfake detectors. We created adversarial perturbations using the Fast\\nGradient Sign Method and the Carlini and Wagner L2 norm attack in both blackbox\\nand whitebox settings. Detectors achieved over 95% accuracy on unperturbed\\ndeepfakes, but less than 27% accuracy on perturbed deepfakes. We also explore\\ntwo improvements to deepfake detectors: (i) Lipschitz regularization, and (ii)\\nDeep Image Prior (DIP). Lipschitz regularization constrains the gradient of the\\ndetector with respect to the input in order to increase robustness to input\\nperturbations. The DIP defense removes perturbations using generative\\nconvolutional neural networks in an unsupervised manner. Regularization\\nimproved the detection of perturbed deepfakes on average, including a 10%\\naccuracy boost in the blackbox case. The DIP defense achieved 95% accuracy on\\nperturbed deepfakes that fooled the original detector, while retaining 98%\\naccuracy in other cases on a 100 image subsample.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Apurva Gandhi'}, {'name': 'Shomik Jain'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Shomik Jain'}\n",
      "\n",
      "\n",
      "author\n",
      "Shomik Jain\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "To appear in the proceedings of the International Joint Conference on\n",
      "  Neural Networks (IJCNN 2020)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2003.10596v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2003.10596v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.4', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2003.08645v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2003.08645v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-03-19T09:44:23Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=3, tm_mday=19, tm_hour=9, tm_min=44, tm_sec=23, tm_wday=3, tm_yday=79, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-03-19T09:44:23Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=3, tm_mday=19, tm_hour=9, tm_min=44, tm_sec=23, tm_wday=3, tm_yday=79, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Detecting Deepfakes with Metric Learning\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Detecting Deepfakes with Metric Learning'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the arrival of several face-swapping applications such as FaceApp,\n",
      "SnapChat, MixBooth, FaceBlender and many more, the authenticity of digital\n",
      "media content is hanging on a very loose thread. On social media platforms,\n",
      "videos are widely circulated often at a high compression factor. In this work,\n",
      "we analyze several deep learning approaches in the context of deepfakes\n",
      "classification in high compression scenario and demonstrate that a proposed\n",
      "approach based on metric learning can be very effective in performing such a\n",
      "classification. Using less number of frames per video to assess its realism,\n",
      "the metric learning approach using a triplet network architecture proves to be\n",
      "fruitful. It learns to enhance the feature space distance between the cluster\n",
      "of real and fake videos embedding vectors. We validated our approaches on two\n",
      "datasets to analyze the behavior in different environments. We achieved a\n",
      "state-of-the-art AUC score of 99.2% on the Celeb-DF dataset and accuracy of\n",
      "90.71% on a highly compressed Neural Texture dataset. Our approach is\n",
      "especially helpful on social media platforms where data compression is\n",
      "inevitable.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'With the arrival of several face-swapping applications such as FaceApp,\\nSnapChat, MixBooth, FaceBlender and many more, the authenticity of digital\\nmedia content is hanging on a very loose thread. On social media platforms,\\nvideos are widely circulated often at a high compression factor. In this work,\\nwe analyze several deep learning approaches in the context of deepfakes\\nclassification in high compression scenario and demonstrate that a proposed\\napproach based on metric learning can be very effective in performing such a\\nclassification. Using less number of frames per video to assess its realism,\\nthe metric learning approach using a triplet network architecture proves to be\\nfruitful. It learns to enhance the feature space distance between the cluster\\nof real and fake videos embedding vectors. We validated our approaches on two\\ndatasets to analyze the behavior in different environments. We achieved a\\nstate-of-the-art AUC score of 99.2% on the Celeb-DF dataset and accuracy of\\n90.71% on a highly compressed Neural Texture dataset. Our approach is\\nespecially helpful on social media platforms where data compression is\\ninevitable.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Akash Kumar'}, {'name': 'Arnav Bhavsar'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Arnav Bhavsar'}\n",
      "\n",
      "\n",
      "author\n",
      "Arnav Bhavsar\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2003.08645v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2003.08645v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2003.06711v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2003.06711v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-01T20:43:34Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=1, tm_hour=20, tm_min=43, tm_sec=34, tm_wday=5, tm_yday=214, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-03-14T22:07:26Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=3, tm_mday=14, tm_hour=22, tm_min=7, tm_sec=26, tm_wday=5, tm_yday=74, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Emotions Don't Lie: An Audio-Visual Deepfake Detection Method Using\n",
      "  Affective Cues\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"Emotions Don't Lie: An Audio-Visual Deepfake Detection Method Using\\n  Affective Cues\"}\n",
      "\n",
      "\n",
      "summary\n",
      "We present a learning-based method for detecting real and fake deepfake\n",
      "multimedia content. To maximize information for learning, we extract and\n",
      "analyze the similarity between the two audio and visual modalities from within\n",
      "the same video. Additionally, we extract and compare affective cues\n",
      "corresponding to perceived emotion from the two modalities within a video to\n",
      "infer whether the input video is \"real\" or \"fake\". We propose a deep learning\n",
      "network, inspired by the Siamese network architecture and the triplet loss. To\n",
      "validate our model, we report the AUC metric on two large-scale deepfake\n",
      "detection datasets, DeepFake-TIMIT Dataset and DFDC. We compare our approach\n",
      "with several SOTA deepfake detection methods and report per-video AUC of 84.4%\n",
      "on the DFDC and 96.6% on the DF-TIMIT datasets, respectively. To the best of\n",
      "our knowledge, ours is the first approach that simultaneously exploits audio\n",
      "and video modalities and also perceived emotions from the two modalities for\n",
      "deepfake detection.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'We present a learning-based method for detecting real and fake deepfake\\nmultimedia content. To maximize information for learning, we extract and\\nanalyze the similarity between the two audio and visual modalities from within\\nthe same video. Additionally, we extract and compare affective cues\\ncorresponding to perceived emotion from the two modalities within a video to\\ninfer whether the input video is \"real\" or \"fake\". We propose a deep learning\\nnetwork, inspired by the Siamese network architecture and the triplet loss. To\\nvalidate our model, we report the AUC metric on two large-scale deepfake\\ndetection datasets, DeepFake-TIMIT Dataset and DFDC. We compare our approach\\nwith several SOTA deepfake detection methods and report per-video AUC of 84.4%\\non the DFDC and 96.6% on the DF-TIMIT datasets, respectively. To the best of\\nour knowledge, ours is the first approach that simultaneously exploits audio\\nand video modalities and also perceived emotions from the two modalities for\\ndeepfake detection.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Trisha Mittal'}, {'name': 'Uttaran Bhattacharya'}, {'name': 'Rohan Chandra'}, {'name': 'Aniket Bera'}, {'name': 'Dinesh Manocha'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Dinesh Manocha'}\n",
      "\n",
      "\n",
      "author\n",
      "Dinesh Manocha\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to ACMMM-2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2003.06711v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2003.06711v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2003.09234v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2003.09234v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-03-11T13:20:42Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=3, tm_mday=11, tm_hour=13, tm_min=20, tm_sec=42, tm_wday=2, tm_yday=71, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-03-11T13:20:42Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=3, tm_mday=11, tm_hour=13, tm_min=20, tm_sec=42, tm_wday=2, tm_yday=71, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFake Detection: Current Challenges and Next Steps\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFake Detection: Current Challenges and Next Steps'}\n",
      "\n",
      "\n",
      "summary\n",
      "High quality fake videos and audios generated by AI-algorithms (the deep\n",
      "fakes) have started to challenge the status of videos and audios as definitive\n",
      "evidence of events. In this paper, we highlight a few of these challenges and\n",
      "discuss the research opportunities in this direction.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'High quality fake videos and audios generated by AI-algorithms (the deep\\nfakes) have started to challenge the status of videos and audios as definitive\\nevidence of events. In this paper, we highlight a few of these challenges and\\ndiscuss the research opportunities in this direction.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Siwei Lyu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Siwei Lyu'}\n",
      "\n",
      "\n",
      "author\n",
      "Siwei Lyu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "arXiv admin note: text overlap with arXiv:1909.12962\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2003.09234v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2003.09234v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2003.01826v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2003.01826v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-03-03T23:04:33Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=3, tm_mday=3, tm_hour=23, tm_min=4, tm_sec=33, tm_wday=1, tm_yday=63, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-03-03T23:04:33Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=3, tm_mday=3, tm_hour=23, tm_min=4, tm_sec=33, tm_wday=1, tm_yday=63, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are\n",
      "  Failing to Reproduce Spectral Distributions\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are\\n  Failing to Reproduce Spectral Distributions'}\n",
      "\n",
      "\n",
      "summary\n",
      "Generative convolutional deep neural networks, e.g. popular GAN\n",
      "architectures, are relying on convolution based up-sampling methods to produce\n",
      "non-scalar outputs like images or video sequences. In this paper, we show that\n",
      "common up-sampling methods, i.e. known as up-convolution or transposed\n",
      "convolution, are causing the inability of such models to reproduce spectral\n",
      "distributions of natural training data correctly. This effect is independent of\n",
      "the underlying architecture and we show that it can be used to easily detect\n",
      "generated data like deepfakes with up to 100% accuracy on public benchmarks.\n",
      "  To overcome this drawback of current generative models, we propose to add a\n",
      "novel spectral regularization term to the training optimization objective. We\n",
      "show that this approach not only allows to train spectral consistent GANs that\n",
      "are avoiding high frequency errors. Also, we show that a correct approximation\n",
      "of the frequency spectrum has positive effects on the training stability and\n",
      "output quality of generative networks.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Generative convolutional deep neural networks, e.g. popular GAN\\narchitectures, are relying on convolution based up-sampling methods to produce\\nnon-scalar outputs like images or video sequences. In this paper, we show that\\ncommon up-sampling methods, i.e. known as up-convolution or transposed\\nconvolution, are causing the inability of such models to reproduce spectral\\ndistributions of natural training data correctly. This effect is independent of\\nthe underlying architecture and we show that it can be used to easily detect\\ngenerated data like deepfakes with up to 100% accuracy on public benchmarks.\\n  To overcome this drawback of current generative models, we propose to add a\\nnovel spectral regularization term to the training optimization objective. We\\nshow that this approach not only allows to train spectral consistent GANs that\\nare avoiding high frequency errors. Also, we show that a correct approximation\\nof the frequency spectrum has positive effects on the training stability and\\noutput quality of generative networks.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ricard Durall'}, {'name': 'Margret Keuper'}, {'name': 'Janis Keuper'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Janis Keuper'}\n",
      "\n",
      "\n",
      "author\n",
      "Janis Keuper\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2003.01826v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2003.01826v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2003.01279v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2003.01279v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-04-27T19:58:25Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=27, tm_hour=19, tm_min=58, tm_sec=25, tm_wday=0, tm_yday=118, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-03-03T01:18:16Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=3, tm_mday=3, tm_hour=1, tm_min=18, tm_sec=16, tm_wday=1, tm_yday=63, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Disrupting Deepfakes: Adversarial Attacks Against Conditional Image\n",
      "  Translation Networks and Facial Manipulation Systems\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Disrupting Deepfakes: Adversarial Attacks Against Conditional Image\\n  Translation Networks and Facial Manipulation Systems'}\n",
      "\n",
      "\n",
      "summary\n",
      "Face modification systems using deep learning have become increasingly\n",
      "powerful and accessible. Given images of a person's face, such systems can\n",
      "generate new images of that same person under different expressions and poses.\n",
      "Some systems can also modify targeted attributes such as hair color or age.\n",
      "This type of manipulated images and video have been coined Deepfakes. In order\n",
      "to prevent a malicious user from generating modified images of a person without\n",
      "their consent we tackle the new problem of generating adversarial attacks\n",
      "against such image translation systems, which disrupt the resulting output\n",
      "image. We call this problem disrupting deepfakes. Most image translation\n",
      "architectures are generative models conditioned on an attribute (e.g. put a\n",
      "smile on this person's face). We are first to propose and successfully apply\n",
      "(1) class transferable adversarial attacks that generalize to different\n",
      "classes, which means that the attacker does not need to have knowledge about\n",
      "the conditioning class, and (2) adversarial training for generative adversarial\n",
      "networks (GANs) as a first step towards robust image translation networks.\n",
      "Finally, in gray-box scenarios, blurring can mount a successful defense against\n",
      "disruption. We present a spread-spectrum adversarial attack, which evades blur\n",
      "defenses. Our open-source code can be found at\n",
      "https://github.com/natanielruiz/disrupting-deepfakes.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"Face modification systems using deep learning have become increasingly\\npowerful and accessible. Given images of a person's face, such systems can\\ngenerate new images of that same person under different expressions and poses.\\nSome systems can also modify targeted attributes such as hair color or age.\\nThis type of manipulated images and video have been coined Deepfakes. In order\\nto prevent a malicious user from generating modified images of a person without\\ntheir consent we tackle the new problem of generating adversarial attacks\\nagainst such image translation systems, which disrupt the resulting output\\nimage. We call this problem disrupting deepfakes. Most image translation\\narchitectures are generative models conditioned on an attribute (e.g. put a\\nsmile on this person's face). We are first to propose and successfully apply\\n(1) class transferable adversarial attacks that generalize to different\\nclasses, which means that the attacker does not need to have knowledge about\\nthe conditioning class, and (2) adversarial training for generative adversarial\\nnetworks (GANs) as a first step towards robust image translation networks.\\nFinally, in gray-box scenarios, blurring can mount a successful defense against\\ndisruption. We present a spread-spectrum adversarial attack, which evades blur\\ndefenses. Our open-source code can be found at\\nhttps://github.com/natanielruiz/disrupting-deepfakes.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Nataniel Ruiz'}, {'name': 'Sarah Adel Bargal'}, {'name': 'Stan Sclaroff'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Stan Sclaroff'}\n",
      "\n",
      "\n",
      "author\n",
      "Stan Sclaroff\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted at CVPR 2020 Workshop on Adversarial Machine Learning in\n",
      "  Computer Vision\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2003.01279v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2003.01279v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2002.06890v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2002.06890v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-11-13T13:18:10Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=13, tm_hour=13, tm_min=18, tm_sec=10, tm_wday=4, tm_yday=318, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-02-17T11:12:39Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=2, tm_mday=17, tm_hour=11, tm_min=12, tm_sec=39, tm_wday=0, tm_yday=48, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Amplifying The Uncanny\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Amplifying The Uncanny'}\n",
      "\n",
      "\n",
      "summary\n",
      "Deep neural networks have become remarkably good at producing realistic\n",
      "deepfakes, images of people that (to the untrained eye) are indistinguishable\n",
      "from real images. Deepfakes are produced by algorithms that learn to\n",
      "distinguish between real and fake images and are optimised to generate samples\n",
      "that the system deems realistic. This paper, and the resulting series of\n",
      "artworks Being Foiled explore the aesthetic outcome of inverting this process,\n",
      "instead optimising the system to generate images that it predicts as being\n",
      "fake. This maximises the unlikelihood of the data and in turn, amplifies the\n",
      "uncanny nature of these machine hallucinations.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deep neural networks have become remarkably good at producing realistic\\ndeepfakes, images of people that (to the untrained eye) are indistinguishable\\nfrom real images. Deepfakes are produced by algorithms that learn to\\ndistinguish between real and fake images and are optimised to generate samples\\nthat the system deems realistic. This paper, and the resulting series of\\nartworks Being Foiled explore the aesthetic outcome of inverting this process,\\ninstead optimising the system to generate images that it predicts as being\\nfake. This maximises the unlikelihood of the data and in turn, amplifies the\\nuncanny nature of these machine hallucinations.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Terence Broad'}, {'name': 'Frederic Fol Leymarie'}, {'name': 'Mick Grierson'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Mick Grierson'}\n",
      "\n",
      "\n",
      "author\n",
      "Mick Grierson\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "Proceedings of the Eighth Conference on Proceedings of the Eighth\n",
      "  Conference on Computation, Communication, Aesthetics & X, 2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2002.06890v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2002.06890v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2002.12749v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2002.12749v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-11-07T22:09:38Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=7, tm_hour=22, tm_min=9, tm_sec=38, tm_wday=5, tm_yday=312, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-02-09T07:10:58Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=2, tm_mday=9, tm_hour=7, tm_min=10, tm_sec=58, tm_wday=6, tm_yday=40, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to\n",
      "  Adversarial Examples\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to\\n  Adversarial Examples'}\n",
      "\n",
      "\n",
      "summary\n",
      "Recent advances in video manipulation techniques have made the generation of\n",
      "fake videos more accessible than ever before. Manipulated videos can fuel\n",
      "disinformation and reduce trust in media. Therefore detection of fake videos\n",
      "has garnered immense interest in academia and industry. Recently developed\n",
      "Deepfake detection methods rely on deep neural networks (DNNs) to distinguish\n",
      "AI-generated fake videos from real videos. In this work, we demonstrate that it\n",
      "is possible to bypass such detectors by adversarially modifying fake videos\n",
      "synthesized using existing Deepfake generation methods. We further demonstrate\n",
      "that our adversarial perturbations are robust to image and video compression\n",
      "codecs, making them a real-world threat. We present pipelines in both white-box\n",
      "and black-box attack scenarios that can fool DNN based Deepfake detectors into\n",
      "classifying fake videos as real.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Recent advances in video manipulation techniques have made the generation of\\nfake videos more accessible than ever before. Manipulated videos can fuel\\ndisinformation and reduce trust in media. Therefore detection of fake videos\\nhas garnered immense interest in academia and industry. Recently developed\\nDeepfake detection methods rely on deep neural networks (DNNs) to distinguish\\nAI-generated fake videos from real videos. In this work, we demonstrate that it\\nis possible to bypass such detectors by adversarially modifying fake videos\\nsynthesized using existing Deepfake generation methods. We further demonstrate\\nthat our adversarial perturbations are robust to image and video compression\\ncodecs, making them a real-world threat. We present pipelines in both white-box\\nand black-box attack scenarios that can fool DNN based Deepfake detectors into\\nclassifying fake videos as real.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Shehzeen Hussain'}, {'name': 'Paarth Neekhara'}, {'name': 'Malhar Jere'}, {'name': 'Farinaz Koushanfar'}, {'name': 'Julian McAuley'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Julian McAuley'}\n",
      "\n",
      "\n",
      "author\n",
      "Julian McAuley\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Published as a conference paper at WACV 2021\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2002.12749v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2002.12749v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2003.00813v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2003.00813v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-02-07T22:36:48Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=2, tm_mday=7, tm_hour=22, tm_min=36, tm_sec=48, tm_wday=4, tm_yday=38, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-02-07T22:36:48Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=2, tm_mday=7, tm_hour=22, tm_min=36, tm_sec=48, tm_wday=4, tm_yday=38, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Deepfakes for Medical Video De-Identification: Privacy Protection and\n",
      "  Diagnostic Information Preservation\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Deepfakes for Medical Video De-Identification: Privacy Protection and\\n  Diagnostic Information Preservation'}\n",
      "\n",
      "\n",
      "summary\n",
      "Data sharing for medical research has been difficult as open-sourcing\n",
      "clinical data may violate patient privacy. Traditional methods for face\n",
      "de-identification wipe out facial information entirely, making it impossible to\n",
      "analyze facial behavior. Recent advancements on whole-body keypoints detection\n",
      "also rely on facial input to estimate body keypoints. Both facial and body\n",
      "keypoints are critical in some medical diagnoses, and keypoints invariability\n",
      "after de-identification is of great importance. Here, we propose a solution\n",
      "using deepfake technology, the face swapping technique. While this swapping\n",
      "method has been criticized for invading privacy and portraiture right, it could\n",
      "conversely protect privacy in medical video: patients' faces could be swapped\n",
      "to a proper target face and become unrecognizable. However, it remained an open\n",
      "question that to what extent the swapping de-identification method could affect\n",
      "the automatic detection of body keypoints. In this study, we apply deepfake\n",
      "technology to Parkinson's disease examination videos to de-identify subjects,\n",
      "and quantitatively show that: face-swapping as a de-identification approach is\n",
      "reliable, and it keeps the keypoints almost invariant, significantly better\n",
      "than traditional methods. This study proposes a pipeline for video\n",
      "de-identification and keypoint preservation, clearing up some ethical\n",
      "restrictions for medical data sharing. This work could make open-source high\n",
      "quality medical video datasets more feasible and promote future medical\n",
      "research that benefits our society.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': \"Data sharing for medical research has been difficult as open-sourcing\\nclinical data may violate patient privacy. Traditional methods for face\\nde-identification wipe out facial information entirely, making it impossible to\\nanalyze facial behavior. Recent advancements on whole-body keypoints detection\\nalso rely on facial input to estimate body keypoints. Both facial and body\\nkeypoints are critical in some medical diagnoses, and keypoints invariability\\nafter de-identification is of great importance. Here, we propose a solution\\nusing deepfake technology, the face swapping technique. While this swapping\\nmethod has been criticized for invading privacy and portraiture right, it could\\nconversely protect privacy in medical video: patients' faces could be swapped\\nto a proper target face and become unrecognizable. However, it remained an open\\nquestion that to what extent the swapping de-identification method could affect\\nthe automatic detection of body keypoints. In this study, we apply deepfake\\ntechnology to Parkinson's disease examination videos to de-identify subjects,\\nand quantitatively show that: face-swapping as a de-identification approach is\\nreliable, and it keeps the keypoints almost invariant, significantly better\\nthan traditional methods. This study proposes a pipeline for video\\nde-identification and keypoint preservation, clearing up some ethical\\nrestrictions for medical data sharing. This work could make open-source high\\nquality medical video datasets more feasible and promote future medical\\nresearch that benefits our society.\"}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Bingquan Zhu'}, {'name': 'Hao Fang'}, {'name': 'Yanan Sui'}, {'name': 'Luming Li'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Luming Li'}\n",
      "\n",
      "\n",
      "author\n",
      "Luming Li\n",
      "\n",
      "\n",
      "arxiv_doi\n",
      "10.1145/3375627.3375849\n",
      "\n",
      "\n",
      "links\n",
      "[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3375627.3375849', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2003.00813v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2003.00813v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted for publication at the AAAI/ACM Conference on Artificial\n",
      "  Intelligence, Ethics, and Society (AIES) 2020\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2001.09598v4\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2001.09598v4\n",
      "\n",
      "\n",
      "updated\n",
      "2021-11-23T05:54:48Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2021, tm_mon=11, tm_mday=23, tm_hour=5, tm_min=54, tm_sec=48, tm_wday=1, tm_yday=327, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-01-27T06:15:01Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=1, tm_mday=27, tm_hour=6, tm_min=15, tm_sec=1, tm_wday=0, tm_yday=27, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FakeLocator: Robust Localization of GAN-Based Face Manipulations\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'FakeLocator: Robust Localization of GAN-Based Face Manipulations'}\n",
      "\n",
      "\n",
      "summary\n",
      "Full face synthesis and partial face manipulation by virtue of the generative\n",
      "adversarial networks (GANs) and its variants have raised wide public concerns.\n",
      "In the multi-media forensics area, detecting and ultimately locating the image\n",
      "forgery has become an imperative task. In this work, we investigate the\n",
      "architecture of existing GAN-based face manipulation methods and observe that\n",
      "the imperfection of upsampling methods therewithin could be served as an\n",
      "important asset for GAN-synthesized fake image detection and forgery\n",
      "localization. Based on this basic observation, we have proposed a novel\n",
      "approach, termed FakeLocator, to obtain high localization accuracy, at full\n",
      "resolution, on manipulated facial images. To the best of our knowledge, this is\n",
      "the very first attempt to solve the GAN-based fake localization problem with a\n",
      "gray-scale fakeness map that preserves more information of fake regions. To\n",
      "improve the universality of FakeLocator across multifarious facial attributes,\n",
      "we introduce an attention mechanism to guide the training of the model. To\n",
      "improve the universality of FakeLocator across different DeepFake methods, we\n",
      "propose partial data augmentation and single sample clustering on the training\n",
      "images. Experimental results on popular FaceForensics++, DFFD datasets and\n",
      "seven different state-of-the-art GAN-based face generation methods have shown\n",
      "the effectiveness of our method. Compared with the baselines, our method\n",
      "performs better on various metrics. Moreover, the proposed method is robust\n",
      "against various real-world facial image degradations such as JPEG compression,\n",
      "low-resolution, noise, and blur.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Full face synthesis and partial face manipulation by virtue of the generative\\nadversarial networks (GANs) and its variants have raised wide public concerns.\\nIn the multi-media forensics area, detecting and ultimately locating the image\\nforgery has become an imperative task. In this work, we investigate the\\narchitecture of existing GAN-based face manipulation methods and observe that\\nthe imperfection of upsampling methods therewithin could be served as an\\nimportant asset for GAN-synthesized fake image detection and forgery\\nlocalization. Based on this basic observation, we have proposed a novel\\napproach, termed FakeLocator, to obtain high localization accuracy, at full\\nresolution, on manipulated facial images. To the best of our knowledge, this is\\nthe very first attempt to solve the GAN-based fake localization problem with a\\ngray-scale fakeness map that preserves more information of fake regions. To\\nimprove the universality of FakeLocator across multifarious facial attributes,\\nwe introduce an attention mechanism to guide the training of the model. To\\nimprove the universality of FakeLocator across different DeepFake methods, we\\npropose partial data augmentation and single sample clustering on the training\\nimages. Experimental results on popular FaceForensics++, DFFD datasets and\\nseven different state-of-the-art GAN-based face generation methods have shown\\nthe effectiveness of our method. Compared with the baselines, our method\\nperforms better on various metrics. Moreover, the proposed method is robust\\nagainst various real-world facial image degradations such as JPEG compression,\\nlow-resolution, noise, and blur.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Yihao Huang'}, {'name': 'Felix Juefei-Xu'}, {'name': 'Qing Guo'}, {'name': 'Yang Liu'}, {'name': 'Geguang Pu'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Geguang Pu'}\n",
      "\n",
      "\n",
      "author\n",
      "Geguang Pu\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "16 pages, accepted to IEEE Transactions on Information Forensics and\n",
      "  Security\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2001.09598v4', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2001.09598v4', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2001.07444v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2001.07444v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-01-21T11:03:50Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=1, tm_mday=21, tm_hour=11, tm_min=3, tm_sec=50, tm_wday=1, tm_yday=21, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-01-21T11:03:50Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=1, tm_mday=21, tm_hour=11, tm_min=3, tm_sec=50, tm_wday=1, tm_yday=21, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Detecting Face2Face Facial Reenactment in Videos\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Detecting Face2Face Facial Reenactment in Videos'}\n",
      "\n",
      "\n",
      "summary\n",
      "Visual content has become the primary source of information, as evident in\n",
      "the billions of images and videos, shared and uploaded on the Internet every\n",
      "single day. This has led to an increase in alterations in images and videos to\n",
      "make them more informative and eye-catching for the viewers worldwide. Some of\n",
      "these alterations are simple, like copy-move, and are easily detectable, while\n",
      "other sophisticated alterations like reenactment based DeepFakes are hard to\n",
      "detect. Reenactment alterations allow the source to change the target\n",
      "expressions and create photo-realistic images and videos. While technology can\n",
      "be potentially used for several applications, the malicious usage of automatic\n",
      "reenactment has a very large social implication. It is therefore important to\n",
      "develop detection techniques to distinguish real images and videos with the\n",
      "altered ones. This research proposes a learning-based algorithm for detecting\n",
      "reenactment based alterations. The proposed algorithm uses a multi-stream\n",
      "network that learns regional artifacts and provides a robust performance at\n",
      "various compression levels. We also propose a loss function for the balanced\n",
      "learning of the streams for the proposed network. The performance is evaluated\n",
      "on the publicly available FaceForensics dataset. The results show\n",
      "state-of-the-art classification accuracy of 99.96%, 99.10%, and 91.20% for no,\n",
      "easy, and hard compression factors, respectively.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Visual content has become the primary source of information, as evident in\\nthe billions of images and videos, shared and uploaded on the Internet every\\nsingle day. This has led to an increase in alterations in images and videos to\\nmake them more informative and eye-catching for the viewers worldwide. Some of\\nthese alterations are simple, like copy-move, and are easily detectable, while\\nother sophisticated alterations like reenactment based DeepFakes are hard to\\ndetect. Reenactment alterations allow the source to change the target\\nexpressions and create photo-realistic images and videos. While technology can\\nbe potentially used for several applications, the malicious usage of automatic\\nreenactment has a very large social implication. It is therefore important to\\ndevelop detection techniques to distinguish real images and videos with the\\naltered ones. This research proposes a learning-based algorithm for detecting\\nreenactment based alterations. The proposed algorithm uses a multi-stream\\nnetwork that learns regional artifacts and provides a robust performance at\\nvarious compression levels. We also propose a loss function for the balanced\\nlearning of the streams for the proposed network. The performance is evaluated\\non the publicly available FaceForensics dataset. The results show\\nstate-of-the-art classification accuracy of 99.96%, 99.10%, and 91.20% for no,\\neasy, and hard compression factors, respectively.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Prabhat Kumar'}, {'name': 'Mayank Vatsa'}, {'name': 'Richa Singh'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Richa Singh'}\n",
      "\n",
      "\n",
      "author\n",
      "Richa Singh\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "9 pages\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2001.07444v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2001.07444v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2001.06564v1\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2001.06564v1\n",
      "\n",
      "\n",
      "updated\n",
      "2020-01-18T00:13:32Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=1, tm_mday=18, tm_hour=0, tm_min=13, tm_sec=32, tm_wday=5, tm_yday=18, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-01-18T00:13:32Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=1, tm_mday=18, tm_hour=0, tm_min=13, tm_sec=32, tm_wday=5, tm_yday=18, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Media Forensics and DeepFakes: an overview\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Media Forensics and DeepFakes: an overview'}\n",
      "\n",
      "\n",
      "summary\n",
      "With the rapid progress of recent years, techniques that generate and\n",
      "manipulate multimedia content can now guarantee a very advanced level of\n",
      "realism. The boundary between real and synthetic media has become very thin. On\n",
      "the one hand, this opens the door to a series of exciting applications in\n",
      "different fields such as creative arts, advertising, film production, video\n",
      "games. On the other hand, it poses enormous security threats. Software packages\n",
      "freely available on the web allow any individual, without special skills, to\n",
      "create very realistic fake images and videos. So-called deepfakes can be used\n",
      "to manipulate public opinion during elections, commit fraud, discredit or\n",
      "blackmail people. Potential abuses are limited only by human imagination.\n",
      "Therefore, there is an urgent need for automated tools capable of detecting\n",
      "false multimedia content and avoiding the spread of dangerous false\n",
      "information. This review paper aims to present an analysis of the methods for\n",
      "visual media integrity verification, that is, the detection of manipulated\n",
      "images and videos. Special emphasis will be placed on the emerging phenomenon\n",
      "of deepfakes and, from the point of view of the forensic analyst, on modern\n",
      "data-driven forensic methods. The analysis will help to highlight the limits of\n",
      "current forensic tools, the most relevant issues, the upcoming challenges, and\n",
      "suggest future directions for research.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'With the rapid progress of recent years, techniques that generate and\\nmanipulate multimedia content can now guarantee a very advanced level of\\nrealism. The boundary between real and synthetic media has become very thin. On\\nthe one hand, this opens the door to a series of exciting applications in\\ndifferent fields such as creative arts, advertising, film production, video\\ngames. On the other hand, it poses enormous security threats. Software packages\\nfreely available on the web allow any individual, without special skills, to\\ncreate very realistic fake images and videos. So-called deepfakes can be used\\nto manipulate public opinion during elections, commit fraud, discredit or\\nblackmail people. Potential abuses are limited only by human imagination.\\nTherefore, there is an urgent need for automated tools capable of detecting\\nfalse multimedia content and avoiding the spread of dangerous false\\ninformation. This review paper aims to present an analysis of the methods for\\nvisual media integrity verification, that is, the detection of manipulated\\nimages and videos. Special emphasis will be placed on the emerging phenomenon\\nof deepfakes and, from the point of view of the forensic analyst, on modern\\ndata-driven forensic methods. The analysis will help to highlight the limits of\\ncurrent forensic tools, the most relevant issues, the upcoming challenges, and\\nsuggest future directions for research.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Luisa Verdoliva'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Luisa Verdoliva'}\n",
      "\n",
      "\n",
      "author\n",
      "Luisa Verdoliva\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2001.06564v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2001.06564v1', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2001.05574v5\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2001.05574v5\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-26T23:19:21Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=26, tm_hour=23, tm_min=19, tm_sec=21, tm_wday=2, tm_yday=239, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-01-13T08:11:27Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=1, tm_mday=13, tm_hour=8, tm_min=11, tm_sec=27, tm_wday=0, tm_yday=13, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Advbox: a toolbox to generate adversarial examples that fool neural\n",
      "  networks\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Advbox: a toolbox to generate adversarial examples that fool neural\\n  networks'}\n",
      "\n",
      "\n",
      "summary\n",
      "In recent years, neural networks have been extensively deployed for computer\n",
      "vision tasks, particularly visual classification problems, where new algorithms\n",
      "reported to achieve or even surpass the human performance. Recent studies have\n",
      "shown that they are all vulnerable to the attack of adversarial examples. Small\n",
      "and often imperceptible perturbations to the input images are sufficient to\n",
      "fool the most powerful neural networks. \\emph{Advbox} is a toolbox to generate\n",
      "adversarial examples that fool neural networks in PaddlePaddle, PyTorch,\n",
      "Caffe2, MxNet, Keras, TensorFlow and it can benchmark the robustness of machine\n",
      "learning models. Compared to previous work, our platform supports black box\n",
      "attacks on Machine-Learning-as-a-service, as well as more attack scenarios,\n",
      "such as Face Recognition Attack, Stealth T-shirt, and DeepFake Face Detect. The\n",
      "code is licensed under the Apache 2.0 and is openly available at\n",
      "https://github.com/advboxes/AdvBox. Advbox now supports Python 3.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In recent years, neural networks have been extensively deployed for computer\\nvision tasks, particularly visual classification problems, where new algorithms\\nreported to achieve or even surpass the human performance. Recent studies have\\nshown that they are all vulnerable to the attack of adversarial examples. Small\\nand often imperceptible perturbations to the input images are sufficient to\\nfool the most powerful neural networks. \\\\emph{Advbox} is a toolbox to generate\\nadversarial examples that fool neural networks in PaddlePaddle, PyTorch,\\nCaffe2, MxNet, Keras, TensorFlow and it can benchmark the robustness of machine\\nlearning models. Compared to previous work, our platform supports black box\\nattacks on Machine-Learning-as-a-service, as well as more attack scenarios,\\nsuch as Face Recognition Attack, Stealth T-shirt, and DeepFake Face Detect. The\\ncode is licensed under the Apache 2.0 and is openly available at\\nhttps://github.com/advboxes/AdvBox. Advbox now supports Python 3.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Dou Goodman'}, {'name': 'Hao Xin'}, {'name': 'Wang Yang'}, {'name': 'Wu Yuesheng'}, {'name': 'Xiong Junfeng'}, {'name': 'Zhang Huan'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Zhang Huan'}\n",
      "\n",
      "\n",
      "author\n",
      "Zhang Huan\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2001.05574v5', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2001.05574v5', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2001.01265v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2001.01265v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-08-10T06:08:29Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=10, tm_hour=6, tm_min=8, tm_sec=29, tm_wday=0, tm_yday=223, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-01-05T16:04:17Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=1, tm_mday=5, tm_hour=16, tm_min=4, tm_sec=17, tm_wday=6, tm_yday=5, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "FDFtNet: Facing Off Fake Images using Fake Detection Fine-tuning Network\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'FDFtNet: Facing Off Fake Images using Fake Detection Fine-tuning Network'}\n",
      "\n",
      "\n",
      "summary\n",
      "Creating fake images and videos such as \"Deepfake\" has become much easier\n",
      "these days due to the advancement in Generative Adversarial Networks (GANs).\n",
      "Moreover, recent research such as the few-shot learning can create highly\n",
      "realistic personalized fake images with only a few images. Therefore, the\n",
      "threat of Deepfake to be used for a variety of malicious intents such as\n",
      "propagating fake images and videos becomes prevalent. And detecting these\n",
      "machine-generated fake images has been quite challenging than ever. In this\n",
      "work, we propose a light-weight robust fine-tuning neural network-based\n",
      "classifier architecture called Fake Detection Fine-tuning Network (FDFtNet),\n",
      "which is capable of detecting many of the new fake face image generation\n",
      "models, and can be easily combined with existing image classification networks\n",
      "and finetuned on a few datasets. In contrast to many existing methods, our\n",
      "approach aims to reuse popular pre-trained models with only a few images for\n",
      "fine-tuning to effectively detect fake images. The core of our approach is to\n",
      "introduce an image-based self-attention module called Fine-Tune Transformer\n",
      "that uses only the attention module and the down-sampling layer. This module is\n",
      "added to the pre-trained model and fine-tuned on a few data to search for new\n",
      "sets of feature space to detect fake images. We experiment with our FDFtNet on\n",
      "the GANsbased dataset (Progressive Growing GAN) and Deepfake-based dataset\n",
      "(Deepfake and Face2Face) with a small input image resolution of 64x64 that\n",
      "complicates detection. Our FDFtNet achieves an overall accuracy of 90.29% in\n",
      "detecting fake images generated from the GANs-based dataset, outperforming the\n",
      "state-of-the-art.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Creating fake images and videos such as \"Deepfake\" has become much easier\\nthese days due to the advancement in Generative Adversarial Networks (GANs).\\nMoreover, recent research such as the few-shot learning can create highly\\nrealistic personalized fake images with only a few images. Therefore, the\\nthreat of Deepfake to be used for a variety of malicious intents such as\\npropagating fake images and videos becomes prevalent. And detecting these\\nmachine-generated fake images has been quite challenging than ever. In this\\nwork, we propose a light-weight robust fine-tuning neural network-based\\nclassifier architecture called Fake Detection Fine-tuning Network (FDFtNet),\\nwhich is capable of detecting many of the new fake face image generation\\nmodels, and can be easily combined with existing image classification networks\\nand finetuned on a few datasets. In contrast to many existing methods, our\\napproach aims to reuse popular pre-trained models with only a few images for\\nfine-tuning to effectively detect fake images. The core of our approach is to\\nintroduce an image-based self-attention module called Fine-Tune Transformer\\nthat uses only the attention module and the down-sampling layer. This module is\\nadded to the pre-trained model and fine-tuned on a few data to search for new\\nsets of feature space to detect fake images. We experiment with our FDFtNet on\\nthe GANsbased dataset (Progressive Growing GAN) and Deepfake-based dataset\\n(Deepfake and Face2Face) with a small input image resolution of 64x64 that\\ncomplicates detection. Our FDFtNet achieves an overall accuracy of 90.29% in\\ndetecting fake images generated from the GANs-based dataset, outperforming the\\nstate-of-the-art.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Hyeonseong Jeon'}, {'name': 'Youngoh Bang'}, {'name': 'Simon S. Woo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Simon S. Woo'}\n",
      "\n",
      "\n",
      "author\n",
      "Simon S. Woo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "IFIP-Sec 2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2001.01265v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2001.01265v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/2001.00179v3\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/2001.00179v3\n",
      "\n",
      "\n",
      "updated\n",
      "2020-06-18T18:17:43Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=6, tm_mday=18, tm_hour=18, tm_min=17, tm_sec=43, tm_wday=3, tm_yday=170, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2020-01-01T09:54:34Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=1, tm_mday=1, tm_hour=9, tm_min=54, tm_sec=34, tm_wday=2, tm_yday=1, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "The free access to large-scale public databases, together with the fast\n",
      "progress of deep learning techniques, in particular Generative Adversarial\n",
      "Networks, have led to the generation of very realistic fake content with its\n",
      "corresponding implications towards society in this era of fake news. This\n",
      "survey provides a thorough review of techniques for manipulating face images\n",
      "including DeepFake methods, and methods to detect such manipulations. In\n",
      "particular, four types of facial manipulation are reviewed: i) entire face\n",
      "synthesis, ii) identity swap (DeepFakes), iii) attribute manipulation, and iv)\n",
      "expression swap. For each manipulation group, we provide details regarding\n",
      "manipulation techniques, existing public databases, and key benchmarks for\n",
      "technology evaluation of fake detection methods, including a summary of results\n",
      "from those evaluations. Among all the aspects discussed in the survey, we pay\n",
      "special attention to the latest generation of DeepFakes, highlighting its\n",
      "improvements and challenges for fake detection.\n",
      "  In addition to the survey information, we also discuss open issues and future\n",
      "trends that should be considered to advance in the field.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'The free access to large-scale public databases, together with the fast\\nprogress of deep learning techniques, in particular Generative Adversarial\\nNetworks, have led to the generation of very realistic fake content with its\\ncorresponding implications towards society in this era of fake news. This\\nsurvey provides a thorough review of techniques for manipulating face images\\nincluding DeepFake methods, and methods to detect such manipulations. In\\nparticular, four types of facial manipulation are reviewed: i) entire face\\nsynthesis, ii) identity swap (DeepFakes), iii) attribute manipulation, and iv)\\nexpression swap. For each manipulation group, we provide details regarding\\nmanipulation techniques, existing public databases, and key benchmarks for\\ntechnology evaluation of fake detection methods, including a summary of results\\nfrom those evaluations. Among all the aspects discussed in the survey, we pay\\nspecial attention to the latest generation of DeepFakes, highlighting its\\nimprovements and challenges for fake detection.\\n  In addition to the survey information, we also discuss open issues and future\\ntrends that should be considered to advance in the field.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Ruben Tolosana'}, {'name': 'Ruben Vera-Rodriguez'}, {'name': 'Julian Fierrez'}, {'name': 'Aythami Morales'}, {'name': 'Javier Ortega-Garcia'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Javier Ortega-Garcia'}\n",
      "\n",
      "\n",
      "author\n",
      "Javier Ortega-Garcia\n",
      "\n",
      "\n",
      "arxiv_journal_ref\n",
      "Information Fusion, 2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/2001.00179v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2001.00179v3', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/1912.13458v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/1912.13458v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-04-19T02:22:40Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=19, tm_hour=2, tm_min=22, tm_sec=40, tm_wday=6, tm_yday=110, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2019-12-31T17:57:56Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2019, tm_mon=12, tm_mday=31, tm_hour=17, tm_min=57, tm_sec=56, tm_wday=1, tm_yday=365, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "Face X-ray for More General Face Forgery Detection\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'Face X-ray for More General Face Forgery Detection'}\n",
      "\n",
      "\n",
      "summary\n",
      "In this paper we propose a novel image representation called face X-ray for\n",
      "detecting forgery in face images. The face X-ray of an input face image is a\n",
      "greyscale image that reveals whether the input image can be decomposed into the\n",
      "blending of two images from different sources. It does so by showing the\n",
      "blending boundary for a forged image and the absence of blending for a real\n",
      "image. We observe that most existing face manipulation methods share a common\n",
      "step: blending the altered face into an existing background image. For this\n",
      "reason, face X-ray provides an effective way for detecting forgery generated by\n",
      "most existing face manipulation algorithms. Face X-ray is general in the sense\n",
      "that it only assumes the existence of a blending step and does not rely on any\n",
      "knowledge of the artifacts associated with a specific face manipulation\n",
      "technique. Indeed, the algorithm for computing face X-ray can be trained\n",
      "without fake images generated by any of the state-of-the-art face manipulation\n",
      "methods. Extensive experiments show that face X-ray remains effective when\n",
      "applied to forgery generated by unseen face manipulation techniques, while most\n",
      "existing face forgery detection or deepfake detection algorithms experience a\n",
      "significant performance drop.\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In this paper we propose a novel image representation called face X-ray for\\ndetecting forgery in face images. The face X-ray of an input face image is a\\ngreyscale image that reveals whether the input image can be decomposed into the\\nblending of two images from different sources. It does so by showing the\\nblending boundary for a forged image and the absence of blending for a real\\nimage. We observe that most existing face manipulation methods share a common\\nstep: blending the altered face into an existing background image. For this\\nreason, face X-ray provides an effective way for detecting forgery generated by\\nmost existing face manipulation algorithms. Face X-ray is general in the sense\\nthat it only assumes the existence of a blending step and does not rely on any\\nknowledge of the artifacts associated with a specific face manipulation\\ntechnique. Indeed, the algorithm for computing face X-ray can be trained\\nwithout fake images generated by any of the state-of-the-art face manipulation\\nmethods. Extensive experiments show that face X-ray remains effective when\\napplied to forgery generated by unseen face manipulation techniques, while most\\nexisting face forgery detection or deepfake detection algorithms experience a\\nsignificant performance drop.'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Lingzhi Li'}, {'name': 'Jianmin Bao'}, {'name': 'Ting Zhang'}, {'name': 'Hao Yang'}, {'name': 'Dong Chen'}, {'name': 'Fang Wen'}, {'name': 'Baining Guo'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Baining Guo'}\n",
      "\n",
      "\n",
      "author\n",
      "Baining Guo\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to CVPR 2020 (Oral)\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/1912.13458v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1912.13458v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n",
      "id\n",
      "http://arxiv.org/abs/1912.11035v2\n",
      "\n",
      "\n",
      "guidislink\n",
      "True\n",
      "\n",
      "\n",
      "link\n",
      "http://arxiv.org/abs/1912.11035v2\n",
      "\n",
      "\n",
      "updated\n",
      "2020-04-04T12:58:16Z\n",
      "\n",
      "\n",
      "updated_parsed\n",
      "time.struct_time(tm_year=2020, tm_mon=4, tm_mday=4, tm_hour=12, tm_min=58, tm_sec=16, tm_wday=5, tm_yday=95, tm_isdst=0)\n",
      "\n",
      "\n",
      "published\n",
      "2019-12-23T18:58:58Z\n",
      "\n",
      "\n",
      "published_parsed\n",
      "time.struct_time(tm_year=2019, tm_mon=12, tm_mday=23, tm_hour=18, tm_min=58, tm_sec=58, tm_wday=0, tm_yday=357, tm_isdst=0)\n",
      "\n",
      "\n",
      "title\n",
      "CNN-generated images are surprisingly easy to spot... for now\n",
      "\n",
      "\n",
      "title_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'CNN-generated images are surprisingly easy to spot... for now'}\n",
      "\n",
      "\n",
      "summary\n",
      "In this work we ask whether it is possible to create a \"universal\" detector\n",
      "for telling apart real images from these generated by a CNN, regardless of\n",
      "architecture or dataset used. To test this, we collect a dataset consisting of\n",
      "fake images generated by 11 different CNN-based image generator models, chosen\n",
      "to span the space of commonly used architectures today (ProGAN, StyleGAN,\n",
      "BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks,\n",
      "implicit maximum likelihood estimation, second-order attention\n",
      "super-resolution, seeing-in-the-dark). We demonstrate that, with careful pre-\n",
      "and post-processing and data augmentation, a standard image classifier trained\n",
      "on only one specific CNN generator (ProGAN) is able to generalize surprisingly\n",
      "well to unseen architectures, datasets, and training methods (including the\n",
      "just released StyleGAN2). Our findings suggest the intriguing possibility that\n",
      "today's CNN-generated images share some common systematic flaws, preventing\n",
      "them from achieving realistic image synthesis. Code and pre-trained networks\n",
      "are available at https://peterwang512.github.io/CNNDetection/ .\n",
      "\n",
      "\n",
      "summary_detail\n",
      "{'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=deepfake&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100', 'value': 'In this work we ask whether it is possible to create a \"universal\" detector\\nfor telling apart real images from these generated by a CNN, regardless of\\narchitecture or dataset used. To test this, we collect a dataset consisting of\\nfake images generated by 11 different CNN-based image generator models, chosen\\nto span the space of commonly used architectures today (ProGAN, StyleGAN,\\nBigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks,\\nimplicit maximum likelihood estimation, second-order attention\\nsuper-resolution, seeing-in-the-dark). We demonstrate that, with careful pre-\\nand post-processing and data augmentation, a standard image classifier trained\\non only one specific CNN generator (ProGAN) is able to generalize surprisingly\\nwell to unseen architectures, datasets, and training methods (including the\\njust released StyleGAN2). Our findings suggest the intriguing possibility that\\ntoday\\'s CNN-generated images share some common systematic flaws, preventing\\nthem from achieving realistic image synthesis. Code and pre-trained networks\\nare available at https://peterwang512.github.io/CNNDetection/ .'}\n",
      "\n",
      "\n",
      "authors\n",
      "[{'name': 'Sheng-Yu Wang'}, {'name': 'Oliver Wang'}, {'name': 'Richard Zhang'}, {'name': 'Andrew Owens'}, {'name': 'Alexei A. Efros'}]\n",
      "\n",
      "\n",
      "author_detail\n",
      "{'name': 'Alexei A. Efros'}\n",
      "\n",
      "\n",
      "author\n",
      "Alexei A. Efros\n",
      "\n",
      "\n",
      "arxiv_comment\n",
      "Accepted to CVPR 2020\n",
      "\n",
      "\n",
      "links\n",
      "[{'href': 'http://arxiv.org/abs/1912.11035v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1912.11035v2', 'rel': 'related', 'type': 'application/pdf'}]\n",
      "\n",
      "\n",
      "arxiv_primary_category\n",
      "{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}\n",
      "\n",
      "\n",
      "tags\n",
      "[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get all data, not recommended\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m search\u001b[38;5;241m.\u001b[39mresults():     \n\u001b[1;32m      4\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_raw\u001b[38;5;241m.\u001b[39mitems(): \u001b[38;5;66;03m#__dict__.items(): ### this is usual way, but here dict has raw\u001b[39;00m\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28mprint\u001b[39m(key)\n",
      "File \u001b[0;32m~/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/arxiv/arxiv.py:585\u001b[0m, in \u001b[0;36mClient.results\u001b[0;34m(self, search)\u001b[0m\n\u001b[1;32m    580\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m results at offset \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    581\u001b[0m     page_size,\n\u001b[1;32m    582\u001b[0m     offset,\n\u001b[1;32m    583\u001b[0m ))\n\u001b[1;32m    584\u001b[0m page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_url(search, offset, page_size)\n\u001b[0;32m--> 585\u001b[0m feed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_page:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m# NOTE: this is an ugly fix for a known bug. The totalresults\u001b[39;00m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;66;03m# value is set to 1 for results with zero entries. If that API\u001b[39;00m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;66;03m# bug is fixed, we can remove this conditional and always set\u001b[39;00m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# `total_results = min(...)`.\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feed\u001b[38;5;241m.\u001b[39mentries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/arxiv/arxiv.py:639\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03mFetches the specified URL and parses it with feedparser.\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03mIf a request fails or is unexpectedly empty, retries the request up to\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m`self.num_retries` times.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# Invoke the recursive helper with initial available retries.\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_left\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_retries\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/arxiv/arxiv.py:672\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[0;34m(self, url, first_page, retries_left, last_err)\u001b[0m\n\u001b[1;32m    665\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(to_sleep)\n\u001b[1;32m    666\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page of results\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m: url,\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_page\u001b[39m\u001b[38;5;124m'\u001b[39m: first_page,\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m'\u001b[39m: retry,\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_err\u001b[39m\u001b[38;5;124m'\u001b[39m: last_err\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;28;01mif\u001b[39;00m last_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    671\u001b[0m })\n\u001b[0;32m--> 672\u001b[0m feed \u001b[38;5;241m=\u001b[39m \u001b[43mfeedparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    674\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/feedparser/api.py:216\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, response_headers, resolve_relative_uris, sanitize_html)\u001b[0m\n\u001b[1;32m    208\u001b[0m result \u001b[38;5;241m=\u001b[39m FeedParserDict(\n\u001b[1;32m    209\u001b[0m     bozo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    210\u001b[0m     entries\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    211\u001b[0m     feed\u001b[38;5;241m=\u001b[39mFeedParserDict(),\n\u001b[1;32m    212\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_open_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_file_stream_or_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandlers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    218\u001b[0m     result\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbozo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbozo_exception\u001b[39m\u001b[38;5;124m'\u001b[39m: error,\n\u001b[1;32m    221\u001b[0m     })\n",
      "File \u001b[0;32m~/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/feedparser/api.py:115\u001b[0m, in \u001b[0;36m_open_resource\u001b[0;34m(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m url_file_stream_or_string\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(url_file_stream_or_string, \u001b[38;5;28mstr\u001b[39m) \\\n\u001b[1;32m    114\u001b[0m    \u001b[38;5;129;01mand\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlparse(url_file_stream_or_string)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mftp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeed\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_file_stream_or_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandlers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# try to open with native open function (if url_file_stream_or_string is a filename)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/feedparser/http.py:171\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, etag, modified, agent, referrer, handlers, request_headers, result)\u001b[0m\n\u001b[1;32m    169\u001b[0m opener \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbuild_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(handlers \u001b[38;5;241m+\u001b[39m [_FeedURLHandler()]))\n\u001b[1;32m    170\u001b[0m opener\u001b[38;5;241m.\u001b[39maddheaders \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# RMK - must clear so we only send our custom User-Agent\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    173\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.9/urllib/request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    516\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 517\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    520\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.9/urllib/request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    533\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 534\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    535\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.9/urllib/request.py:1375\u001b[0m, in \u001b[0;36mHTTPHandler.http_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/urllib/request.py:1350\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m-> 1350\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1352\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:1347\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1347\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 268\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get all data, not recommended\n",
    "\n",
    "for result in search.results():     \n",
    "      for key, value in result._raw.items(): #__dict__.items(): ### this is usual way, but here dict has raw\n",
    "            print(key)\n",
    "            print(value)\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to \n",
    "\n",
    "# overwrite 'w' or append 'a'\n",
    "action = 'a'\n",
    "\n",
    "#save as \n",
    "save_as = 'arxiv_' + QUERY\n",
    "\n",
    "for result in search.results():\n",
    "      with open (OUTPUT_PATH + save_as + '.jsonl', action) as f:\n",
    "            json.dump(result._raw, f, default=str) # use raw as __dict__ has raw in it, thus more data\n",
    "            f.write('\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client search (for larger searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n",
      "Sweep done\n"
     ]
    }
   ],
   "source": [
    "friendly_client = arxiv.Client(\n",
    "  page_size = 1000, # 2000 max\n",
    "  delay_seconds = 10, #3 min\n",
    "  num_retries = 5\n",
    ")\n",
    "\n",
    "action = 'a'\n",
    "\n",
    "#save as \n",
    "save_as = 'arxiv_' + QUERY\n",
    "\n",
    "# Prints 1000 titles before needing to make another request.\n",
    "for result in friendly_client.results(arxiv.Search(query=QUERY, sort_by = arxiv.SortCriterion.SubmittedDate)):\n",
    "      with open (OUTPUT_PATH + save_as + '.jsonl', action) as f:\n",
    "            json.dump(result._raw, f, default=str) # use raw as __dict__ has raw in it, thus more data\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here for loading existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arxiv_disinformation.jsonl', 'arxiv_deepfake.jsonl']\n"
     ]
    }
   ],
   "source": [
    "# Check files in data folder\n",
    "datafiles = [f for f in listdir(OUTPUT_PATH) if isfile(join(OUTPUT_PATH, f))]\n",
    "print(datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df\n",
    "load_file = datafiles[1]\n",
    "\n",
    "df = pd.read_json(OUTPUT_PATH + load_file, convert_dates=True, lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'guidislink', 'link', 'updated', 'updated_parsed', 'published',\n",
       "       'published_parsed', 'title', 'title_detail', 'summary',\n",
       "       'summary_detail', 'authors', 'author_detail', 'author', 'arxiv_comment',\n",
       "       'links', 'arxiv_primary_category', 'tags', 'arxiv_affiliation',\n",
       "       'arxiv_journal_ref', 'arxiv_doi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_detail</th>\n",
       "      <th>author</th>\n",
       "      <th>arxiv_comment</th>\n",
       "      <th>links</th>\n",
       "      <th>arxiv_primary_category</th>\n",
       "      <th>tags</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>http://arxiv.org/abs/1809.00888v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/1809.00888v1</td>\n",
       "      <td>2018-09-04T10:59:22Z</td>\n",
       "      <td>[2018, 9, 4, 10, 59, 22, 1, 247, 0]</td>\n",
       "      <td>2018-09-04T10:59:22Z</td>\n",
       "      <td>[2018, 9, 4, 10, 59, 22, 1, 247, 0]</td>\n",
       "      <td>MesoNet: a Compact Facial Video Forgery Detect...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>This paper presents a method to automatically ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'name': 'Darius Afchar'}, {'name': 'Vincent ...</td>\n",
       "      <td>{'name': 'Isao Echizen'}</td>\n",
       "      <td>Isao Echizen</td>\n",
       "      <td>accepted to WIFS 2018</td>\n",
       "      <td>[{'title': 'doi', 'href': 'http://dx.doi.org/1...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1109/WIFS.2018.8630761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>http://arxiv.org/abs/1806.02877v2</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/1806.02877v2</td>\n",
       "      <td>2018-06-11T19:28:49Z</td>\n",
       "      <td>[2018, 6, 11, 19, 28, 49, 0, 162, 0]</td>\n",
       "      <td>2018-06-07T19:36:09Z</td>\n",
       "      <td>[2018, 6, 7, 19, 36, 9, 3, 158, 0]</td>\n",
       "      <td>In Ictu Oculi: Exposing AI Generated Fake Face...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>The new developments in deep generative networ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'name': 'Yuezun Li'}, {'name': 'Ming-Ching C...</td>\n",
       "      <td>{'name': 'Siwei Lyu'}</td>\n",
       "      <td>Siwei Lyu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'href': 'http://arxiv.org/abs/1806.02877v2',...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  guidislink  \\\n",
       "436  http://arxiv.org/abs/1809.00888v1        True   \n",
       "437  http://arxiv.org/abs/1806.02877v2        True   \n",
       "\n",
       "                                  link               updated  \\\n",
       "436  http://arxiv.org/abs/1809.00888v1  2018-09-04T10:59:22Z   \n",
       "437  http://arxiv.org/abs/1806.02877v2  2018-06-11T19:28:49Z   \n",
       "\n",
       "                           updated_parsed             published  \\\n",
       "436   [2018, 9, 4, 10, 59, 22, 1, 247, 0]  2018-09-04T10:59:22Z   \n",
       "437  [2018, 6, 11, 19, 28, 49, 0, 162, 0]  2018-06-07T19:36:09Z   \n",
       "\n",
       "                        published_parsed  \\\n",
       "436  [2018, 9, 4, 10, 59, 22, 1, 247, 0]   \n",
       "437   [2018, 6, 7, 19, 36, 9, 3, 158, 0]   \n",
       "\n",
       "                                                 title  \\\n",
       "436  MesoNet: a Compact Facial Video Forgery Detect...   \n",
       "437  In Ictu Oculi: Exposing AI Generated Fake Face...   \n",
       "\n",
       "                                          title_detail  \\\n",
       "436  {'type': 'text/plain', 'language': None, 'base...   \n",
       "437  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                               summary  ...  \\\n",
       "436  This paper presents a method to automatically ...  ...   \n",
       "437  The new developments in deep generative networ...  ...   \n",
       "\n",
       "                                               authors  \\\n",
       "436  [{'name': 'Darius Afchar'}, {'name': 'Vincent ...   \n",
       "437  [{'name': 'Yuezun Li'}, {'name': 'Ming-Ching C...   \n",
       "\n",
       "                author_detail        author          arxiv_comment  \\\n",
       "436  {'name': 'Isao Echizen'}  Isao Echizen  accepted to WIFS 2018   \n",
       "437     {'name': 'Siwei Lyu'}     Siwei Lyu                    NaN   \n",
       "\n",
       "                                                 links  \\\n",
       "436  [{'title': 'doi', 'href': 'http://dx.doi.org/1...   \n",
       "437  [{'href': 'http://arxiv.org/abs/1806.02877v2',...   \n",
       "\n",
       "                                arxiv_primary_category  \\\n",
       "436  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "437  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "\n",
       "                                                  tags arxiv_affiliation  \\\n",
       "436  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "437  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "\n",
       "    arxiv_journal_ref                  arxiv_doi  \n",
       "436               NaN  10.1109/WIFS.2018.8630761  \n",
       "437               NaN                        NaN  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da0017f62db7e548d7b812a9c7093f1ca47065612340949c7e49f676fc86bc13"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('dr_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
