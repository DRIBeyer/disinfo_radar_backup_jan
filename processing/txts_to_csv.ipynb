{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "#from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = '../data/'\n",
    "OUTPUT_PATH = '../output_data/'\n",
    "MODEL_PATH = '../data/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "['models', 'raw', 'deepfake_txt', 'deepfakepdfs']\n",
      "[(0, 'models'), (1, 'raw'), (2, 'deepfake_txt'), (3, 'deepfakepdfs')]\n"
     ]
    }
   ],
   "source": [
    "# Check directories in data folder\n",
    "data_dirs = [f for f in listdir(DATA_PATH) if not isfile(join(DATA_PATH, f))]\n",
    "#print(data_dirs)\n",
    "\n",
    "print(list(zip([index for index, value in enumerate(data_dirs)], data_dirs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/deepfake_txt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick which directory to work with\n",
    "selected_dir = data_dirs[2]\n",
    "\n",
    "dir_path = DATA_PATH + selected_dir\n",
    "dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in Path(dir_path).iterdir():\n",
    "\n",
    "      with open(file, \"r\") as file_open:\n",
    "            results[\"file_name\"] = file.name\n",
    "            results[\"text\"].append(file_open.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1806.02877v2.txt</td>\n",
       "      <td>ADAPTIVE FREQUENCY LEARNING IN TWO-BRANCH FACE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_name                                               text\n",
       "0  1806.02877v2.txt  ADAPTIVE FREQUENCY LEARNING IN TWO-BRANCH FACE..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = pd.DataFrame(results)\n",
    "\n",
    "df_x.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A New Approach to Improve Learning-based\\nDeepfake Detection in Realistic Conditions\\nYuhang Lu and Touradj Ebrahimi\\n\\narXiv:2203.11807v1 [cs.CV] 22 Mar 2022\\n\\nMultimedia Signal Processing Group (MMSPG)\\nÉcole Polytechnique Fédérale de Lausanne (EPFL)\\nCH-1015 Lausanne, Switzerland\\nEmail: firstname.lastname@epfl.ch\\n\\nAbstract—Deep convolutional neural networks have achieved\\nexceptional results on multiple detection and recognition tasks.\\nHowever, the performance of such detectors are often evaluated\\nin public benchmarks under constrained and non-realistic situations. The impact of conventional distortions and processing\\noperations found in imaging workflows such as compression,\\nnoise, and enhancement are not sufficiently studied. Currently,\\nonly a few researches have been done to improve the detector\\nrobustness to unseen perturbations. This paper proposes a\\nmore effective data augmentation scheme based on real-world\\nimage degradation process. This novel technique is deployed\\nfor deepfake detection tasks and has been evaluated by a more\\nrealistic assessment framework. Extensive experiments show that\\nthe proposed data augmentation scheme improves generalization\\nability to unpredictable data distortions and unseen datasets.\\nIndex Terms—Data augmentation, Deepfake detection, Generalization\\n\\nI. I NTRODUCTION\\nIn recent years, deep convolutional neural networks\\n(DCNN) trained with large-scale datasets have demonstrated\\nsignificant improvements over most computer vision tasks,\\nsuch as object detection, face recognition, forgery detection,\\netc. However, the performance of the DCNN-based methods\\nare often assessed by specific public benchmarks under constrained situations that do not necessarily match reality. It has\\nbeen shown that a dramatic performance deterioration could\\noccur when facing unseen contents, perturbations or postprocessing operations [1], [2], [3]. In realistic scenarios, image\\ndata can suffer from extrinsic distortions during acquisition,\\nsuch as varying illumination, occlusion, noise, and afterwards\\neven undergo pre- and post-processing operations in storage\\nand delivery. This could be a crucial problem for safety- and\\nsecurity-critical applications, such as surveillance, autonomous\\ndriving, and digital forensics, to mention a few examples.\\nFace manipulation detection is used in many such applications. Deepfakes refer to manipulated face contents by\\ndeep learning tools. The development of such techniques and\\nwide availability of open source software have simplified the\\ncreation of face manipulations, posing serious public concerns.\\nWhile numerous deepfake detection methods have been published and have reported promising results on specific benchThe authors acknowledge support from CHIST-ERA project XAIface\\n(CHIST-ERA-19-XAI-011) with funding from the Swiss National Science\\nFoundation (SNSF) under grant number 20CH21 195532.\\n\\nmarks, they have often been developed and assessed under\\nunrealistic scenarios. A deployed detector could mistakenly\\nblock a pristine yet heavily compressed image. At the same\\ntime, a malicious agent could also fool the detector by simply\\nadding imperceptible noise to fake media contents. Therefore,\\nan effective technique to improve the model robustness to\\nrealistic distortions and processing operations is desired.\\nIn this work the following contributions have been made.\\n• Inspired by real-world data degradation process, a novel\\ndata augmentation scheme is proposed, which improves\\nthe generalization ability of deepfake detectors while at\\nmarginal loss of accuracy on original benchmark.\\n• A new assessment framework is introduced for generic\\nlearning-based detection systems, which rigorously measures the robustness of a detector under more realistic\\nsituations.\\n• Extensive experiments and ablation studies have been\\nconducted to evaluate the effectiveness of the augmentation scheme with the help of the framework.\\nII. R ELATED W ORK\\nRobustness benchmark In recent years, research has been\\nconducted to explore the robustness of CNN-based methods\\ntowards real-world image corruptions. Dodge and Karam [1]\\nmeasured the performance of image classification models with\\ndata disturbed by noise, blurring, and contrast changes. In [3],\\nHendrycks et al. presented a corrupted version of ImageNet\\n[4] to benchmark the robustness of image recognition models\\nagainst common image manipulations. [5], [6], [7] focused\\non a safety-critical task, autonomous driving, and provided\\nrobustness benchmark for various relevant vision tasks, such as\\nobject detection and semantic segmentation. However, current\\nefforts have put more emphasis on corruptions from extrinsic\\nenvironment and often focus on only one specific task. In this\\nwork, we introduce an assessment framework that is suitable\\nfor multiple detection tasks and additionally considers the\\nimpact of realistic processing operations in the real world.\\nFace manipulation detection Face-centric detectors are\\noften treated as a binary classification problem in computer\\nvision. Early on, solutions based on facial expressions [8],\\nhead movements [9] and eye blinking [10] were proposed.\\nCurrent studies leverage deep learning technique to address\\nsuch detection problems. Zhou et al. [11] proposed to detect\\n\\n\\x0cAcquisition\\n\\n•\\n•\\n\\nNoise\\nLow-resolution\\n……\\n\\nPre-Processing\\n\\n•\\n•\\n•\\n•\\n\\nDe-mosaicking\\nResizing\\nDe-noising\\nGamma Correction\\n……\\n\\nCompression\\n\\n•\\n•\\n•\\n\\nPost-Processing\\n\\nJPEG\\nJPEG 2000\\nDL-Compression\\n……\\n\\n•\\n•\\n\\nDe-blocking\\nDe-noising\\n……\\n\\nFig. 1: A realistic data acquisition and transmission pipeline, including various natural image distortions and processing operations.\\n\\nthe deepfakes with a two-stream neural network. In [12],\\nRössler et al. retrained an XceptionNet [13] with manipulated\\nface dataset which outperforms on their proposed benchmark.\\nNguyen et al. [14] combined traditional CNN and Capsule\\nnetworks [15], which requires fewer parameters. Attention\\nmechanisms have also been adopted to further improve the\\ndetection systems [16]. Furthermore, to assist in a faster\\nprogress and better advancement of such tasks, numerous\\nlarge-scale datasets, benchmarks, and competitions [12], [17],\\n[18], [19] have been made publicly available. For instance,\\nFaceForensics++ [12] is one of the most popular face forensics\\ndatasets. Recently, Meta and Microsoft also organized a wellknown Deepfake Detection Challenge based on their released\\ndataset, DFDC [19].\\nData augmentation against corruption Data augmentation\\nis a popular technique in deep learning-based approaches,\\nwhich often includes operations like flipping, cropping, translation, etc. Many studies leverage data augmentation technique to improve the generalization ability and to reduce the\\nnegative impact of natural distortions. Mixup [20] linearly\\ncombined two images and corresponding labels in the training\\nbatch. AugMix [21] utilized stochasticity and cascaded various\\naugmentation operations and achieved the state-of-the-art on\\nImageNet-C [3]. [22], [23] have explored augmenting training\\ndata with Gaussian noise and managed to improve the performance of a object classifier on corrupted images.\\nIII. I MPROVED T RAINING S CHEME BY DATA\\nAUGMENTATION\\nTo reduce the negative impact of realistic distortions and\\npost-processing operations on detection performance, we propose a simple but effective data augmentation approach that\\nleads to a robustness improvement.\\nMany studies have explored using corrupted data as augmentations by adding Gaussian noise or applying compression\\nto training data [22], [23], or transferring the data style [5].\\nDespite that such techniques have proven to enhance the\\nrobustness, they are usually inadequate for more complex\\nreal-world conditions and do not bring desired levels of\\nimprovements.\\nThis work is motivated by a typical data acquisition and\\ntransmission pipeline in real world, as shown in Figure 1.\\nBefore being used for vision tasks, image data often suffers\\nfrom natural distortions during acquisition followed by a set\\nof processing operations. Based on the observation of data\\n\\n(a) Unaltered\\n\\n(b) JPEG\\n\\n(c) GB\\n\\n(d) GN\\n\\n(e) GC\\n\\n(f) GN+GB\\n\\n(g) GB+GN+GC\\n\\n(h) All\\n\\nFig. 2: Example of some typical frames in the training dataset after\\napplying the stochastic data augmentation scheme. Some notations\\nare explained as following. GB: Gaussian blur. GN: Gaussian noise.\\nGC: Gamma correction. +: mixture. All: all the operations cascaded.\\n\\ndegradation process, a carefully designed augmentation chain\\nwas conceived, which produces augmented training data that\\nare much closer to realistic conditions.\\nGenerally, the brightness and contrast of input image x\\nare first modified by image enhancement operator enh. Afterwards, the image is convoluted with an image blurring kernel\\nf , followed by additive Gaussian noise n. At the end, JPEG\\ncompression is applied to obtain the augmented training data\\nxaug . The augmentation chain is described by the following\\nformula.\\nxaug = JPEG[(enh(x) ~ f ) + n]\\n\\n(1)\\n\\nMoreover, directly cascading several augmentation operations could result in a reduction of performance on the\\noriginal benchmark. The proposed augmentation technique is\\nimplemented in a stochastic manner, i.e. each operation occurs\\nwith a certain probability and randomness level, which also\\nbetter reflects the natural data degradation process. Figure 2\\nshows a few samples of typical augmented data.\\nIn detail, the augmentation operations are explained in\\nsequence as follows.\\nEnhancement: The augmentation chain begins with image\\nenhancement operation. A probability of 50% is adopted to\\napply either a brightness or a contrast operation on the training\\ndata which will be then non-linearly modified by a factor\\nrandomly selected from [0.5, 1.5].\\n\\n\\x0cTABLE I: AUC (%) scores of two detectors tested on unaltered and distorted variants of FFpp and Celeb-DFv2 test set respectively. CapsuleForensics detector is shortened as Capsule. The suffixes Raw and Full refer to different quality settings of FFpp. DL-Comp is shortened\\nfrom deep learning-based compression. DnCNN [24] refers to a learning-based denoising approach.\\nJPEG\\nMethods\\n\\nCapsule\\n\\nXceptionNet\\n\\nDL-Comp\\n\\nGau Noise\\n\\nGau Blur\\n\\nGamma Corr\\n\\nResize\\n\\nGau Noise+DnCNN\\n\\nTrainSet\\n\\nUnaltered\\n\\n95\\n\\n60\\n\\n30\\n\\nHigh\\n\\nMed\\n\\nLow\\n\\n5\\n\\n30\\n\\n50\\n\\nPois-Gau\\nNoise\\n\\n3\\n\\n7\\n\\n11\\n\\n0.1\\n\\n0.75\\n\\n2.5\\n\\nx4\\n\\nx8\\n\\nx16\\n\\n10\\n\\n30\\n\\n50\\n\\nFFpp-Raw\\nFFpp-Full\\nFFpp-Raw+Aug\\n\\n99.20\\n94.52\\n98.16\\n\\n97.91\\n94.95\\n97.97\\n\\n76.48\\n93.97\\n96.36\\n\\n59.60\\n84.50\\n94.08\\n\\n44.76\\n86.83\\n93.81\\n\\n45.50\\n60.98\\n71.41\\n\\n49.08\\n55.69\\n59.74\\n\\n61.80\\n89.03\\n97.05\\n\\n51.26\\n57.95\\n83.51\\n\\n49.16\\n51.11\\n75.09\\n\\n55.63\\n64.87\\n90.04\\n\\n67.19\\n85.72\\n96.86\\n\\n41.78\\n58.83\\n90.32\\n\\n47.74\\n56.05\\n80.31\\n\\n49.50\\n56.02\\n60.17\\n\\n98.86\\n93.86\\n97.68\\n\\n96.12\\n85.44\\n96.91\\n\\n67.48\\n87.05\\n93.54\\n\\n47.82\\n69.93\\n79.22\\n\\n46.90\\n54.15\\n58.05\\n\\n75.09\\n86.24\\n90.61\\n\\n66.68\\n80.54\\n84.32\\n\\n60.63\\n73.85\\n77.26\\n\\nCelebDFv2\\nCelebDFv2+Aug\\n\\n99.14\\n98.20\\n\\n99.32\\n98.20\\n\\n98.88\\n97.68\\n\\n93.07\\n96.01\\n\\n-\\n\\n-\\n\\n-\\n\\n95.24\\n96.88\\n\\n59.32\\n83.51\\n\\n60.57\\n74.71\\n\\n-\\n\\n99.01\\n97.53\\n\\n91.04\\n94.57\\n\\n77.52\\n88.72\\n\\n79.6\\n81.84\\n\\n98.52\\n98.02\\n\\n94.62\\n96.85\\n\\n89.22\\n94.04\\n\\n66.98\\n79.91\\n\\n61.94\\n61.51\\n\\n94.98\\n96.00\\n\\n85.70\\n92.12\\n\\n75.50\\n86.19\\n\\nFFpp-Raw\\nFFpp-Raw+Aug\\n\\n99.56\\n98.44\\n\\n76.77\\n98.25\\n\\n56.00\\n97.36\\n\\n54.20\\n96.12\\n\\n50.16\\n98.03\\n\\n50.37\\n87.76\\n\\n50.10\\n82.74\\n\\n50.12\\n97.37\\n\\n49.64\\n91.71\\n\\n49.30\\n88.70\\n\\n48.98\\n94.57\\n\\n68.76\\n98.31\\n\\n55.61\\n97.35\\n\\n50.70\\n94.51\\n\\n54.66\\n80.48\\n\\n98.66\\n98.25\\n\\n70.45\\n97.75\\n\\n68.60\\n97.30\\n\\n55.80\\n86.26\\n\\n50.45\\n67.14\\n\\n63.77\\n94.18\\n\\n57.95\\n90.68\\n\\n55.92\\n86.95\\n\\nCelebDFv2\\nCelebDFv2+Aug\\n\\n99.73\\n99.01\\n\\n99.59\\n99.00\\n\\n99.78\\n98.80\\n\\n97.75\\n98.24\\n\\n-\\n\\n-\\n\\n-\\n\\n94.85\\n98.80\\n\\n52.49\\n95.86\\n\\n52.50\\n92.46\\n\\n-\\n\\n98.77\\n98.84\\n\\n91.81\\n97.18\\n\\n79.94\\n95.03\\n\\n74.98\\n72.53\\n\\n99.53\\n98.89\\n\\n97.49\\n98.11\\n\\n96.01\\n97.60\\n\\n72.20\\n89.08\\n\\n63.03\\n73.59\\n\\n97.81\\n97.80\\n\\n91.44\\n95.83\\n\\n80.29\\n92.44\\n\\nSmoothing: Image blurring operation is then applied with a\\nselected probability of 50%. Either Gaussian blur or Average\\nblur filter is used with a kernel size varying in the range [3, 15].\\nAdditive Gaussian Noise: For each batch of training data, a\\nprobability of 30% is adopted to add a Gaussian noise. The\\nstandard deviation of the Gaussian noise varies randomly in\\nthe interval [0, 50].\\nJPEG Compression : Finally, JPEG compression is applied\\nwith a selected probability of 70%. The quality factor corresponding to the compression is randomly chosen in the the\\nrange [10, 95].\\nIV. I LLUSTRATIVE E XAMPLE F OR D EEPFAKE D ETECTION\\nThe usage of the proposed augmentation strategy is illustrated in the context of deepfake detection , which has become\\na hot topic in digital forensics.\\n1) Detection Methods: Experiments have been conducted\\nwith two deep learning-based deepfake detectors, both of\\nwhich have reported excellent performance on popular benchmarks.\\nCapsule-Forensics [14] achieves high detection accuracy\\nand meanwhile the network maintains a rather small amount\\nof parameters by combining conventional CNN and Capsule\\nnetwork [15].\\nXceptionNet [13] is a popular CNN architecture in many\\ncomputer vision tasks. Ros̈sler et al. first adopted it in [12] to\\ndetect face manipulations and achieved excellent performance\\nin the FaceForensics++ benchmark on both compressed and\\nuncompressed contents.\\n2) Datasets: Two widely used face manipulation datasets\\nare selected for extensive experimentations to demonstrate the\\neffectiveness of the proposed augmentation technique.\\nFaceForensics++ [12], denoted by FFpp, contains 1000\\npristine and 4000 manipulated video generated by four different deepfake creation algorithms. Additionally, raw video\\ncontents are compressed with two quality parameters using\\nthe H.264 codec, dented as C23 and C40. In our experiments,\\ndata of all quality levels are involved for training while only\\nuncompressed data are used for the final assessment.\\nCeleb-DFv2 [17] is another high quality dataset, with 590\\npristine celebrity video and 5639 fake video. The test data\\nis selected as recommended by [17] while the training and\\nvalidation set was split in 80% and 20% accordingly.\\n\\n3) Implementation Details: Both detectors were trained\\nwith Adam optimizer with β1 = 0.9, β2 = 0.999. The\\nCapsule-Forensics model is trained from scratch for 25 epochs\\nwith a learning rate of 5 × 10−4 , and the XceptionNet model\\nis fine tuned on the manipulated face datasets for 10 epochs\\nwith learning rate of 1×10−3 . For the datasets, 100 frames are\\nrandomly sampled from each video for training and 32 frames\\nare extracted for testing. Face regions are detected and cropped\\nusing dlib toolbox [25]. The proposed data augmentation\\ntechnique is then adopted during the training process.\\nV. E XPERIMENT R ESULTS BY R EALISTIC A SSESSMENT\\nF RAMEWORK\\nWe demonstrate the effectiveness of our data augmentation\\nstrategy by evaluating it through a realistic assessment framework. In this section, the proposed assessment framework for\\ngeneric detection and recognition tasks is first described. It\\nprovides a way to broadly benchmark robustness for different\\nmodels towards realistic data distortions and processing operations, and at the same time providing insights on further\\nimprovement. Afterwards, the evaluation results are presented\\nand analyzed in-depth.\\nA. Assessment Framework Details\\nIn the assessment framework, six major categories of processing operations or added distortions are included, which are\\nfurther divided into more than ten minor types.\\nNoise: Noise is a typical natural distortion during image\\nacquisition. The framework adopts zero-mean Gaussian noise\\nwith 6 levels of variance. A synthetic Poissonian-Gaussian\\nnoise [26] is also considered in the framework to better reflect\\nthe realistic situations.\\nResizing: Images captured in outdoor environment often\\nhave limited resolution and can dramatically reduce the performance of learning-based detectors [27], [28]. Thus, the impact\\nof low-resolution images is measured.\\nCompression: Compression is a widely used processing\\noperation before transmission. Both JPEG compression with\\nmultiple ratios and a deep learning-based compression technique developed by Ballé et al. [29] are considered in the\\nframework.\\nSmoothing: Image blurring is another commonly employed\\noperation to reduce noise. The impact of three different filters\\nwith various kernel sizes and a learning-based denoising\\ntechnique [24] is evaluated by the framework.\\n\\n\\x0c80\\n70\\n\\n70\\n\\n60\\n\\n60\\n\\n50\\n\\n50\\n\\n50\\n\\n30\\n\\n0\\n\\n100\\n\\n100\\n\\n90\\n\\n90\\n\\n80\\n\\n80\\n\\n70\\n\\n60\\n\\n50\\n\\n50\\n\\n90\\n\\n80\\n\\n80\\n\\nAUC (%)\\n\\n100\\n\\n1\\nGamma Corr value\\n\\n10\\n\\n70\\n60\\n\\n50\\n0\\n\\n50\\n10\\n20\\n30\\n40\\n50\\n1\\n40Gaussian noise\\n50 variance 60\\n70\\nJPEG Compression rate\\n(Gaussian noise + DnCNN)\\n\\nCapsule-Raw\\n\\nCapsule-Raw+Aug\\n\\n50\\n\\n30\\n\\n0\\n\\nXceptionNet-Raw\\n\\n6\\n11\\n80\\n90\\nDownscale factor\\n(Low Resolutin)\\n\\n16\\n100\\n\\nXceptionNet-Raw+Aug\\n\\nFig. 3: Performance comparison of models that are trained on FFppRaw only and trained with the proposed data augmentation scheme\\n(+Aug). Results of two different detectors are presented.\\n\\nEnhancement: Image enhancement technique is frequently\\nused to adjust image brightness and contrast for better display.\\nWe change the contrast and brightness of images by separately\\napplying linear adjustment and Gamma correction.\\nCombinations: A combination of two or three distortions\\nor operations are considered, such as combining noise and\\ncompression, making the test data to better reflect realistic\\nsituations.\\nTo employ assessment framework, one detector should be\\ntrained on its original target datasets, such as FFpp for deepfake detection tasks. Processing operations and corruptions are\\nonly applied on testing data.\\nB. Experimental Results\\nDuring the evaluation, we utilize Accuracy (ACC), the Area\\nUnder Receiver Operating Characteristic Curve (AUC), and\\nF1-score in all experiments. In this section, only AUC scores\\nand a subset of processing operations and severity levels are\\npresented.\\nThe assessment results with the framework are presented\\nin Table I. The information regarding the models trained with\\nthe proposed augmentation are denoted as +Aug. In general,\\nmost of realistic distortions and processing are exceedingly\\nharmful to normally trained learning-based deepfake detectors.\\nFor instance, Capsule-Forensics method shows very high AUC\\n\\n10\\n20\\n30\\n40\\nGaussian noise variance\\n\\n50\\n\\n100\\n\\n100\\n90\\n90\\n80\\n80\\n\\n90\\n\\n70\\n70\\n\\n80\\n70\\n60\\n\\n10\\n\\n35\\n5 10\\n7 15 9\\nGaussian blur kernel size\\nCapsule-Raw\\n\\n90\\n\\n60\\n\\n70\\n50\\nJPEG Compression ratio\\n\\n100\\n\\n50\\n50\\n\\n100\\n\\n70\\n\\n70\\n\\n60\\n60\\n\\n0.1\\n\\n11\\n\\n50\\n\\n70\\n\\n60\\n\\n3\\n5\\n7\\n9\\nGaussian blur kernel size\\n\\n10\\n20\\n30\\n40\\nGaussian noise variance\\n\\n80\\n\\n60\\n90\\n\\nAUC\\nTitle (%)\\n\\n70\\n50\\nJPEG Compression rate\\n\\nAUC (%)\\n\\nAUC (%)\\n\\n90\\n\\n80\\n\\n60\\n\\n1\\n\\nAUC (%)\\n\\n80\\n\\n100\\n\\n90\\n\\n70\\n\\n90\\n\\n30\\n\\nAUC (%)\\n\\n90\\nAUC (%)\\n\\nAUC (%)\\n\\n90\\n\\n100\\n\\nAUC (%(\\n\\n100\\n\\nAUC (%)\\n\\n100\\n\\n20 11\\n\\nCapsule-Raw+Aug\\n\\n25\\nTitle\\n\\n50\\n\\n0.1 30\\n\\nCapsule-Raw+Aug_nr\\n\\n35\\n45\\n140\\nGamma Corr values\\n\\n50\\n10\\n\\nCapsule-Raw+GN_Aug\\n\\nFig. 4: Frame-level AUC scores on four types of realistically distorted\\ndata after applying different augmentation strategy. Our proposed\\naugmentation scheme is more robust to all corruptions.\\n\\nscores on both uncompressed FFpp and Celeb-DFv2 test set\\nafter training on respective datasets, but then suffers from\\ndrastic performance drop on modified data from our assessment framework. Similar trends have been observed with the\\nXceptionNet detector.\\nIn comparison, it is evident that training with the proposed augmentation technique on the same dataset remarkably\\nimproves the performance on nearly all kinds of processed\\ndata even with intense severity. Figure 3 further illustrates\\nthe impact of increasing the severity of four different types\\nof realistic distortions and processing operations, i.e. JPEG\\ncompression, Gaussian noise, Gaussian blur, and Gamma correction. The data augmentation scheme significantly improves\\nthe robustness of the two detectors and meanwhile they still\\nmaintain high performance on original unaltered data.\\nFurthermore, as an ablation study, the effect of the cascaded\\naugmentation chain as well as its randomized implementation\\nis investigated. The results are presented in Figure 4. We first\\ntrain a Capsule-Forensics model on FFpp dataset augmented\\nby only Gaussian noise, denoted as +GN Aug. It is noticeable\\nthat the model only shows an improvement on noise corrupted\\ndata while still affected by other corruptions. The other model\\nis trained with the same augmentation operations but without\\nstochastic scheme, named +Aug nr. As a result, there is an\\nobvious performance decay on original unaltered benchmark.\\nThe reason for performance difference is that the proposed\\nstochastic-based augmentation scheme can always maintain\\na certain amount of the original and less-distorted data for\\ntraining.\\nFinally, a cross-dataset assessment has been conducted to\\nevaluate the generalization ability of the models trained with\\nthe proposed augmentation scheme on unseen datasets. The\\nresults are shown in Table II. We train the selected detectors on\\nFFpp dataset but test them on Celeb-DFv2 test set for framelevel AUC scores. The two methods both obtain relatively\\nlow scores on the new dataset, although mixing compressed\\n\\n\\x0cTABLE II: Cross-dataset evaluation on Celeb-DFv2 (AUC(%)) after\\ntraining on FFpp dataset. The suffixes denote training set quality and\\nthe augmentation technique.\\nMethod\\n\\nTrainSet\\n\\nFFpp\\n\\nCeleb-DFv2\\n\\nCapsule\\n\\nFFpp-Raw\\nFFpp-C23\\nFFpp-Full\\nFFpp-Raw+Aug\\n\\n99.20\\n96.32\\n94.52\\n97.82\\n\\n54.39\\n59.31\\n68.19\\n71.86\\n\\nXceptionNet\\n\\nFFpp-Raw\\nFFpp-Full\\nFFpp-Raw+Aug\\n\\n99.56\\n99.02\\n98.44\\n\\n50.00\\n64.58\\n73.88\\n\\ndata for training slightly improves the tranferability. The\\nproposed augmentation scheme brings a certain performance\\nimprovement for both detectors on Celeb-DFv2, showing the\\ncapability to detect unseen forensic media contents.\\nVI. C ONCLUSION\\nCurrent detection methods are designed to achieve as high\\nperformance as possible on specific benchmarks. This often\\nresults in sacrificing generalization ability to more realistic\\nscenarios. In this paper, a carefully conceived data augmentation scheme based on natural image degradation process is\\nproposed. Extensive experiments show that the simple but effective technique significantly improves the model robustness\\nagainst various realistic distortions and processing operations\\nin typical imaging workflows.\\nR EFERENCES\\n[1] S. F. Dodge and L. Karam, “Understanding how image quality affects\\ndeep neural networks,” 2016 Eighth International Conference on Quality\\nof Multimedia Experience (QoMEX), pp. 1–6, 2016.\\n[2] Y. Zhou, S. Song, and N.-M. Cheung, “On classification of distorted images with deep convolutional neural networks,” 2017 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp.\\n1213–1217, 2017.\\n[3] D. Hendrycks and T. Dietterich, “Benchmarking neural network robustness to common corruptions and perturbations,” Proceedings of the\\nInternational Conference on Learning Representations, 2019.\\n[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\\nA large-scale hierarchical image database,” in 2009 IEEE Conference on\\nComputer Vision and Pattern Recognition, 2009, pp. 248–255.\\n[5] C. Michaelis, B. Mitzkus, R. Geirhos, E. Rusak, O. Bringmann, A. S.\\nEcker, M. Bethge, and W. Brendel, “Benchmarking robustness in object\\ndetection: Autonomous driving when winter is coming,” arXiv preprint\\narXiv:1907.07484, 2019.\\n[6] C. Kamann and C. Rother, “Benchmarking the robustness of semantic\\nsegmentation models,” 2020 IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), Jun 2020. [Online]. Available:\\nhttp://dx.doi.org/10.1109/CVPR42600.2020.00885\\n[7] C. Sakaridis, D. Dai, and L. Van Gool, “Acdc: The adverse conditions\\ndataset with correspondences for semantic driving scene understanding,”\\nin Proceedings of the IEEE/CVF International Conference on Computer\\nVision (ICCV), October 2021, pp. 10 765–10 775.\\n[8] S. Agarwal, H. Farid, Y. Gu, M. He, K. Nagano, and H. Li, “Protecting\\nworld leaders against deep fakes,” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.\\n[9] X. Yang, Y. Li, and S. Lyu, “Exposing deep fakes using inconsistent\\nhead poses,” ICASSP 2019 - 2019 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP), pp. 8261–8265,\\n2019.\\n[10] T. Jung, S. Kim, and K. Kim, “Deepvision: Deepfakes detection using\\nhuman eye blinking pattern,” IEEE Access, vol. 8, pp. 83 144–83 154,\\n2020.\\n\\n[11] P. Zhou, X. Han, V. I. Morariu, and L. S. Davis, “Two-Stream Neural\\nNetworks for Tampered Face Detection,” in 2017 IEEE Conference on\\nComputer Vision and Pattern Recognition Workshops (CVPRW), Jul.\\n2017, pp. 1831–1839, iSSN: 2160-7516.\\n[12] A. Rössler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and\\nM. Nießner, “FaceForensics++: Learning to detect manipulated facial\\nimages,” in International Conference on Computer Vision (ICCV), 2019.\\n[13] F. Chollet, “Xception: Deep learning with depthwise separable convolutions,” 2017 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), pp. 1800–1807, 2017.\\n[14] H. H. Nguyen, J. Yamagishi, and I. Echizen, “Use of a capsule network\\nto detect fake images and videos,” ArXiv, vol. abs/1910.12467, 2019.\\n[15] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between\\ncapsules,” Advances in neural information processing systems, vol. 30,\\n2017.\\n[16] H. Zhao, W. Zhou, D. Chen, T. Wei, W. Zhang, and N. Yu, “Multiattentional deepfake detection,” 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2185–2194, 2021.\\n[17] Y. Li, X. Yang, P. Sun, H. Qi, and S. Lyu, “Celeb-df: A large-scale\\nchallenging dataset for deepfake forensics,” in IEEE Conference on\\nComputer Vision and Patten Recognition (CVPR), 2020.\\n[18] L. Jiang, R. Li, W. Wu, C. Qian, and C. C. Loy, “Deeperforensics-1.0:\\nA large-scale dataset for real-world face forgery detection,” 2020.\\n[19] B. Dolhansky, J. Bitton, B. Pflaum, J. Lu, R. Howes, M. Wang, and\\nC. C. Ferrer, “The deepfake detection challenge dataset,” 2020.\\n[20] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond\\nempirical risk minimization,” in International Conference on Learning\\nRepresentations, 2018.\\n[21] D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lakshminarayanan, “Augmix: A simple data processing method to improve\\nrobustness and uncertainty,” in International Conference on Learning\\nRepresentations, 2019.\\n[22] N. Ford, J. Gilmer, N. Carlini, and E. D. Cubuk, “Adversarial examples\\nare a natural consequence of test error in noise,” in ICML, 2019.\\n[23] E. Rusak, L. Schott, R. S. Zimmermann, J. Bitterwolf, O. Bringmann,\\nM. Bethge, and W. Brendel, “Increasing the robustness of dnns against\\nimage corruptions by playing the game of noise,” 2020.\\n[24] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, “Beyond a gaussian\\ndenoiser: Residual learning of deep cnn for image denoising,” IEEE\\nTransactions on Image Processing, vol. 26, pp. 3142–3155, 2017.\\n[25] D. E. King, “Dlib-ml: A machine learning toolkit,” Journal of Machine\\nLearning Research, vol. 10, pp. 1755–1758, 2009.\\n[26] A. Foi, M. Trimeche, V. Katkovnik, and K. Egiazarian, “Practical\\npoissonian-gaussian noise modeling and fitting for single-image rawdata,” IEEE Transactions on Image Processing, vol. 17, no. 10, pp.\\n1737–1754, 2008.\\n[27] T. Marciniak, A. Chmielewska, R. Weychan, M. Parzych, and\\nA. Dabrowski, “Influence of low resolution of images on reliability\\nof face detection and recognition,” Multimedia Tools and Applications,\\nvol. 74, 06 2013.\\n[28] P. Li, L. Prieto, D. Mery, and P. J. Flynn, “On low-resolution face recognition in the wild: Comparisons and new techniques,” IEEE Transactions\\non Information Forensics and Security, vol. 14, pp. 2000–2012, 2019.\\n[29] J. Ballé, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, “Variational\\nimage compression with a scale hyperprior,” in International Conference\\non Learning Representations, 2018.\\n\\n\\x0c'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking text\n",
    "df_x.text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING - DELETE DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.remove(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/deepfake_txt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH + selected_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: consider if source (e.g. arxiv) should be here:\n",
    "# if yes: edit processing files tto change directory names so we can keep this automatic-ish\n",
    "df_x.to_csv(DATA_PATH + selected_dir + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f045682951559cbc0979d5d7223b93f289f756c5241efdcb485f4eca938569a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('dri_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
