{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# INSTALL FIRST (noted again when needed below):\n",
    "# nltk.download('stopwords')\n",
    "# python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data in, out data goes to main data folder\n",
    "DATA_PATH = '../data/raw/'\n",
    "OUTPUT_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check files in data folder\n",
    "datafiles = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f))]\n",
    "\n",
    "print('Index, Filename')\n",
    "print(list(zip([index for index, value in enumerate(datafiles)], datafiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df\n",
    "load_file = datafiles[0]\n",
    "\n",
    "df = pd.read_json(DATA_PATH + load_file, convert_dates=True, lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL FIRST: python -m spacy download en_core_web_sm\n",
    "# Initialize spacy 'en' model\n",
    "nlp = spacy.load(\"en_core_web_sm\") # disable=['parser', 'ner']) ## if you need efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column name where you want to do stuff\n",
    "action_col = 'summary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    cleaned_text = text.lower() # lower case\n",
    "    cleaned_text = cleaned_text.replace(\"\\n\", \" \")\n",
    "    cleaned_text = cleaned_text.replace(\"\\t\", \" \")\n",
    "    cleaned_text = cleaned_text.replace('\\r', '')\n",
    "\n",
    "    cleaned_text = re.sub('[^\\S\\r\\n]{2,}', ' ', cleaned_text) # extra spaces\n",
    "    cleaned_text = cleaned_text.rstrip()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaning'] = df[action_col].dropna().apply(lambda x: cleaning(x))\n",
    "df.cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat punctuation list\n",
    "\n",
    "special_punctuation = '：，,《。》“„:一・«»”“]'\n",
    "\n",
    "final_punctuation = string.punctuation + special_punctuation\n",
    "final_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(txt):\n",
    "    txt_nopunct = ''.join([c for c in txt if c not in final_punctuation])\n",
    "    return txt_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaning'] = df['cleaning'].dropna().apply(lambda x: remove_punctuation(x))\n",
    "df.cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, lemmatize, drop POSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to keep even if they are not in POS\n",
    "to_keep = ['disinformation'] #deepfake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization function\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    doc = nlp(\"\".join(texts)) #nlp(sent) #\" \".join(sent)) \n",
    "    texts_out = [token.lemma_ for token in doc if token.pos_ in allowed_postags or token.text in to_keep]\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma_text'] = df['cleaning'].dropna().apply(lambda x:  lemmatization(x, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed, unless no lemmaization is done\n",
    "#df['tokens'] = df.dropna().apply(lambda row: nltk.word_tokenize(row['tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat stopword list\n",
    "\n",
    "# RUN\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "lang = 'english'\n",
    "\n",
    "stop_words = list(stopwords.words(lang))\n",
    "\n",
    "# add in anything else we need to remove, eg. for some analysis, any search tags would be dropped from text\n",
    "new_stop_words = [] \n",
    "\n",
    "final_stop_words = stop_words + new_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(txt):\n",
    "    txt_nostops = [w for w in txt if not w in final_stop_words]\n",
    "    #txt_nostops = ' '.join([w for w in txt if not w in stop_words]) # Alternate\n",
    "    return txt_nostops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['lemma_text'].dropna().apply(lambda x:  remove_stopwords(x))\n",
    "df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional functions\n",
    "\n",
    "def remove_numbers(txt):\n",
    "    result = ''.join([i for i in txt if not i.isdigit()])    \n",
    "    return result\n",
    "\n",
    "def get_numbers(txt):\n",
    "    x = re.findall(r'\\d+', txt)\n",
    "    return len(x)\n",
    "\n",
    "def pos_count(pos, txt):\n",
    "    x = [token for token in txt if token.endswith(pos)]\n",
    "    #y = [token.split('/')[0] for token in x] # use when I need lists with just these!\n",
    "    return len(x)\n",
    "    #return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(x):\n",
    "      tag = x[0]['term']\n",
    "      # TO DO - scrape https://arxiv.org/category_taxonomy to translate codes to plain english\n",
    "      return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['tags'].dropna().apply(lambda x:  get_tag(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period(x, period):\n",
    "      #output = x.split(',')[period] ## For string splitting\n",
    "      output = x[:period]\n",
    "      if len(output) == 1:\n",
    "            return output[0]\n",
    "      else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['published_parsed'].dropna().apply(lambda x:  get_period(x, 1))\n",
    "df['month_year'] = df['published_parsed'].dropna().apply(lambda x:  get_period(x, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('lemma_text', axis=1, inplace=True)\n",
    "#df.drop('cleaning', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = load_file.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_PATH + out_file + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da0017f62db7e548d7b812a9c7093f1ca47065612340949c7e49f676fc86bc13"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('dr_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
