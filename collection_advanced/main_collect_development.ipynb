{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import lxml\n",
    "\n",
    "import json\n",
    "#from dateutil import rrule\n",
    "\n",
    "## needs a py file to import properly in this structure\n",
    "#from utils.collection_utils import datetime_parse\n",
    "\n",
    "from itertools import combinations, permutations, chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "IN_DATA_PATH = '../data/input_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! Datetime function needs work\n",
    "\n",
    "If the month dict makes a replacement, need to tag that as a month and limit my permutations as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_parse(x):\n",
    "      '''\n",
    "      Parse datetime out of a string\n",
    "      More functionality should be added as issues encountered \n",
    "      '''\n",
    "      # Remove all non-alpha-numeric\n",
    "      out = re.sub(r'[^0-9a-zA-Z:]+', ' ', x)\n",
    "\n",
    "      ## Remove any starting tag with :\n",
    "      # str.split('Updated: ', expand=True)\n",
    "      \n",
    "      # Remove time \n",
    "      # .str.replace(r'\\b(([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9](:[0-5][0-9])?\\s?([AaPp][Mm])?)', ' ')\n",
    "      \n",
    "      month_dict = {\n",
    "      'January':'1', 'February':'2', 'March':'3', 'April':'4', 'May':'5', 'June':'6',\n",
    "      'July':'7', 'August':'8', 'September':'9', 'October':'10', 'November':'11', \n",
    "      'December':'12', 'Jan':'1', 'Feb':'2', 'Mar':'3', 'Apr':'4', 'May':'5',\n",
    "      'Jun':'6', 'Jul':'7', 'Aug':'8', 'Sep':'9', 'Oct':'10', 'Nov':'11', 'Dec':'12'\n",
    "      }\n",
    "\n",
    "      # Removing digits longer than 4 long and words not in months\n",
    "      #out = [out.replace(key, value) for key, value in month_dict.items() if key in out][0]\n",
    "      \n",
    "      # Removing digits longer than 4 long and words not in months (now redundant)\n",
    "      #out = re.sub(r'[0-9]\\d{4,}', ' ', out)\n",
    "      #out = re.sub(r'[0-9]+:[0-9]+', ' ', out) # all after : not tested\n",
    "\n",
    "      out = out.split(' ')\n",
    "      out = [word for word in out if word.isdigit() or word in list(month_dict.keys())]\n",
    "      \n",
    "      out = ' '.join(out[:3])\n",
    "      # Strip loose whitespace\n",
    "\n",
    "      out = out.strip()\n",
    "\n",
    "      # Lists for parsing\n",
    "      month = ['%b', '%m', '%B']\n",
    "      day = ['%d']\n",
    "      year = ['%Y', '%y']\n",
    "\n",
    "      varieties = list(permutations(chain(month,day,year), 3))\n",
    "      for v in varieties:\n",
    "            v = ' '.join(v)\n",
    "            try:\n",
    "                  date = [datetime.strptime(str(out), v)]# if d != 0 else d for d in out] #old tag used elsewhere, here for ref.\n",
    "\n",
    "                  if date is not None:\n",
    "                        print('Successfully parsed with format: ' + v)\n",
    "                        return date\n",
    "                        break\n",
    "            except:\n",
    "                  #print('Failed: ' + v)\n",
    "                  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed with format: %B %d %Y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2020, 11, 14, 0, 0)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = 'Issue 8: November 14, 2020'\n",
    "datetime_parse(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To load as dataframe: no value at present \n",
    "\n",
    "# load_file = IN_DATA_PATH + 'collection_urls_df.jsonl'\n",
    "\n",
    "# df = pd.read_json(load_file, convert_dates=True, lines=True, orient='records')\n",
    "# df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file = IN_DATA_PATH + 'collection_urls_dict.json'\n",
    "\n",
    "with open(load_file) as handle:\n",
    "    sources = json.loads(handle.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNA_Russia': 'https://www.cna.org/centers/cna/sppp/rsp/russia-ai-archive#newsletters',\n",
       " 'CNA_China': 'https://www.cna.org/centers/cna/cip/china/china-ai-newsletter',\n",
       " 'MIT_Technology_Review': 'https://www.technologyreview.com/',\n",
       " 'Synced': 'https://syncedreview.com/',\n",
       " 'IEEE_spectrum': 'https://spectrum.ieee.org/',\n",
       " 'Import_AI': 'https://us13.campaign-archive.com/home/?u=67bd06787e84d73db24fb0aa5&id=6c9d98ff2c',\n",
       " 'ChinAI': 'https://chinai.substack.com/archive?sort=new'}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 25, 15, 54, 12, 285750)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get today's date\n",
    "# To do: create a \"last scraped json\"\n",
    "today = datetime.now()\n",
    "\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for saving collected data\n",
    "\n",
    "title_list = []\n",
    "url_list = []\n",
    "dates = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = sources['Import_AI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company\n",
      "http://eepurl.com/hZjozT\n",
      "04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data\n",
      "http://eepurl.com/hYSyi9\n",
      "03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains\n",
      "http://eepurl.com/hYaLin\n",
      "03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless.\n",
      "http://eepurl.com/hXCPbv\n",
      "03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well\n",
      "http://eepurl.com/hWsYwH\n",
      "02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears\n",
      "http://eepurl.com/hVSMf1\n",
      "02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m\n",
      "http://eepurl.com/hVhHez\n",
      "02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI\n",
      "http://eepurl.com/hUEhnX\n",
      "02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project.\n",
      "http://eepurl.com/hT5gnr\n",
      "02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition\n",
      "http://eepurl.com/hTyKXj\n",
      "01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs\n",
      "http://eepurl.com/hSYp5z\n",
      "01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet\n",
      "http://eepurl.com/hSmoov\n",
      "01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance\n",
      "http://eepurl.com/hRQPeD\n",
      "12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI\n",
      "http://eepurl.com/hQZI61\n",
      "12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs\n",
      "http://eepurl.com/hPQFJv\n",
      "12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like\n",
      "http://eepurl.com/hPe-3n\n",
      "11/22/2021 - Import AI 275: Facebook dreams of a world-spanning neural net; Microsoft announces a 30-petaflop supercomputer; FTC taps AI Now for AI advice\n",
      "http://eepurl.com/hN0Cdb\n",
      "11/15/2021 - Import AI 274: Multilingual models cement power structures; a giant British Sign Language dataset;Â  and benchmarks for the UN SDGs\n",
      "http://eepurl.com/hNnGtb\n",
      "11/08/2021 - Import AI 273: Corruption VS Surveillance; Baidu makes better object detection; understanding the legal risk of datasets\n",
      "http://eepurl.com/hMRZZ9\n",
      "11/01/2021 - Import AI #272: AGI-never or AGI-soon?, simulating stock markets; evaluating unsupervised RL\n",
      "http://eepurl.com/hMdXB1\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Get list of all Import AI issues with date, title, and link\n",
    "\n",
    "response = requests.post(base_url) #headers=header)\n",
    "\n",
    "html = soup(response.text, 'lxml')\n",
    "\n",
    "objects = html.find_all('li', class_=\"campaign\")\n",
    "for obj in objects:\n",
    "      print(obj.text)\n",
    "      print(obj.a['href'])\n",
    "      title_list.append(obj.text)\n",
    "      url_list.append(obj.a['href'])\n",
    "\n",
    "print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04 11 2022 Import AI 291: Google trains the world s biggest language model how robots can be smarter about the world Conjecture a new AI alignment company\n",
      "Successfully parsed with format: %m %d %Y\n",
      "04 05 2022 Import AI 290: China plans massive models DeepMind makes a smaller and smarter model open source CLIP data\n",
      "Successfully parsed with format: %m %d %Y\n",
      "03 28 2022 Import AI 289: Copyright v AI art NIST tries to measure bias in AI solar powered Markov chains\n",
      "Successfully parsed with format: %m %d %Y\n",
      "03 21 2022 Import AI 288: Chinese researchers try to train 100trillion brain scale models 33 of AI benchmarks are meaningless \n",
      "Successfully parsed with format: %m %d %Y\n",
      "03 07 2022 Import AI 287: 10 exaflop supercomputer Google deploys differential privacy humans can outsmart deepfakes pretty well\n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 28 2022 Import AI 286: Fairness through dumbness planet scale AI computing another AI safety startup appears\n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 21 2022 Import AI 285: RL Fusion why RL demands better public policy Cohere raises 125m\n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 14 2022 Import AI 284: 20bn GPT model diachronic LMs what people think about AI\n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 07 2022 Import AI 283: Open source 20B GPT3 Chinese researchers make better adversarial example attacks Mozilla launches AI auditing project \n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 01 2022 Import AI 282: Facebook s AI supercomputer Anduril gets a SOCOM contract Twitter talks about running an algo bias competition\n",
      "Successfully parsed with format: %m %d %Y\n",
      "01 24 2022 Import AI 281: China does more surveillance research than US and Europe Google reveals its text model LaMDA Microsoft improves MoEs\n",
      "Successfully parsed with format: %m %d %Y\n",
      "01 17 2022 Import AI 280: Why bigger is worse for RL AI generated Pokemon real world EfficientNet\n",
      "Successfully parsed with format: %m %d %Y\n",
      "01 10 2022 Import AI 279: Baidu adds knowledge to a language model US military AI how China thinks about AI governance\n",
      "Successfully parsed with format: %m %d %Y\n",
      "12 27 2021 Import AI 278: Can we ever trust an AI what the future of semiconductors looks like better images of AI\n",
      "Successfully parsed with format: %m %d %Y\n",
      "12 13 2021 Import AI 277: DeepMind builds a GPT 3 model Catalan GLUE FTC plans AI regs\n",
      "Successfully parsed with format: %m %d %Y\n",
      "12 06 2021 Import AI 276: Tracking journalists with computer vision spotting factory defects with AI and what simulated war might look like\n",
      "Successfully parsed with format: %m %d %Y\n",
      "11 22 2021 Import AI 275: Facebook dreams of a world spanning neural net Microsoft announces a 30 petaflop supercomputer FTC taps AI Now for AI advice\n",
      "Successfully parsed with format: %m %d %Y\n",
      "11 15 2021 Import AI 274: Multilingual models cement power structures a giant British Sign Language dataset and benchmarks for the UN SDGs\n",
      "Successfully parsed with format: %m %d %Y\n",
      "11 08 2021 Import AI 273: Corruption VS Surveillance Baidu makes better object detection understanding the legal risk of datasets\n",
      "Successfully parsed with format: %m %d %Y\n",
      "11 01 2021 Import AI 272: AGI never or AGI soon simulating stock markets evaluating unsupervised RL\n",
      "Successfully parsed with format: %m %d %Y\n"
     ]
    }
   ],
   "source": [
    "# Parse date from list\n",
    "\n",
    "for item in title_list:\n",
    "      date_sequence = datetime_parse(item)\n",
    "      dates.append(date_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://eepurl.com/hZjozT</td>\n",
       "      <td>04/11/2022 - Import AI 291: Google trains the ...</td>\n",
       "      <td>2022-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://eepurl.com/hYSyi9</td>\n",
       "      <td>04/05/2022 - Import AI 290: China plans massiv...</td>\n",
       "      <td>2022-04-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        url  \\\n",
       "0  http://eepurl.com/hZjozT   \n",
       "1  http://eepurl.com/hYSyi9   \n",
       "\n",
       "                                               title       date  \n",
       "0  04/11/2022 - Import AI 291: Google trains the ... 2022-04-11  \n",
       "1  04/05/2022 - Import AI 290: China plans massiv... 2022-04-05  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save collection to dataframe\n",
    "\n",
    "df_collected = pd.DataFrame(list(zip(title_list, url_list, dates)), \n",
    "            columns=['title', 'url', 'date'])\n",
    "\n",
    "df_collected.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'title', 'date'], dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collected.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter keywords in json referenced here\n",
    "\n",
    "# For testing only\n",
    "#search_terms = ['DeepMind', 'Google']\n",
    "\n",
    "load_file = IN_DATA_PATH + 'collection_searchterms.json'\n",
    "\n",
    "with open(load_file) as handle:\n",
    "    search_terms = json.loads(handle.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeepMind', 'Google']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms['search_term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to save to\n",
    "relevant_text = []\n",
    "\n",
    "#preping list fo dictionary conversion\n",
    "relevant_text.append(['title', 'url', 'date', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: http://eepurl.com/hZjozT\n",
      "True\n",
      "Fetching: http://eepurl.com/hYSyi9\n",
      "True\n",
      "Fetching: http://eepurl.com/hYaLin\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Gets text for results within timedelta window\n",
    "#     This is probably best changed to a \"last scraped\" date from a json\n",
    "\n",
    "for index, row in df_collected.iterrows():\n",
    "      if row.date >= today - timedelta(days=30):\n",
    "            print('Fetching: ' + row.url)\n",
    "            response = requests.post(row.url) #headers=header)\n",
    "\n",
    "            html = soup(response.text, 'lxml')\n",
    "\n",
    "            objects = html.find_all('p')#, class_=\"campaign\")\n",
    "            issue_text = []\n",
    "            for obj in objects:\n",
    "                  issue_text.append(obj.text)\n",
    "                  #print(obj.text)\n",
    "            issue_text = ' '.join(issue_text)\n",
    "            #print(issue_text)\n",
    "            #print('--------------------------------')\n",
    "            \n",
    "            for word in search_terms['search_term']:\n",
    "                  if word in issue_text:\n",
    "                        save = list(row)\n",
    "                        save.append(issue_text)\n",
    "                        relevant_text.append(save)                  \n",
    "                        print('Search term found: ' +  word)\n",
    "                        break\n",
    "                  else: \n",
    "                        pass\n",
    "            time.sleep(5)\n",
    "      else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To save to json - not recommend as datetimes not serializable, would have to convert dt back to string\n",
    "# Note - ugly dictionary, e.g. not jsonl or indented, but kept simple unless requested\n",
    "\n",
    "# data_dict = {k: v for k, *v in zip(*relevant_text)}\n",
    "\n",
    "# with open('data.json', 'w') as f:\n",
    "#     json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(relevant_text[1:],columns=relevant_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: add if file clause\n",
    "old_df = pd.read_csv(DATA_PATH + 'open_ai_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([new_df, old_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If timing for collection and timedelta etc are good, should not be needed, but good to be sure\n",
    "# If you instead have date issues, you should do this in the collect loop to eliminate unneeeded collectio\n",
    "# e.g. if url in list(old_df.url): pass\n",
    "\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://eepurl.com/hZjozT</td>\n",
       "      <td>04/11/2022 - Import AI 291: Google trains the ...</td>\n",
       "      <td>2022-04-11 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://eepurl.com/hYSyi9</td>\n",
       "      <td>04/05/2022 - Import AI 290: China plans massiv...</td>\n",
       "      <td>2022-04-05 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://eepurl.com/hYaLin</td>\n",
       "      <td>03/28/2022 - Import AI 289: Copyright v AI art...</td>\n",
       "      <td>2022-03-28 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        url  \\\n",
       "0  http://eepurl.com/hZjozT   \n",
       "1  http://eepurl.com/hYSyi9   \n",
       "2  http://eepurl.com/hYaLin   \n",
       "\n",
       "                                               title                 date  \\\n",
       "0  04/11/2022 - Import AI 291: Google trains the ...  2022-04-11 00:00:00   \n",
       "1  04/05/2022 - Import AI 290: China plans massiv...  2022-04-05 00:00:00   \n",
       "2  03/28/2022 - Import AI 289: Copyright v AI art...  2022-03-28 00:00:00   \n",
       "\n",
       "                                                text  \n",
       "0  Welcome to Import AI, a newsletter about artif...  \n",
       "1  Welcome to Import AI, a newsletter about artif...  \n",
       "2  Welcome to Import AI, a newsletter about artif...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(DATA_PATH + 'open_ai_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://syncedreview.com/'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = sources['Synced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 22, 9, 46, 7, 425081)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date part for url\n",
    "year = datetime.now().year\n",
    "month = datetime.now().month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://syncedreview.com/2022/4/'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_url = base_url + str(year) + '/' +  str(month) + '/'\n",
    "current_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists for saving\n",
    "title_list = []\n",
    "url_list = []\n",
    "summary_list = []\n",
    "date_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "# Setting up a fake useragent\n",
    "# Can do for all collections, but for now limited to where is seems necessary only\n",
    "# Ignore the error \n",
    "\n",
    "ua = UserAgent(verify_ssl=False, cache=False)\n",
    "\n",
    "user_agent = ua.random\n",
    "header = {'User-Agent': user_agent}\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url, headers=header)\n",
    "\n",
    "html = soup(response.text, 'lxml')\n",
    "\n",
    "obj = html.find('div',id= \"primary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dates = obj.find_all(class_='entry-date')\n",
    "for d in dates:\n",
    "      d_parsed = datetime_parse(d.text)\n",
    "      date_list.append(d_parsed[0])\n",
    "      #date_list.append(d.text)\n",
    "\n",
    "titles = obj.find_all(class_='entry-title')\n",
    "for t in titles:\n",
    "      title_list.append(t.text)\n",
    "      url_list.append(t.a['href'])\n",
    "\n",
    "summaries = obj.find_all(class_='entry-summary')\n",
    "for s in summaries:\n",
    "      summary_list.append(s.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Tsinghua &amp; NKUâs Visual Attention Network Comb...</td>\n",
       "      <td>https://syncedreview.com/2022/02/23/deepmind-p...</td>\n",
       "      <td>2022-02-23</td>\n",
       "      <td>\\nIn the new paper Visual Attention Network, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>DeepMind Trains Agents to Control Computers as...</td>\n",
       "      <td>https://syncedreview.com/2022/02/22/deepmind-p...</td>\n",
       "      <td>2022-02-22</td>\n",
       "      <td>\\nDeepMind trains agents to use keyboard and m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "38  Tsinghua & NKUâs Visual Attention Network Comb...   \n",
       "39  DeepMind Trains Agents to Control Computers as...   \n",
       "\n",
       "                                                  url       date  \\\n",
       "38  https://syncedreview.com/2022/02/23/deepmind-p... 2022-02-23   \n",
       "39  https://syncedreview.com/2022/02/22/deepmind-p... 2022-02-22   \n",
       "\n",
       "                                              summary  \n",
       "38  \\nIn the new paper Visual Attention Network, a...  \n",
       "39  \\nDeepMind trains agents to use keyboard and m...  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collected2 = pd.DataFrame(list(zip(title_list, url_list, date_list, summary_list)), \n",
    "            columns=['title', 'url', 'date', 'summary'])\n",
    "\n",
    "df_collected2.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword = 'AlexNet'\n",
    "search_terms['search_term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_text = []\n",
    "\n",
    "relevant_text.append(['title', 'url', 'date', 'summary', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://syncedreview.com/2022/04/20/uc-berkeley-intels-photorealistic-denoising-method-boosts-video-quality-on-moonless-nights/\n",
      "Fetching: https://syncedreview.com/2022/04/19/toward-self-improving-neural-networks-schmidhuber-teams-scalable-self-referential-weight-matrix-learns-to-modify-itself/\n",
      "Fetching: https://syncedreview.com/2022/04/18/meet-deepdpm-no-predefined-number-of-clusters-needed-for-deep-clustering-tasks/\n",
      "Fetching: https://syncedreview.com/2022/04/14/alibabas-usi-a-unified-scheme-for-training-any-backbone-on-imagenet-that-delivers-top-results-without-hyperparameter-tuning/\n",
      "Fetching: https://syncedreview.com/2022/04/13/openais-unclip-text-to-image-system-leverages-contrastive-and-diffusion-models-to-achieve-sota-performance/\n",
      "Fetching: https://syncedreview.com/2022/04/12/google-builds-language-models-with-socratic-dialogue-to-improve-zero-shot-multimodal-reasoning-capabilities/\n",
      "Search term found: Google\n",
      "Fetching: https://syncedreview.com/2022/04/11/maryland-u-google-introduce-lilnetx-simultaneously-optimizing-dnn-size-cost-structured-sparsity-accuracy/\n",
      "Search term found: Google\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_collected2.iterrows():\n",
    "      if row.date >= today - timedelta(days=14):\n",
    "            print('Fetching: ' + row.url)\n",
    "            response = requests.post(row.url) #headers=header)\n",
    "\n",
    "            html = soup(response.text, 'lxml')\n",
    "\n",
    "            objects = html.find_all('div', class_=\"entry-content\")\n",
    "            article_text = []\n",
    "            for obj in objects:\n",
    "                  paragraph_text = obj.find_all('p')\n",
    "                  for p in paragraph_text:\n",
    "                        article_text.append(p.text)\n",
    "                  #print(obj.text)\n",
    "            article_text = ' '.join(article_text)\n",
    "\n",
    "            for word in search_terms['search_term']:\n",
    "                  if word in article_text:\n",
    "                        save = list(row)\n",
    "                        save.append(article_text)\n",
    "                        relevant_text.append(save)                  \n",
    "                        print('Search term found: ' +  word)\n",
    "                        break\n",
    "            else: \n",
    "                  pass\n",
    "\n",
    "            time.sleep(5)\n",
    "      else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(relevant_text[1:],columns=relevant_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Builds Language Models with Socratic Di...</td>\n",
       "      <td>https://syncedreview.com/2022/04/12/google-bui...</td>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>\\n In the new paper Socratic Models: Composing...</td>\n",
       "      <td>Large-scale language-based foundation models s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maryland U &amp; Google Introduce LilNetX: Simulta...</td>\n",
       "      <td>https://syncedreview.com/2022/04/11/maryland-u...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>\\nA team from the University of Maryland and G...</td>\n",
       "      <td>The current conventional wisdom on deep neural...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Google Builds Language Models with Socratic Di...   \n",
       "1  Maryland U & Google Introduce LilNetX: Simulta...   \n",
       "\n",
       "                                                 url       date  \\\n",
       "0  https://syncedreview.com/2022/04/12/google-bui... 2022-04-12   \n",
       "1  https://syncedreview.com/2022/04/11/maryland-u... 2022-04-11   \n",
       "\n",
       "                                             summary  \\\n",
       "0  \\n In the new paper Socratic Models: Composing...   \n",
       "1  \\nA team from the University of Maryland and G...   \n",
       "\n",
       "                                                text  \n",
       "0  Large-scale language-based foundation models s...  \n",
       "1  The current conventional wisdom on deep neural...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: add if file clause\n",
    "old_df = pd.read_csv(DATA_PATH + 'synced_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([new_df, old_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If timing for collection and timedelta etc are good, should not be needed, but good to be sure\n",
    "# If you instead have date issues, you should do this in the collect loop to eliminate unneeeded collectio\n",
    "# e.g. if url in list(old_df.url): pass\n",
    "\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://eepurl.com/hZjozT</td>\n",
       "      <td>04/11/2022 - Import AI 291: Google trains the ...</td>\n",
       "      <td>2022-04-11 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://eepurl.com/hYSyi9</td>\n",
       "      <td>04/05/2022 - Import AI 290: China plans massiv...</td>\n",
       "      <td>2022-04-05 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://eepurl.com/hYaLin</td>\n",
       "      <td>03/28/2022 - Import AI 289: Copyright v AI art...</td>\n",
       "      <td>2022-03-28 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        url  \\\n",
       "0  http://eepurl.com/hZjozT   \n",
       "1  http://eepurl.com/hYSyi9   \n",
       "2  http://eepurl.com/hYaLin   \n",
       "\n",
       "                                               title                 date  \\\n",
       "0  04/11/2022 - Import AI 291: Google trains the ...  2022-04-11 00:00:00   \n",
       "1  04/05/2022 - Import AI 290: China plans massiv...  2022-04-05 00:00:00   \n",
       "2  03/28/2022 - Import AI 289: Copyright v AI art...  2022-03-28 00:00:00   \n",
       "\n",
       "                                                text  \n",
       "0  Welcome to Import AI, a newsletter about artif...  \n",
       "1  Welcome to Import AI, a newsletter about artif...  \n",
       "2  Welcome to Import AI, a newsletter about artif...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(DATA_PATH + 'synced_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = sources['MIT_Technology_Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_designator = 'topic/'\n",
    "topic_tags = ['artificial-intelligence', 'humans-and-technology', 'computing'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_url \u001b[38;5;241m=\u001b[39m \u001b[43mbase_url\u001b[49m \u001b[38;5;241m+\u001b[39m topic_designator \u001b[38;5;241m+\u001b[39m topic_tags[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m final_url\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_url' is not defined"
     ]
    }
   ],
   "source": [
    "final_url = base_url + topic_designator + topic_tags[0]\n",
    "final_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in topic_tags:\n",
    "      final_url = base_url + topic_designator + tag\n",
    "      # then search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Server': 'nginx', 'Date': 'Fri, 22 Apr 2022 10:42:32 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Content-Length': '34115', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'ETag': 'W/\"2ed7b-nH1JQs3vy3bcAMjKg3IzOTe7W+Y\"', 'Cache-Control': 'max-age=300, must-revalidate', 'Content-Encoding': 'gzip', 'X-rq': 'hhn2 0 2 9980', 'Age': '0', 'X-Cache': 'miss', 'Vary': 'X-Mobile-Class, Accept-Encoding', 'Accept-Ranges': 'bytes', 'Strict-Transport-Security': 'max-age=31536000;includeSubdomains;preload'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(final_url)\n",
    "\n",
    "# print headers of response\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ROOT_URL = \"https://wp.technologyreview.com/wp-json/irving/v1/data/term_archive?category_name=artificial-intelligence&page=1\"\n",
    "'''\n",
    "API_ROOT_URL\n",
    "slug\n",
    "?\n",
    "requestType\n",
    "=\n",
    "query\n",
    "&page=1'''\n",
    "\n",
    "def searchApi():\n",
    "    endpoint = API_ROOT_URL\n",
    "    data = {\n",
    "        \"slug\": \"term_archive\",\n",
    "        \"requestType\": \"category_name\",\n",
    "        \"query\": \"artificial-intelligence\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(endpoint)#, data=data)\n",
    "        if(response.status_code == 200):\n",
    "            #print(response.json())\n",
    "            for msg in response:\n",
    "                print(msg)\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchApi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_Link = \"https://wp.technologyreview.com/wp-json/irving/v1/data/term_archive?category_name=\"\n",
    "#artificial-intelligence\"\n",
    "#&page=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Server': 'nginx', 'Date': 'Mon, 25 Apr 2022 13:47:18 GMT', 'Content-Type': 'application/json; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Robots-Tag': 'noindex', 'Link': '<https://wp.technologyreview.com/wp-json/>; rel=\"https://api.w.org/\"', 'X-Content-Type-Options': 'nosniff', 'Access-Control-Expose-Headers': 'X-WP-Total, X-WP-TotalPages, Link', 'Access-Control-Allow-Headers': 'Authorization, X-WP-Nonce, Content-Disposition, Content-MD5, Content-Type', 'Access-Control-Allow-Origin': 'https://www.technologyreview.com', 'Cache-Control': 'max-age=60', 'Allow': 'GET', 'X-rq': 'hhn2 0 4 9980', 'Content-Encoding': 'gzip', 'Age': '0', 'X-Cache': 'miss', 'Vary': 'Accept-Encoding, Origin', 'Accept-Ranges': 'bytes', 'Strict-Transport-Security': 'max-age=31536000;includeSubdomains;preload'}\n"
     ]
    }
   ],
   "source": [
    "# Making a get test request\n",
    "try:\n",
    "      response = requests.get(API_Link)\n",
    "\n",
    "      # print headers of response\n",
    "      if(response.status_code == 200):\n",
    "            print(response.headers)\n",
    "            \n",
    "except Exception:\n",
    "      print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(API_Link, pages=1):\n",
    "    with requests.get(API_Link + '&page=' + str(pages)) as response:\n",
    "        page_soup = soup(response.content, 'lxml')\n",
    "        return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for tag in topic_tags:\n",
    "      response = get_response(API_Link + tag, pages=str(1))\n",
    "\n",
    "      #parse\n",
    "      j_response = json.loads(response.text)\n",
    "      responses = responses + j_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'config', 'children', 'componentGroups'])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['excerpt', 'permalink', 'teaseCTA', 'title', 'topic', 'postDate', 'postFormat', 'topicLink', 'themeName', 'showImage', 'sponsorTagline', 'sponsorUrl', 'hideFeaturedImage', 'hideFooter', 'hideHeader', 'bodyLayout', 'storyColorTheme', 'headlineTextColor', 'headerBgColor', 'squareLeadImage', 'summaryBullets', 'subtopic', 'mitNewsTopic', 'mitNewsSubtopic', 'podcastLength', 'customEyebrow', 'byline', 'syndicate', 'hidePostContentAds', 'color'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[1]['config'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, url, date, summary]\n",
       "Index: []"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial empty df\n",
    "collected_df = pd.DataFrame(columns=['title', 'url', 'date', 'summary'])\n",
    "collected_df.loc[collected_df.index, :] = ['x', 'url', 'date', 'summary']\n",
    "\n",
    "collected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_keys = ['title', 'permalink', 'postDate', 'excerpt']\n",
    "\n",
    "entries = []\n",
    "entries.append(['title', 'url', 'date', 'summary'])\n",
    "\n",
    "for item in responses:\n",
    "      entry = [item['config'][key] for key in needed_keys]\n",
    "      entries.append(entry)\n",
    "      #collected_df.loc[collected_df.index.max() + 1, :] = entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(entries[1:],columns=entries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_from_url(x):\n",
    "\n",
    "      pat = r\"(20[0-2][0-9]([-_/]?)[0-9]{2}(?:\\2[0-9]{2})?)\"\n",
    "      dates = re.compile(pat)\n",
    "\n",
    "      res = dates.search(x)\n",
    "\n",
    "      res = datetime_parse(res[0])\n",
    "  \n",
    "      return res[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n"
     ]
    }
   ],
   "source": [
    "new_df['date'] = new_df['url'].apply(lambda x: date_from_url(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_text = []\n",
    "\n",
    "relevant_text.append(['title', 'url', 'date', 'summary', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.technologyreview.com/2022/04/22/1051002/download-language-preserving-ai-hackers-breach-critical-infrastructure-power-grid-gas-pipeline/\n",
      "Search term found: Google\n",
      "Fetching: https://www.technologyreview.com/2022/04/22/1050394/artificial-intelligence-for-the-people/\n",
      "Search term found: Google\n",
      "Fetching: https://www.technologyreview.com/2022/04/21/1050381/the-gig-workers-fighting-back-against-the-algorithms/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     25\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m       \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m       \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in new_df.iterrows():\n",
    "      if row.date >= today - timedelta(days=7):\n",
    "            print('Fetching: ' + row.url)\n",
    "            response = requests.post(row.url, headers=header)\n",
    "\n",
    "            html = soup(response.text, 'lxml')\n",
    "\n",
    "            objects = html.find_all('div', id=\"content--body\")\n",
    "            article_text = []\n",
    "            for obj in objects:\n",
    "                  paragraph_text = obj.find_all('p')\n",
    "                  for p in paragraph_text:\n",
    "                        article_text.append(p.text)\n",
    "                  #print(obj.text)\n",
    "            article_text = ' '.join(article_text)\n",
    "\n",
    "            for word in search_terms['search_term']:\n",
    "                  if word in article_text:\n",
    "                        save = list(row)\n",
    "                        save.append(article_text)\n",
    "                        relevant_text.append(save)                  \n",
    "                        print('Search term found: ' +  word)\n",
    "                        break\n",
    "            else: \n",
    "                  pass\n",
    "\n",
    "            time.sleep(5)\n",
    "      else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = pd.DataFrame(relevant_text[1:],columns=relevant_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: add if file clause\n",
    "old_df = pd.read_csv(DATA_PATH + 'synced_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([new_df, old_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If timing for collection and timedelta etc are good, should not be needed, but good to be sure\n",
    "# If you instead have date issues, you should do this in the collect loop to eliminate unneeeded collectio\n",
    "# e.g. if url in list(old_df.url): pass\n",
    "\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(DATA_PATH + 'mit_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currently not working well - only 4 returns due to dynamic scrolling\n",
    "I can solve with Selenium, but trying to avoid using it\n",
    "\n",
    "Currently working on a requests version, but convoluated urls and html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://spectrum.ieee.org/res/load_more_posts/data.js?site_id=20265424&node_id=/root/blocks/block[search]/abtests/abtest[1]/row/column[1]/choose/when[getparams.order]/choose/otherwise/element_wrapper-&resource_id=search_deepfake&path_params={}&formats=html&q=deepfake&topic=&order=newest&rm_lazy_load=16&pn=0&pn_strategy=\n",
      "21 Oct 2021\n",
      "21 Sep 2021\n",
      "02 Jul 2021\n",
      "27 May 2021\n",
      "\\n    \\n        Gaming-Related Malware on the Rise on Mobile, PCs\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/mobile-malware-increasing\\\"\n",
      "\\n    \\n        7 Revealing Ways AIs Fail\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/ai-failures\\\"\n",
      "\\n    \\n        2022 U.S. Budget Funds New ICBMs\\u2014A Reckless Diversion?\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/2022-united-states-budget-funds-new-icbms-reckless-diversion\\\"\n",
      "\\n    \\n        AI Modifies Actor Performances for Flawless Dubbing\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/ai-modifies-actor-performances-for-flawless-dubbing\\\"\n",
      "https://spectrum.ieee.org/res/load_more_posts/data.js?site_id=20265424&node_id=/root/blocks/block[search]/abtests/abtest[1]/row/column[1]/choose/when[getparams.order]/choose/otherwise/element_wrapper-&resource_id=search_deepfake&path_params={}&formats=html&q=deepfake&topic=&order=newest&rm_lazy_load=16&pn=1&pn_strategy=\n",
      "21 Oct 2021\n",
      "21 Sep 2021\n",
      "02 Jul 2021\n",
      "27 May 2021\n",
      "\\n    \\n        Gaming-Related Malware on the Rise on Mobile, PCs\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/mobile-malware-increasing\\\"\n",
      "\\n    \\n        7 Revealing Ways AIs Fail\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/ai-failures\\\"\n",
      "\\n    \\n        2022 U.S. Budget Funds New ICBMs\\u2014A Reckless Diversion?\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/2022-united-states-budget-funds-new-icbms-reckless-diversion\\\"\n",
      "\\n    \\n        AI Modifies Actor Performances for Flawless Dubbing\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/ai-modifies-actor-performances-for-flawless-dubbing\\\"\n",
      "https://spectrum.ieee.org/res/load_more_posts/data.js?site_id=20265424&node_id=/root/blocks/block[search]/abtests/abtest[1]/row/column[1]/choose/when[getparams.order]/choose/otherwise/element_wrapper-&resource_id=search_deepfake&path_params={}&formats=html&q=deepfake&topic=&order=newest&rm_lazy_load=16&pn=2&pn_strategy=\n",
      "04 May 2021\n",
      "08 Apr 2021\n",
      "11 Mar 2021\n",
      "03 Mar 2021\n",
      "\\n    \\n        Too Perilous For AI? EU Proposes Risk-Based Rules\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/euairules\\\"\n",
      "\\n    \\n        Are AI Algorithms Playing Fairly with Age, Gender, and Skin Color?\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/are-ai-algorithms-playing-fairly-with-age-gender-and-skin-color\\\"\n",
      "\\n    \\n        Improved Technology for Deepfakes Highlights a Supply Chain Problem\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/deepfakes-supply-chain\\\"\n",
      "\\n    \\n        2D video to 3D faces: Facing down a key challenge in VR\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/2d-video-to-3d-faces-key-challenge-in-virtual-reality\\\"\n",
      "https://spectrum.ieee.org/res/load_more_posts/data.js?site_id=20265424&node_id=/root/blocks/block[search]/abtests/abtest[1]/row/column[1]/choose/when[getparams.order]/choose/otherwise/element_wrapper-&resource_id=search_deepfake&path_params={}&formats=html&q=deepfake&topic=&order=newest&rm_lazy_load=16&pn=3&pn_strategy=\n",
      "07 Jan 2021\n",
      "06 Jan 2021\n",
      "19 Nov 2020\n",
      "28 Sep 2020\n",
      "\\n    \\n        10 Exciting Engineering Milestones to Look for in 2021\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/10-exciting-engineering-milestones-to-look-for-in-2021\\\"\n",
      "\\n    \\n        Peering Into the COVID-19 End Game\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/peering-into-the-covid-19-end-game\\\"\n",
      "\\n    \\n        How Facebook\\u2019s AI Tools Tackle Misinformation\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/how-facebooks-ai-tools-tackle-misinformation\\\"\n",
      "\\n    \\n        The Subtle Effects of Blood Circulation Can Be Used to Detect Deep Fakes\\n    \\n\n",
      "\\\"https://spectrum.ieee.org/blook-circulation-can-be-used-to-detect-deep-fakes\\\"\n"
     ]
    }
   ],
   "source": [
    "## Can get all the rest (set range based on date) but missed first page returned\n",
    "\n",
    "counter = 0\n",
    "for i in range(1, 5): #while\n",
    "      url = \"https://spectrum.ieee.org/res/load_more_posts/data.js?site_id=20265424&node_id=/root/blocks/block[search]/abtests/abtest[1]/row/column[1]/choose/when[getparams.order]/choose/otherwise/element_wrapper-&resource_id=search_deepfake&path_params={}&formats=html&q=deepfake&topic=&order=newest&rm_lazy_load=16&pn=%s&pn_strategy=\" % (counter)\n",
    "      #url = \"https://spectrum.ieee.org/res/load_more_posts/data.js?site_id=20265424&node_id=/root/blocks/block[search]/abtests/abtest[1]/row/column[1]/choose/otherwise/choose/otherwise/element_wrapper-&resource_id=search_deepfake&path_params={}&formats=html&q=deepfake&order=newest&rm_lazy_load=1&pn=%s&pn_strategy=\" % (counter)\n",
    "      print(url)\n",
    "      counter +=1\n",
    "      # ua = UserAgent(verify_ssl=False, cache=False)\n",
    "\n",
    "      # user_agent = ua.random\n",
    "      # header = {'User-Agent': user_agent}\n",
    "\n",
    "      response = requests.post(url)#, headers=header)\n",
    "\n",
    "      html = soup(response.text, 'lxml')\n",
    "\n",
    "      objects = html.find('div')#, class_='posts-wrapper')#elid=True)#class_= 'clearfix')\n",
    "\n",
    "      for obj in objects:\n",
    "            items = obj.find_all('span')\n",
    "            for it in items:\n",
    "                  print(it.text)\n",
    "            items = obj.find_all('h2')#, class_ = 'widget__head')#, id='col-right')\n",
    "            #print(len(items))\n",
    "            for i in items:\n",
    "                  print(i.text)\n",
    "                  print(i.a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url ='https://spectrum.ieee.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = 'https://spectrum.ieee.org/search/?q='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_url = 'https://spectrum.ieee.org/topic/artificial-intelligence/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query connector\n",
    "C = \"&\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_criteria = \"order=newest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_criteria = \"topic=artificial-intelligence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 22, 9, 46, 7, 425081)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialized above\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 4, 22)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 3, 23)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_to = today.date() - timedelta(days=30)\n",
    "back_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeepMind', 'Google']"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms['search_term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://spectrum.ieee.org/search/?q=DeepMind&order=newest\n",
      "https://spectrum.ieee.org/search/?q=Google&order=newest\n"
     ]
    }
   ],
   "source": [
    "for term in search_terms['search_term']:\n",
    "      final_url = query_url + term + C + sort_criteria\n",
    "      print(final_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://spectrum.ieee.org/search/?q=Google&order=newest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "print(final_url)\n",
    "ua = UserAgent(verify_ssl=False, cache=False)\n",
    "\n",
    "user_agent = ua.random\n",
    "header = {'User-Agent': user_agent}\n",
    "\n",
    "response = requests.post(final_url, headers=header)\n",
    "\n",
    "html = soup(response.text, 'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "url_list = []\n",
    "date_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n",
      "\n",
      "        Video Friday: DALL-E 2\n",
      "    \n",
      "\n",
      "https://spectrum.ieee.org/video-friday-dall-e-2\n",
      "08 Apr 2022\n",
      "Successfully parsed with format: %m %b %Y\n",
      "\n",
      "\n",
      "        How a Parachute Accident Helped Jump-start Augmented Reality\n",
      "    \n",
      "\n",
      "https://spectrum.ieee.org/history-of-augmented-reality\n",
      "07 Apr 2022\n",
      "Successfully parsed with format: %m %b %Y\n",
      "\n",
      "\n",
      "        Volunteers Scramble to Preserve Ukraineâs Digital Culture\n",
      "    \n",
      "\n",
      "https://spectrum.ieee.org/archiving-ukraine-culture\n",
      "06 Apr 2022\n",
      "Successfully parsed with format: %m %b %Y\n",
      "\n",
      "\n",
      "        How the Wayback Machine Is Saving Digital Ukraine\n",
      "    \n",
      "\n",
      "https://spectrum.ieee.org/internet-archive-ukraine\n",
      "06 Apr 2022\n",
      "Successfully parsed with format: %m %b %Y\n"
     ]
    }
   ],
   "source": [
    "# Works some at least but only fetches top 4 due to page scroll\n",
    "\n",
    "objects = html.find_all('div',  class_='section_column')#elid=True)#class_= 'clearfix')\n",
    "\n",
    "for obj in objects:\n",
    "      items = obj.find_all('div', id='col-right')\n",
    "      print(len(items))\n",
    "      for i in items:\n",
    "\n",
    "            print(i.h2.text)\n",
    "            title_list.append(i.h2.text)\n",
    "\n",
    "            print(i.h2.a['href'])\n",
    "            url_list.append(i.h2.a['href'])\n",
    "\n",
    "            date = i.div.span\n",
    "            print(date.text)\n",
    "            try: \n",
    "                  #not working well\n",
    "                  d_parsed = datetime_parse(date.text)\n",
    "                  date_list.append(d_parsed[0])\n",
    "            except:\n",
    "                  date_list.append('None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n        Video Friday: DALL-E 2\\n    \\n</td>\n",
       "      <td>https://spectrum.ieee.org/video-friday-dall-e-2</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n        How a Parachute Accident Helped Ju...</td>\n",
       "      <td>https://spectrum.ieee.org/history-of-augmented...</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n        Volunteers Scramble to Preserve Uk...</td>\n",
       "      <td>https://spectrum.ieee.org/archiving-ukraine-cu...</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n        How the Wayback Machine Is Saving ...</td>\n",
       "      <td>https://spectrum.ieee.org/internet-archive-ukr...</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \\n\\n        Video Friday: DALL-E 2\\n    \\n   \n",
       "1  \\n\\n        How a Parachute Accident Helped Ju...   \n",
       "2  \\n\\n        Volunteers Scramble to Preserve Uk...   \n",
       "3  \\n\\n        How the Wayback Machine Is Saving ...   \n",
       "\n",
       "                                                 url       date  \n",
       "0    https://spectrum.ieee.org/video-friday-dall-e-2 2022-04-01  \n",
       "1  https://spectrum.ieee.org/history-of-augmented... 2022-04-01  \n",
       "2  https://spectrum.ieee.org/archiving-ukraine-cu... 2022-04-01  \n",
       "3  https://spectrum.ieee.org/internet-archive-ukr... 2022-04-01  "
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collected3 = pd.DataFrame(list(zip(title_list, url_list, date_list)), \n",
    "            columns=['title', 'url', 'date',])\n",
    "\n",
    "df_collected3.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to save to\n",
    "relevant_text = []\n",
    "\n",
    "#preping list fo dictionary conversion\n",
    "relevant_text.append(['title', 'url', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 22, 9, 46, 7, 425081)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_collected3.iterrows():\n",
    "      if row.date >= today - timedelta(days=30):\n",
    "            print('Fetching: ' + row.url)\n",
    "            response = requests.post(row.url) #headers=header)\n",
    "\n",
    "            html = soup(response.text, 'lxml')\n",
    "\n",
    "            objects = html.find_all('div', id=\"col-center\")\n",
    "            article_text = []\n",
    "            for obj in objects:\n",
    "                  paragraph_text = obj.find_all('p')\n",
    "                  for p in paragraph_text:\n",
    "                        article_text.append(p.text)\n",
    "                  #print(obj.text)\n",
    "            article_text = ' '.join(article_text)\n",
    "   \n",
    "            for word in search_terms['search_term']:\n",
    "                  if word in article_text:\n",
    "                        save = list(row)\n",
    "                        save.append(article_text)\n",
    "                        relevant_text.append(save)                  \n",
    "                        print('Search term found: ' +  word)\n",
    "                        break\n",
    "            else: \n",
    "                  pass\n",
    "\n",
    "            time.sleep(5)\n",
    "      else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  urllib.request import urlopen\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_1 = 'https://www.cna.org/centers/cna/sppp/rsp/russia-ai-archive#newsletters'\n",
    "base_url_2  = \"https://www.cna.org/centers/cna/cip/china/china-ai-newsletters/issue-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PDF \n",
    "# clean up PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 22, 15, 37, 18, 711935)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.now()\n",
    "\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 3, 23)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_to = today.date() - timedelta(days=30)\n",
    "back_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loop over base_url_1 and_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "ua = UserAgent(verify_ssl=False, cache=False)\n",
    "\n",
    "user_agent = ua.random\n",
    "header = {'User-Agent': user_agent}\n",
    "\n",
    "response = requests.get(base_url_2, headers=header)\n",
    "\n",
    "html = soup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "url_list = []\n",
    "date_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "13\n",
      "Issue 1: November 2, 2021 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-1.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 2: November 9, 2021 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-2.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 3: November 18, 2021 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-3.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 4: December 2, 2021 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-4.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 5: December 16, 2021 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-5.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 6: January 13, 2022 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-6.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 7: January 27, 2022 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-7.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 8: February 10, 2022 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-8.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 9: February 24, 2022 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-9.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 10: March 10, 2022 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-10.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 11: March 24, 2022 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-11.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 12: April 7, 2022 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-12.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 13: April 21, 2022 [ web | pdfÂ ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-13.pdf\n",
      "Successfully parsed with format: %B %d %Y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "objects = html.find_all('div', id='newsletters')#, href=True)#elid=True)#class_= 'clearfix')\n",
    "print(len(objects))\n",
    "\n",
    "for obj in objects:\n",
    "      items = obj.find_all('li')#, href=True)\n",
    "      print(len(items))\n",
    "      for i in items:\n",
    "            print(i.text)\n",
    "            title_list.append(i.text)\n",
    "            urls = i.find_all('a', href=True)\n",
    "            print(urls[1]['href'])\n",
    "            url_list.append('https://www.cna.org' + urls[1]['href'])\n",
    "\n",
    "            date = i.text\n",
    "            try:\n",
    "                  d_parsed = datetime_parse(date)\n",
    "                  date_list.append(d_parsed[0])\n",
    "            except:\n",
    "                  date_list.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Issue 1: November 2, 2021 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Issue 2: November 9, 2021 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Issue 3: November 18, 2021 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Issue 4: December 2, 2021 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Issue 5: December 16, 2021 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Issue 6: January 13, 2022 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Issue 7: January 27, 2022 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Issue 8: February 10, 2022 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Issue 9: February 24, 2022 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Issue 10: March 10, 2022 [ web | pdfÂ ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  \\\n",
       "0   Issue 1: November 2, 2021 [ web | pdfÂ ]   \n",
       "1   Issue 2: November 9, 2021 [ web | pdfÂ ]   \n",
       "2  Issue 3: November 18, 2021 [ web | pdfÂ ]   \n",
       "3   Issue 4: December 2, 2021 [ web | pdfÂ ]   \n",
       "4  Issue 5: December 16, 2021 [ web | pdfÂ ]   \n",
       "5   Issue 6: January 13, 2022 [ web | pdfÂ ]   \n",
       "6   Issue 7: January 27, 2022 [ web | pdfÂ ]   \n",
       "7  Issue 8: February 10, 2022 [ web | pdfÂ ]   \n",
       "8  Issue 9: February 24, 2022 [ web | pdfÂ ]   \n",
       "9    Issue 10: March 10, 2022 [ web | pdfÂ ]   \n",
       "\n",
       "                                                 url       date  \n",
       "0  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-02-01  \n",
       "1  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-09-01  \n",
       "2  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-11-18  \n",
       "3  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-02-01  \n",
       "4  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-12-16  \n",
       "5  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-01-13  \n",
       "6  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-01-27  \n",
       "7  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-10-01  \n",
       "8  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-02-24  \n",
       "9  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-10-01  "
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save collection to dataframe\n",
    "\n",
    "df_collected = pd.DataFrame(list(zip(title_list, url_list, date_list)), \n",
    "            columns=['title', 'url', 'date'])\n",
    "\n",
    "df_collected.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = 'CNA'\n",
    "pdf_dir = DATA_PATH + QUERY + 'pdfs'\n",
    "\n",
    "if not os.path.exists(pdf_dir): \n",
    "  os.makedirs(pdf_dir)\n",
    "\n",
    "have = set(os.listdir(pdf_dir))\n",
    "\n",
    "# Time out for requests\n",
    "timeout_secs = 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-8.pdf\n",
      "../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-8.pdf\n",
      "fetching https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-8.pdf into ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-8.pdf\n",
      "Fetching: https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-10.pdf\n",
      "../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-10.pdf\n",
      "fetching https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-10.pdf into ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-10.pdf\n",
      "Fetching: https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-12.pdf\n",
      "../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-12.pdf\n",
      "fetching https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-12.pdf into ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-12.pdf\n",
      "Fetching: https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-13.pdf\n",
      "../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-13.pdf\n",
      "fetching https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-13.pdf into ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-13.pdf\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_collected.iterrows():\n",
    "      if row.date >= today - timedelta(days=14):\n",
    "\n",
    "            basename = row.url.split('/')[-1]\n",
    "            fname = os.path.join(pdf_dir, basename)\n",
    "            print(fname)\n",
    "\n",
    "            # try:\n",
    "            if not basename in have:\n",
    "                  print('fetching %s into %s' % (row.url, fname))\n",
    "                  req = urlopen(row.url, None, timeout_secs)\n",
    "                  with open(fname, 'wb') as fp:\n",
    "                        shutil.copyfileobj(req, fp)\n",
    "            else:\n",
    "                  print('%s exists, skipping' % (fname, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for a program and a folder\n",
    "if not shutil.which('pdftotext'): # needs Python 3.3+\n",
    "  print('ERROR: you don\\'t have pdftotext installed. Install it first before calling this script')\n",
    "  sys.exit()\n",
    "\n",
    "if not os.path.exists(DATA_PATH + QUERY + '_txt'):\n",
    "      os.makedirs(DATA_PATH + QUERY + '_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying paths\n",
    "txt_dir = DATA_PATH + QUERY + '_txt'\n",
    "pdf_dir = DATA_PATH + QUERY + 'pdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/6 skipping AI-and-autonomy-in-Russia-Issue-31.pdf.txt, already exists.\n",
      "1/6 skipping AI-and-Autonomy-in-Russia-Issue-29-January-10-2022.pdf.txt, already exists.\n",
      "2/6 pdftotext ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-8.pdf ../data/CNA_txt/ChinaAI-Autonomy-Report-Issue-8.pdf.txt\n",
      "3/6 pdftotext ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-10.pdf ../data/CNA_txt/ChinaAI-Autonomy-Report-Issue-10.pdf.txt\n",
      "4/6 pdftotext ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-12.pdf ../data/CNA_txt/ChinaAI-Autonomy-Report-Issue-12.pdf.txt\n",
      "5/6 pdftotext ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-13.pdf ../data/CNA_txt/ChinaAI-Autonomy-Report-Issue-13.pdf.txt\n"
     ]
    }
   ],
   "source": [
    "# Note- should make a function in utils and import\n",
    "\n",
    "have = set(os.listdir(txt_dir))\n",
    "files = os.listdir(pdf_dir)\n",
    "\n",
    "for i,f in enumerate(files):\n",
    "\n",
    "  txt_basename = f + '.txt'\n",
    "  \n",
    "  if txt_basename in have:\n",
    "    print('%d/%d skipping %s, already exists.' % (i, len(files), txt_basename, ))\n",
    "    continue\n",
    "\n",
    "  pdf_path = os.path.join(pdf_dir, f)\n",
    "  txt_path = os.path.join(txt_dir, txt_basename)\n",
    "  \n",
    "  cmd = \"pdftotext %s %s\" % (pdf_path, txt_path)\n",
    "  os.system(cmd)\n",
    "\n",
    "  print('%d/%d %s' % (i, len(files), cmd))\n",
    "\n",
    "  # check output was made\n",
    "  if not os.path.isfile(txt_path):\n",
    "    # there was an error with converting the pdf\n",
    "    print('there was a problem with parsing %s to text, creating an empty text file.' % (pdf_path, ))\n",
    "    os.system('touch ' + txt_path) # create empty file, but it's a record of having tried to convert\n",
    "\n",
    "  time.sleep(0.01) #  for ctrl+c termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be nice I should parse these a bit and clean them up\n",
    "# THEN open txt files and insert into DF (naturally create DF)\n",
    "\n",
    "# See txts_to_csv notebook, easy task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response():\n",
    "    #base = test_link\n",
    "    final_url = base_url\n",
    "    #driver = webdriver.Chrome()\n",
    "    \n",
    "    with requests.get(final_url) as response:\n",
    "        #response = requests.get(url, headers=headers)\n",
    "        page_soup = soup(response.content, 'lxml')\n",
    "        return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seleniumwire import webdriver  # Import from seleniumwire\n",
    "\n",
    "# Create a new instance of the Firefox driver\n",
    "#driver = webdriver.Chrome()\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--user-agent=\"Mozilla/5.0 (Windows Phone 10.0; Android 4.2.1; Microsoft; Lumia 640 XL LTE) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Mobile Safari/537.36 Edge/12.10166\"')\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "user_agent = driver.execute_script(\"return navigator.userAgent;\")\n",
    "# Go to the Google home page\n",
    "\n",
    "driver.get(base_url_1)\n",
    "\n",
    "# with driver.requests.get(base_url) as response:\n",
    "#     #response = requests.get(url, headers=headers)\n",
    "#     page_soup = soup(response.content, 'lxml')\n",
    "#     #return page_soup\n",
    "\n",
    "# Access requests via the `requests` attribute\n",
    "for request in driver.requests:\n",
    "    if request.response:\n",
    "        print(\n",
    "            request.path,\n",
    "            request.response.status_code,\n",
    "            request.response.headers['Content-Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f045682951559cbc0979d5d7223b93f289f756c5241efdcb485f4eca938569a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('dri_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
