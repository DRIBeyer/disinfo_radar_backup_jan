{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usuals\n",
    "import numpy as np\n",
    "from numpy import quantile, where, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "# Scientific\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils as skl_utils\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Supporting\n",
    "\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = '../data/'\n",
    "OUTPUT = '../output_data/'\n",
    "MODEL_PATH = '../data/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index, Filename\n",
      "[(0, 'arxiv_disinformation.csv'), (1, 'arxiv_deepfake.csv'), (2, 'deepfake_txt.csv')]\n"
     ]
    }
   ],
   "source": [
    "# Check files in data folder\n",
    "datafiles = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f))]\n",
    "\n",
    "print('Index, Filename')\n",
    "print(list(zip([index for index, value in enumerate(datafiles)], datafiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv_deepfake.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a file name, can use\n",
    "filename = datafiles[1]\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "CONVERTERS = {'tokens': eval, 'published_parsed': eval, 'tags': eval, 'arxiv_primary_category': eval}\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename, converters=CONVERTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>arxiv_primary_category</th>\n",
       "      <th>tags</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2203.14315v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.14315v1</td>\n",
       "      <td>2022-03-27T14:25:52Z</td>\n",
       "      <td>[2022, 3, 27, 14, 25, 52, 6, 86, 0]</td>\n",
       "      <td>2022-03-27T14:25:52Z</td>\n",
       "      <td>[2022, 3, 27, 14, 25, 52, 6, 86, 0]</td>\n",
       "      <td>Adaptive Frequency Learning in Two-branch Face...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Face forgery has attracted increasing attentio...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>face forgery has attracted increasing attentio...</td>\n",
       "      <td>[face, forgery, attract, increase, attention, ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2203.13964v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.13964v1</td>\n",
       "      <td>2022-03-26T01:55:37Z</td>\n",
       "      <td>[2022, 3, 26, 1, 55, 37, 5, 85, 0]</td>\n",
       "      <td>2022-03-26T01:55:37Z</td>\n",
       "      <td>[2022, 3, 26, 1, 55, 37, 5, 85, 0]</td>\n",
       "      <td>Fusing Global and Local Features for Generaliz...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>With the development of the Generative Adversa...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>with the development of the generative adversa...</td>\n",
       "      <td>[development, generative, adversarial, network...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2203.12208v2</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.12208v2</td>\n",
       "      <td>2022-03-25T16:00:07Z</td>\n",
       "      <td>[2022, 3, 25, 16, 0, 7, 4, 84, 0]</td>\n",
       "      <td>2022-03-23T05:52:23Z</td>\n",
       "      <td>[2022, 3, 23, 5, 52, 23, 2, 82, 0]</td>\n",
       "      <td>Self-supervised Learning of Adversarial Exampl...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Recent studies in deepfake detection have yiel...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>recent studies in deepfake detection have yiel...</td>\n",
       "      <td>[recent, study, deepfake, detection, yield, pr...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  guidislink  \\\n",
       "0  http://arxiv.org/abs/2203.14315v1        True   \n",
       "1  http://arxiv.org/abs/2203.13964v1        True   \n",
       "2  http://arxiv.org/abs/2203.12208v2        True   \n",
       "\n",
       "                                link               updated  \\\n",
       "0  http://arxiv.org/abs/2203.14315v1  2022-03-27T14:25:52Z   \n",
       "1  http://arxiv.org/abs/2203.13964v1  2022-03-26T01:55:37Z   \n",
       "2  http://arxiv.org/abs/2203.12208v2  2022-03-25T16:00:07Z   \n",
       "\n",
       "                        updated_parsed             published  \\\n",
       "0  [2022, 3, 27, 14, 25, 52, 6, 86, 0]  2022-03-27T14:25:52Z   \n",
       "1   [2022, 3, 26, 1, 55, 37, 5, 85, 0]  2022-03-26T01:55:37Z   \n",
       "2    [2022, 3, 25, 16, 0, 7, 4, 84, 0]  2022-03-23T05:52:23Z   \n",
       "\n",
       "                      published_parsed  \\\n",
       "0  [2022, 3, 27, 14, 25, 52, 6, 86, 0]   \n",
       "1   [2022, 3, 26, 1, 55, 37, 5, 85, 0]   \n",
       "2   [2022, 3, 23, 5, 52, 23, 2, 82, 0]   \n",
       "\n",
       "                                               title  \\\n",
       "0  Adaptive Frequency Learning in Two-branch Face...   \n",
       "1  Fusing Global and Local Features for Generaliz...   \n",
       "2  Self-supervised Learning of Adversarial Exampl...   \n",
       "\n",
       "                                        title_detail  \\\n",
       "0  {'type': 'text/plain', 'language': None, 'base...   \n",
       "1  {'type': 'text/plain', 'language': None, 'base...   \n",
       "2  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                             summary  ...  \\\n",
       "0  Face forgery has attracted increasing attentio...  ...   \n",
       "1  With the development of the Generative Adversa...  ...   \n",
       "2  Recent studies in deepfake detection have yiel...  ...   \n",
       "\n",
       "                              arxiv_primary_category  \\\n",
       "0  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "1  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "2  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "\n",
       "                                                tags arxiv_affiliation  \\\n",
       "0  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "1  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "2  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "\n",
       "  arxiv_journal_ref arxiv_doi  \\\n",
       "0               NaN       NaN   \n",
       "1               NaN       NaN   \n",
       "2               NaN       NaN   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  face forgery has attracted increasing attentio...   \n",
       "1  with the development of the generative adversa...   \n",
       "2  recent studies in deepfake detection have yiel...   \n",
       "\n",
       "                                              tokens  year month_year category  \n",
       "0  [face, forgery, attract, increase, attention, ...  2022  [2022, 3]    cs.CV  \n",
       "1  [development, generative, adversarial, network...  2022  [2022, 3]    cs.CV  \n",
       "2  [recent, study, deepfake, detection, yield, pr...  2022  [2022, 3]    cs.CV  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data frame\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here for tests we will load a second df, not do a traditional train test split, as we want some sort of bias in the second set - to ensure outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If \"cleaning\" column kept from Preprocssing, can use that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(txt):\n",
    "        x = ' '.join(txt)\n",
    "        #x = [token.split('/')[0] for token in x] # use when we need lists with just these!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_merged'] = df['tokens'].dropna().apply(lambda x: join_tokens(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'development generative adversarial network gan deepfake aisynthesize image high quality human hardly distinguish real image imperative medium forensic develop detector expose accurately exist detection method show high performance generate image detection tend generalize poorly scenario synthetic image usually generate unseen model use unknown source datum work emphasize importance combine information whole image informative patch improve generalization ability aisynthesize image detection specifically design twobranch model combine global spatial information whole image local informative feature multiple patch select novel patch selection module multihead attention mechanism far utilize fuse global local feature collect highly diverse dataset synthesize model various object resolution evaluate model experimental result demonstrate high accuracy good generalization ability method detect generated image'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_merged'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with the development of the generative adversarial networks gans and deepfakes aisynthesized images are now of such high quality that humans can hardly distinguish them from real images it is imperative for media forensics to develop detectors to expose them accurately existing detection methods have shown high performance in generated images detection but they tend to generalize poorly in the realworld scenarios where the synthetic images are usually generated with unseen models using unknown source data in this work we emphasize the importance of combining information from the whole image and informative patches in improving the generalization ability of aisynthesized image detection specifically we design a twobranch model to combine global spatial information from the whole image and local informative features from multiple patches selected by a novel patch selection module multihead attention mechanism is further utilized to fuse the global and local features we collect a highly diverse dataset synthesized by 19 models with various objects and resolutions to evaluate our model experimental results demonstrate the high accuracy and good generalization ability of our method in detecting generated images'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaning'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2 Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates doc2vec vectors for each document in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, action_column, vector_size=100, learning_rate=0.02, epochs=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self._model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "        self.action_column = action_column\n",
    "\n",
    "    def fit(self, df_x, df_y=None):\n",
    "        tagged_x = [TaggedDocument(str(row[self.action_column]).split(), [index]) for index, row in df_x.iterrows()] # edit this: will not work on Chinese\n",
    "\n",
    "        model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers) # maybe want to try Word2Vec\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train(skl_utils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)\n",
    "            model.alpha -= self.learning_rate\n",
    "            model.min_alpha = model.alpha\n",
    "\n",
    "        self._model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_x):\n",
    "        return np.asmatrix(np.array([self._model.infer_vector(str(row[self.action_column]).split())\n",
    "                                     for index, row in df_x.iterrows()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2116480.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2282118.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2128742.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2376591.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 1971142.87it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2168955.32it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2027709.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2200125.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 4073403.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2163845.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 3453205.17it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 3993706.85it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2420428.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2439714.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2414067.22it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2635731.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 3950763.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 3950763.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 4302353.99it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 4469842.22it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2417243.62it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2355263.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 1712120.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 4480744.27it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 1820718.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 1903735.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 1921658.11it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2237643.30it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2370458.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2153698.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 4458993.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 1646151.57it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2729725.34it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2370458.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2376591.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2276462.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 1874597.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2367403.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2109190.76it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 3892171.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2059534.92it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 1715317.60it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2395182.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 1771557.52it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 4282296.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 4513771.87it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2430033.27it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 3985043.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2433251.86it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2085249.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initializing model\n",
    "doc2vec_tr = Doc2VecTransformer('tokens_merged', \n",
    "                              vector_size=300,#normally imo 150\n",
    "                              epochs= 50,\n",
    "                              )\n",
    "\n",
    "# Fitting\n",
    "#doc2vec_tr.fit(df)\n",
    "fitted = doc2vec_tr.fit(df)\n",
    "\n",
    "#Transforming\n",
    "doc2vec_vectors = fitted.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_vectors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv_deepfake.csv'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving as: ../data/models/arxiv_deepfake_doc_vectors.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/models/arxiv_deepfake_doc_vectors.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADD SAVE KV\n",
    "# ADD LOAD KV\n",
    "m = MODEL_PATH + filename.split('.')[0] + '_doc_vectors.pkl'\n",
    "print('Saving as: ' + m)\n",
    "\n",
    "joblib.dump(fitted, m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index, Model Name\n",
      "[(0, 'arxiv_disinformation_doc_vectors.pkl'), (1, 'arxiv_deepfake_doc_vectors.pkl'), (2, 'train_arxiv_deepfake_doc_vectors.pkl'), (3, 'arxiv_deepfake_svm_model.pkl'), (4, 'arxiv_deepfake_iso_model.pkl')]\n"
     ]
    }
   ],
   "source": [
    "# Check files in models folder\n",
    "models = [f for f in listdir(MODEL_PATH) if isfile(join(MODEL_PATH, f))]\n",
    "\n",
    "print('Index, Model Name')\n",
    "print(list(zip([index for index, value in enumerate(models)], models)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load a tfidf model / just for testing here\n",
    "model_name = datafiles[0]\n",
    "\n",
    "doc2vec_vectors = joblib.load(MODEL_PATH + model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then transform again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit model\n",
    "'''\n",
    "Adjust nu hyperparameter to, simplifing, \n",
    "increase/decrease \"novelty\" sensitivity. \n",
    "It is very high now = less outliers\n",
    "'''\n",
    "\n",
    "model = OneClassSVM(kernel = 'rbf', \n",
    "                  gamma = 'scale', \n",
    "                  nu = 0.001).fit(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/arxiv_deepfake_svm_model.pkl']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, MODEL_PATH + filename.split('.')[0] + \"_svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = model.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 5\n"
     ]
    }
   ],
   "source": [
    "# Filter outlier index\n",
    "outlier_index = where(y_pred == -1)\n",
    "indexes = list(outlier_index[0])\n",
    "\n",
    "print('Outliers: ' + str(len(indexes)))\n",
    "\n",
    "# Un-used, for inspection\n",
    "#outlier_values = doc2vec_vectors.iloc[outlier_index]\n",
    "#outlier_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with just outliers\n",
    "\n",
    "df_misclass = df[df.index.isin(indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>tags</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>category</th>\n",
       "      <th>tokens_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://arxiv.org/abs/2203.06825v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.06825v1</td>\n",
       "      <td>2022-03-14T02:44:56Z</td>\n",
       "      <td>[2022, 3, 14, 2, 44, 56, 0, 73, 0]</td>\n",
       "      <td>2022-03-14T02:44:56Z</td>\n",
       "      <td>[2022, 3, 14, 2, 44, 56, 0, 73, 0]</td>\n",
       "      <td>Fairness Evaluation in Deepfake Detection Mode...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Fairness of deepfake detectors in the presence...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fairness of deepfake detectors in the presence...</td>\n",
       "      <td>[fairness, deepfake, detector, presence, anoma...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>fairness deepfake detector presence anomaly we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>http://arxiv.org/abs/2110.01640v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2110.01640v1</td>\n",
       "      <td>2021-10-04T18:02:56Z</td>\n",
       "      <td>[2021, 10, 4, 18, 2, 56, 0, 277, 0]</td>\n",
       "      <td>2021-10-04T18:02:56Z</td>\n",
       "      <td>[2021, 10, 4, 18, 2, 56, 0, 277, 0]</td>\n",
       "      <td>An Experimental Evaluation on Deepfake Detecti...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Significant advances in deep learning have obt...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>significant advances in deep learning have obt...</td>\n",
       "      <td>[significant, advance, deep, learning, obtain,...</td>\n",
       "      <td>2021</td>\n",
       "      <td>[2021, 10]</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>significant advance deep learning obtain hallm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>http://arxiv.org/abs/2012.10580v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2012.10580v1</td>\n",
       "      <td>2020-12-19T03:02:15Z</td>\n",
       "      <td>[2020, 12, 19, 3, 2, 15, 5, 354, 0]</td>\n",
       "      <td>2020-12-19T03:02:15Z</td>\n",
       "      <td>[2020, 12, 19, 3, 2, 15, 5, 354, 0]</td>\n",
       "      <td>Identifying Invariant Texture Violation for Ro...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Existing deepfake detection methods have repor...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>existing deepfake detection methods have repor...</td>\n",
       "      <td>[exist, deepfake, detection, method, report, p...</td>\n",
       "      <td>2020</td>\n",
       "      <td>[2020, 12]</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>exist deepfake detection method report promisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  guidislink  \\\n",
       "8    http://arxiv.org/abs/2203.06825v1        True   \n",
       "46   http://arxiv.org/abs/2110.01640v1        True   \n",
       "128  http://arxiv.org/abs/2012.10580v1        True   \n",
       "\n",
       "                                  link               updated  \\\n",
       "8    http://arxiv.org/abs/2203.06825v1  2022-03-14T02:44:56Z   \n",
       "46   http://arxiv.org/abs/2110.01640v1  2021-10-04T18:02:56Z   \n",
       "128  http://arxiv.org/abs/2012.10580v1  2020-12-19T03:02:15Z   \n",
       "\n",
       "                          updated_parsed             published  \\\n",
       "8     [2022, 3, 14, 2, 44, 56, 0, 73, 0]  2022-03-14T02:44:56Z   \n",
       "46   [2021, 10, 4, 18, 2, 56, 0, 277, 0]  2021-10-04T18:02:56Z   \n",
       "128  [2020, 12, 19, 3, 2, 15, 5, 354, 0]  2020-12-19T03:02:15Z   \n",
       "\n",
       "                        published_parsed  \\\n",
       "8     [2022, 3, 14, 2, 44, 56, 0, 73, 0]   \n",
       "46   [2021, 10, 4, 18, 2, 56, 0, 277, 0]   \n",
       "128  [2020, 12, 19, 3, 2, 15, 5, 354, 0]   \n",
       "\n",
       "                                                 title  \\\n",
       "8    Fairness Evaluation in Deepfake Detection Mode...   \n",
       "46   An Experimental Evaluation on Deepfake Detecti...   \n",
       "128  Identifying Invariant Texture Violation for Ro...   \n",
       "\n",
       "                                          title_detail  \\\n",
       "8    {'type': 'text/plain', 'language': None, 'base...   \n",
       "46   {'type': 'text/plain', 'language': None, 'base...   \n",
       "128  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                               summary  ...  \\\n",
       "8    Fairness of deepfake detectors in the presence...  ...   \n",
       "46   Significant advances in deep learning have obt...  ...   \n",
       "128  Existing deepfake detection methods have repor...  ...   \n",
       "\n",
       "                                                  tags arxiv_affiliation  \\\n",
       "8    [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "46   [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "128  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "\n",
       "    arxiv_journal_ref arxiv_doi  \\\n",
       "8                 NaN       NaN   \n",
       "46                NaN       NaN   \n",
       "128               NaN       NaN   \n",
       "\n",
       "                                              cleaning  \\\n",
       "8    fairness of deepfake detectors in the presence...   \n",
       "46   significant advances in deep learning have obt...   \n",
       "128  existing deepfake detection methods have repor...   \n",
       "\n",
       "                                                tokens  year  month_year  \\\n",
       "8    [fairness, deepfake, detector, presence, anoma...  2022   [2022, 3]   \n",
       "46   [significant, advance, deep, learning, obtain,...  2021  [2021, 10]   \n",
       "128  [exist, deepfake, detection, method, report, p...  2020  [2020, 12]   \n",
       "\n",
       "    category                                      tokens_merged  \n",
       "8      cs.CV  fairness deepfake detector presence anomaly we...  \n",
       "46     cs.CV  significant advance deep learning obtain hallm...  \n",
       "128    cs.CV  exist deepfake detection method report promisi...  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect dataframe\n",
    "\n",
    "df_misclass.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add compare function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to getting outliers using SVM model but diffferent criteria (more of a % than a absolute value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.score_samples(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10631984670769612\n"
     ]
    }
   ],
   "source": [
    "# Change treshhold as needed\n",
    "\n",
    "thresh = quantile(scores, 0.03)\n",
    "print(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# getting indexes\n",
    "\n",
    "index = where(scores<=thresh)\n",
    "index = list(index[0])\n",
    "print(len(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>tags</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>category</th>\n",
       "      <th>tokens_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://arxiv.org/abs/2203.06825v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.06825v1</td>\n",
       "      <td>2022-03-14T02:44:56Z</td>\n",
       "      <td>[2022, 3, 14, 2, 44, 56, 0, 73, 0]</td>\n",
       "      <td>2022-03-14T02:44:56Z</td>\n",
       "      <td>[2022, 3, 14, 2, 44, 56, 0, 73, 0]</td>\n",
       "      <td>Fairness Evaluation in Deepfake Detection Mode...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Fairness of deepfake detectors in the presence...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fairness of deepfake detectors in the presence...</td>\n",
       "      <td>[fairness, deepfake, detector, presence, anoma...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>fairness deepfake detector presence anomaly we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>http://arxiv.org/abs/2110.01640v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2110.01640v1</td>\n",
       "      <td>2021-10-04T18:02:56Z</td>\n",
       "      <td>[2021, 10, 4, 18, 2, 56, 0, 277, 0]</td>\n",
       "      <td>2021-10-04T18:02:56Z</td>\n",
       "      <td>[2021, 10, 4, 18, 2, 56, 0, 277, 0]</td>\n",
       "      <td>An Experimental Evaluation on Deepfake Detecti...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Significant advances in deep learning have obt...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>significant advances in deep learning have obt...</td>\n",
       "      <td>[significant, advance, deep, learning, obtain,...</td>\n",
       "      <td>2021</td>\n",
       "      <td>[2021, 10]</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>significant advance deep learning obtain hallm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>http://arxiv.org/abs/2103.09396v3</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2103.09396v3</td>\n",
       "      <td>2021-10-03T01:05:56Z</td>\n",
       "      <td>[2021, 10, 3, 1, 5, 56, 6, 276, 0]</td>\n",
       "      <td>2021-03-17T01:48:34Z</td>\n",
       "      <td>[2021, 3, 17, 1, 48, 34, 2, 76, 0]</td>\n",
       "      <td>Pros and Cons of GAN Evaluation Measures: New ...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>This work is an update of a previous paper on ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this work is an update of a previous paper on ...</td>\n",
       "      <td>[work, update, previous, paper, topic, publish...</td>\n",
       "      <td>2021</td>\n",
       "      <td>[2021, 3]</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>work update previous paper topic publish year ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  guidislink  \\\n",
       "8    http://arxiv.org/abs/2203.06825v1        True   \n",
       "46   http://arxiv.org/abs/2110.01640v1        True   \n",
       "106  http://arxiv.org/abs/2103.09396v3        True   \n",
       "\n",
       "                                  link               updated  \\\n",
       "8    http://arxiv.org/abs/2203.06825v1  2022-03-14T02:44:56Z   \n",
       "46   http://arxiv.org/abs/2110.01640v1  2021-10-04T18:02:56Z   \n",
       "106  http://arxiv.org/abs/2103.09396v3  2021-10-03T01:05:56Z   \n",
       "\n",
       "                          updated_parsed             published  \\\n",
       "8     [2022, 3, 14, 2, 44, 56, 0, 73, 0]  2022-03-14T02:44:56Z   \n",
       "46   [2021, 10, 4, 18, 2, 56, 0, 277, 0]  2021-10-04T18:02:56Z   \n",
       "106   [2021, 10, 3, 1, 5, 56, 6, 276, 0]  2021-03-17T01:48:34Z   \n",
       "\n",
       "                        published_parsed  \\\n",
       "8     [2022, 3, 14, 2, 44, 56, 0, 73, 0]   \n",
       "46   [2021, 10, 4, 18, 2, 56, 0, 277, 0]   \n",
       "106   [2021, 3, 17, 1, 48, 34, 2, 76, 0]   \n",
       "\n",
       "                                                 title  \\\n",
       "8    Fairness Evaluation in Deepfake Detection Mode...   \n",
       "46   An Experimental Evaluation on Deepfake Detecti...   \n",
       "106  Pros and Cons of GAN Evaluation Measures: New ...   \n",
       "\n",
       "                                          title_detail  \\\n",
       "8    {'type': 'text/plain', 'language': None, 'base...   \n",
       "46   {'type': 'text/plain', 'language': None, 'base...   \n",
       "106  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                               summary  ...  \\\n",
       "8    Fairness of deepfake detectors in the presence...  ...   \n",
       "46   Significant advances in deep learning have obt...  ...   \n",
       "106  This work is an update of a previous paper on ...  ...   \n",
       "\n",
       "                                                  tags arxiv_affiliation  \\\n",
       "8    [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "46   [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "106  [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...               NaN   \n",
       "\n",
       "    arxiv_journal_ref arxiv_doi  \\\n",
       "8                 NaN       NaN   \n",
       "46                NaN       NaN   \n",
       "106               NaN       NaN   \n",
       "\n",
       "                                              cleaning  \\\n",
       "8    fairness of deepfake detectors in the presence...   \n",
       "46   significant advances in deep learning have obt...   \n",
       "106  this work is an update of a previous paper on ...   \n",
       "\n",
       "                                                tokens  year  month_year  \\\n",
       "8    [fairness, deepfake, detector, presence, anoma...  2022   [2022, 3]   \n",
       "46   [significant, advance, deep, learning, obtain,...  2021  [2021, 10]   \n",
       "106  [work, update, previous, paper, topic, publish...  2021   [2021, 3]   \n",
       "\n",
       "    category                                      tokens_merged  \n",
       "8      cs.CV  fairness deepfake detector presence anomaly we...  \n",
       "46     cs.CV  significant advance deep learning obtain hallm...  \n",
       "106    cs.LG  work update previous paper topic publish year ...  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating second df\n",
    "df_misclass_2 = df[df.index.isin(index)]\n",
    "\n",
    "# And viewing it\n",
    "df_misclass_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'contamination': 0.01, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 100, 'n_jobs': None, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "iso_model = IsolationForest(n_estimators=100,\n",
    "                  max_samples='auto',\n",
    "                  contamination=float(0.01),\n",
    "                  random_state=42\n",
    "                  )\n",
    "\n",
    "# Fitting model\n",
    "iso_model.fit(doc2vec_vectors)\n",
    "\n",
    "print(iso_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/arxiv_deepfake_iso_model.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(iso_model, MODEL_PATH + filename.split('.')[0] + \"_iso_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "data['scores'] = iso_model.decision_function(doc2vec_vectors)\n",
    "\n",
    "data['anomaly_score'] = iso_model.predict(doc2vec_vectors) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>anomaly_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.021069</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>-0.001319</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>-0.040489</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-0.140542</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>-0.131213</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       scores  anomaly_score\n",
       "89  -0.021069             -1\n",
       "184 -0.001319             -1\n",
       "283 -0.040489             -1\n",
       "295 -0.140542             -1\n",
       "322 -0.131213             -1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['anomaly_score']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 5\n"
     ]
    }
   ],
   "source": [
    "outlier_index = where(data['anomaly_score'] == -1)\n",
    "indexes = list(outlier_index[0])\n",
    "\n",
    "print('Outliers: ' + str(len(indexes)))\n",
    "\n",
    "isolation_misclass = df[df.index.isin(indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89     TAR: Generalized Forensic Framework to Detect ...\n",
       "184             Detecting Deepfakes with Metric Learning\n",
       "283    FakeAVCeleb: A Novel Audio-Video Multimodal De...\n",
       "295    FFR_FD: Effective and Fast Detection of DeepFa...\n",
       "322    MagDR: Mask-guided Detection and Reconstructio...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolation_misclass.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Simularity Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize, fit and predict\n",
    "auto_encoder = MLPRegressor(hidden_layer_sizes=(\n",
    "                                                 600,\n",
    "                                                 150, \n",
    "                                                 600,\n",
    "                                               ))\n",
    "\n",
    "auto_encoder.fit(doc2vec_vectors, doc2vec_vectors)\n",
    "\n",
    "predicted_vectors = auto_encoder.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual loss\n",
    "pd.DataFrame(auto_encoder.loss_curve_).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_consine_similarity(tupple):\n",
    "    return tupple[1]\n",
    "\n",
    "def get_computed_similarities(vectors, predicted_vectors, reverse=False):\n",
    "    data_size = len(df)\n",
    "    cosine_similarities = []\n",
    "    for i in range(data_size):\n",
    "        cosine_sim_val = (1 - cosine(vectors[i], predicted_vectors[i]))\n",
    "        cosine_similarities.append((i, cosine_sim_val))\n",
    "\n",
    "    return sorted(cosine_similarities, key=key_consine_similarity, reverse=reverse)\n",
    "\n",
    "def display_top_n(sorted_cosine_similarities, n=5):\n",
    "    for i in range(n):\n",
    "        index, consine_sim_val = sorted_cosine_similarities[i]\n",
    "        print('Title: ', df.iloc[index, 7])\n",
    "        print('ID: ', df.iloc[index, 0])  \n",
    "        print('Cosine Sim Val :', consine_sim_val)\n",
    "        print('---------------------------------')\n",
    "\n",
    "# add function to sort by percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify how many 'outliers' you want to see\n",
    "N = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top n unique')\n",
    "\n",
    "sorted_cosine_similarities = get_computed_similarities(vectors=doc2vec_vectors, predicted_vectors=predicted_vectors)\n",
    "\n",
    "display_top_n(sorted_cosine_similarities=sorted_cosine_similarities, n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the cosines - will revise during first test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn histogram\n",
    "# Can use to adjust the N above (or percent, once we have that function) to see the low cluster\n",
    "\n",
    "sns.distplot(losses, hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "\n",
    "# # Add labels\n",
    "# plt.title('Title')\n",
    "# plt.xlabel('Label x')\n",
    "# plt.ylabel('Label y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIRC not fully functional yet - for more Cosine work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_unique_index, cosine_sim_val = sorted_cosine_similarities[0]\n",
    "print(most_unique_index)\n",
    "most_unique_plot =df.iloc[most_unique_index, 9] # index here matters!\n",
    "most_unique_words_counter = Counter(preprocess_string(most_unique_plot))\n",
    "print(most_unique_words_counter)\n",
    "\n",
    "# intersected_common_word_counter = common_word_counter & most_unique_words_counter\n",
    "\n",
    "# intersected_common_words = [word[0] for word in intersected_common_word_counter.items()]\n",
    "# intersected_common_word_counts = [word[1] for word in intersected_common_word_counter.items()]\n",
    "\n",
    "# intersected_common_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73afed2e6e5cae272ca1e451939a06651cb7194d426a79bc157d2d09ec23572e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('sab22_venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
