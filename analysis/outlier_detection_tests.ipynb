{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usuals\n",
    "import numpy as np\n",
    "from numpy import quantile, where, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "# Scientific\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils as skl_utils\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Supporting\n",
    "\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = '../data/'\n",
    "OUTPUT = '../output_data/'\n",
    "MODEL_PATH = '../data/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index, Filename\n",
      "[(0, 'arxiv_disinformation.csv'), (1, 'arxiv_deepfake.csv'), (2, 'deepfake_txt.csv')]\n"
     ]
    }
   ],
   "source": [
    "# Check files in data folder\n",
    "datafiles = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f))]\n",
    "\n",
    "print('Index, Filename')\n",
    "print(list(zip([index for index, value in enumerate(datafiles)], datafiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv_deepfake.csv'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a file name, can use\n",
    "filename = datafiles[1]\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "CONVERTERS = {'tokens': eval, 'published_parsed': eval, 'tags': eval, 'arxiv_primary_category': eval}\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename, converters=CONVERTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>arxiv_primary_category</th>\n",
       "      <th>tags</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2203.14315v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.14315v1</td>\n",
       "      <td>2022-03-27T14:25:52Z</td>\n",
       "      <td>[2022, 3, 27, 14, 25, 52, 6, 86, 0]</td>\n",
       "      <td>2022-03-27T14:25:52Z</td>\n",
       "      <td>[2022, 3, 27, 14, 25, 52, 6, 86, 0]</td>\n",
       "      <td>Adaptive Frequency Learning in Two-branch Face...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Face forgery has attracted increasing attentio...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>face forgery has attracted increasing attentio...</td>\n",
       "      <td>[face, forgery, attract, increase, attention, ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2203.13964v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.13964v1</td>\n",
       "      <td>2022-03-26T01:55:37Z</td>\n",
       "      <td>[2022, 3, 26, 1, 55, 37, 5, 85, 0]</td>\n",
       "      <td>2022-03-26T01:55:37Z</td>\n",
       "      <td>[2022, 3, 26, 1, 55, 37, 5, 85, 0]</td>\n",
       "      <td>Fusing Global and Local Features for Generaliz...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>With the development of the Generative Adversa...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>with the development of the generative adversa...</td>\n",
       "      <td>[development, generative, adversarial, network...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2203.12208v2</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.12208v2</td>\n",
       "      <td>2022-03-25T16:00:07Z</td>\n",
       "      <td>[2022, 3, 25, 16, 0, 7, 4, 84, 0]</td>\n",
       "      <td>2022-03-23T05:52:23Z</td>\n",
       "      <td>[2022, 3, 23, 5, 52, 23, 2, 82, 0]</td>\n",
       "      <td>Self-supervised Learning of Adversarial Exampl...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Recent studies in deepfake detection have yiel...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>recent studies in deepfake detection have yiel...</td>\n",
       "      <td>[recent, study, deepfake, detection, yield, pr...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  guidislink  \\\n",
       "0  http://arxiv.org/abs/2203.14315v1        True   \n",
       "1  http://arxiv.org/abs/2203.13964v1        True   \n",
       "2  http://arxiv.org/abs/2203.12208v2        True   \n",
       "\n",
       "                                link               updated  \\\n",
       "0  http://arxiv.org/abs/2203.14315v1  2022-03-27T14:25:52Z   \n",
       "1  http://arxiv.org/abs/2203.13964v1  2022-03-26T01:55:37Z   \n",
       "2  http://arxiv.org/abs/2203.12208v2  2022-03-25T16:00:07Z   \n",
       "\n",
       "                        updated_parsed             published  \\\n",
       "0  [2022, 3, 27, 14, 25, 52, 6, 86, 0]  2022-03-27T14:25:52Z   \n",
       "1   [2022, 3, 26, 1, 55, 37, 5, 85, 0]  2022-03-26T01:55:37Z   \n",
       "2    [2022, 3, 25, 16, 0, 7, 4, 84, 0]  2022-03-23T05:52:23Z   \n",
       "\n",
       "                      published_parsed  \\\n",
       "0  [2022, 3, 27, 14, 25, 52, 6, 86, 0]   \n",
       "1   [2022, 3, 26, 1, 55, 37, 5, 85, 0]   \n",
       "2   [2022, 3, 23, 5, 52, 23, 2, 82, 0]   \n",
       "\n",
       "                                               title  \\\n",
       "0  Adaptive Frequency Learning in Two-branch Face...   \n",
       "1  Fusing Global and Local Features for Generaliz...   \n",
       "2  Self-supervised Learning of Adversarial Exampl...   \n",
       "\n",
       "                                        title_detail  \\\n",
       "0  {'type': 'text/plain', 'language': None, 'base...   \n",
       "1  {'type': 'text/plain', 'language': None, 'base...   \n",
       "2  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                             summary  ...  \\\n",
       "0  Face forgery has attracted increasing attentio...  ...   \n",
       "1  With the development of the Generative Adversa...  ...   \n",
       "2  Recent studies in deepfake detection have yiel...  ...   \n",
       "\n",
       "                              arxiv_primary_category  \\\n",
       "0  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "1  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "2  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "\n",
       "                                                tags arxiv_affiliation  \\\n",
       "0  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "1  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "2  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "\n",
       "  arxiv_journal_ref arxiv_doi  \\\n",
       "0               NaN       NaN   \n",
       "1               NaN       NaN   \n",
       "2               NaN       NaN   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  face forgery has attracted increasing attentio...   \n",
       "1  with the development of the generative adversa...   \n",
       "2  recent studies in deepfake detection have yiel...   \n",
       "\n",
       "                                              tokens  year month_year category  \n",
       "0  [face, forgery, attract, increase, attention, ...  2022  [2022, 3]    cs.CV  \n",
       "1  [development, generative, adversarial, network...  2022  [2022, 3]    cs.CV  \n",
       "2  [recent, study, deepfake, detection, yield, pr...  2022  [2022, 3]    cs.CV  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data frame\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here for tests we will load a second df, not do a traditional train test split, as we want some sort of bias in the second set - to ensure outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'guidislink', 'link', 'updated', 'updated_parsed', 'published',\n",
       "       'published_parsed', 'title', 'title_detail', 'summary',\n",
       "       'summary_detail', 'authors', 'author_detail', 'author', 'arxiv_comment',\n",
       "       'links', 'arxiv_primary_category', 'tags', 'arxiv_affiliation',\n",
       "       'arxiv_journal_ref', 'arxiv_doi', 'cleaning', 'tokens', 'year',\n",
       "       'month_year', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27T14:25:52Z\n",
      "2018-06-07T19:36:09Z\n"
     ]
    }
   ],
   "source": [
    "print(df.published.max())\n",
    "print(df.published.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df['published'] > '2022-01-01T14:25:52Z']\n",
    "df_train = df[df['published'] < '2022-01-01T14:25:52Z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse trainin\n",
    "df_test = df[df['published'] < '2020-01-01T14:25:52Z']\n",
    "df_train = df[df['published'] > '2020-01-01T14:25:52Z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n",
      "44\n",
      "394\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "\n",
    "print(len(df_test))\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If \"cleaning\" column kept from Preprocssing, can use that column (but has ALL words, eg also stopwords and all POSs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(txt):\n",
    "        x = ' '.join(txt)\n",
    "        #x = [token.split('/')[0] for token in x] # use when we need lists with just these!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20685/356501187.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['tokens_merged'] = df_test['tokens'].dropna().apply(lambda x: join_tokens(x))\n"
     ]
    }
   ],
   "source": [
    "df_test['tokens_merged'] = df_test['tokens'].dropna().apply(lambda x: join_tokens(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20685/2420611658.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['tokens_merged'] = df_train['tokens'].dropna().apply(lambda x: join_tokens(x))\n"
     ]
    }
   ],
   "source": [
    "df_train['tokens_merged'] = df_train['tokens'].dropna().apply(lambda x: join_tokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Doc2 Vec and SVM to df_train first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2 Vec for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates doc2vec vectors for each document in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, action_column, vector_size=100, learning_rate=0.02, epochs=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self._model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "        self.action_column = action_column\n",
    "\n",
    "    def fit(self, df_x, df_y=None):\n",
    "        tagged_x = [TaggedDocument(str(row[self.action_column]).split(), [index]) for index, row in df_x.iterrows()] # edit this: will not work on Chinese\n",
    "\n",
    "        model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers) # maybe want to try Word2Vec\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train(skl_utils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)\n",
    "            model.alpha -= self.learning_rate\n",
    "            model.min_alpha = model.alpha\n",
    "\n",
    "        self._model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_x):\n",
    "        return np.asmatrix(np.array([self._model.infer_vector(str(row[self.action_column]).split())\n",
    "                                     for index, row in df_x.iterrows()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 1735877.92it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4040478.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3906751.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3982062.11it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3508610.99it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2408973.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2183032.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2631458.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3631990.72it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2295216.36it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 1782692.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 977265.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2266880.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2239235.47it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 1850566.38it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2282535.60it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2221177.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 1167059.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4237322.50it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2126841.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2314503.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2327543.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4050381.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3906751.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2292032.98it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3888366.53it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2143392.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 1530144.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2507671.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2691458.92it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 1385210.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2507671.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3772958.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3435666.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2206349.50it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4183685.51it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4259164.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3738813.97it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3888366.53it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2010408.49it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 1976741.36it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4011057.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4395095.15it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2107851.76it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4162609.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4090484.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 4194304.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2344050.75it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 2796202.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394/394 [00:00<00:00, 3233964.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initializing model\n",
    "doc2vec_tr = Doc2VecTransformer('tokens_merged', \n",
    "                              vector_size=150,#normally imo 150\n",
    "                              epochs= 50,\n",
    "                              )\n",
    "\n",
    "# Fitting\n",
    "#doc2vec_tr.fit(df_train)\n",
    "fitted = doc2vec_tr.fit(df_train)\n",
    "\n",
    "#Transforming\n",
    "doc2vec_vectors = fitted.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc2VecTransformer(action_column='tokens_merged', epochs=50, vector_size=150)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_vectors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv_deepfake.csv'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving as: ../data/models/train_arxiv_deepfake_doc_vectors.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/models/train_arxiv_deepfake_doc_vectors.pkl']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADD SAVE KV\n",
    "# ADD LOAD KV\n",
    "m = MODEL_PATH + 'train_' + filename.split('.')[0] + '_doc_vectors.pkl'\n",
    "print('Saving as: ' + m)\n",
    "\n",
    "joblib.dump(doc2vec_tr, m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit model\n",
    "'''\n",
    "Adjust nu hyperparameter to, simplifing, \n",
    "increase/decrease \"novelty\" sensitivity. \n",
    "It is very high now = less outliers\n",
    "'''\n",
    "\n",
    "model = OneClassSVM(kernel = 'rbf', \n",
    "                  gamma = 'scale', \n",
    "                  nu = 0.001).fit(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/train_arxiv_deepfake_svm_model.pkl']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, MODEL_PATH + 'train_' + filename.split('.')[0] + \"_svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_pred = model.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 6\n"
     ]
    }
   ],
   "source": [
    "# Filter outlier index\n",
    "outlier_index = where(y_pred == -1)\n",
    "indexes = list(outlier_index[0])\n",
    "\n",
    "print('Outliers: ' + str(len(indexes)))\n",
    "\n",
    "# Un-used, for inspection\n",
    "#outlier_values = doc2vec_vectors.iloc[outlier_index]\n",
    "#outlier_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index, Model Name\n",
      "[(0, 'arxiv_disinformation_doc_vectors.pkl'), (1, 'arxiv_deepfake_doc_vectors.pkl'), (2, 'train_arxiv_deepfake_doc_vectors.pkl'), (3, 'arxiv_deepfake_svm_model.pkl'), (4, 'arxiv_deepfake_iso_model.pkl'), (5, 'train_arxiv_deepfake_svm_model.pkl'), (6, 'train_arxiv_deepfake_iso_model.pkl')]\n"
     ]
    }
   ],
   "source": [
    "# Check files in data folder\n",
    "models = [f for f in listdir(MODEL_PATH) if isfile(join(MODEL_PATH, f))]\n",
    "\n",
    "print('Index, Model Name')\n",
    "print(list(zip([index for index, value in enumerate(models)], models)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_arxiv_deepfake_doc_vectors.pkl\n",
      "train_arxiv_deepfake_svm_model.pkl\n",
      "train_arxiv_deepfake_iso_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# to load a model\n",
    "vector_name = models[2]\n",
    "print(vector_name)\n",
    "model_name = models[5]\n",
    "print(model_name)\n",
    "iso_model_name = models[6]\n",
    "print(iso_model_name)\n",
    "\n",
    "vectorizer = joblib.load(MODEL_PATH + vector_name)\n",
    "svm_model = joblib.load(MODEL_PATH + model_name)\n",
    "iso_model = joblib.load(MODEL_PATH + iso_model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>tags</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>category</th>\n",
       "      <th>tokens_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2203.14315v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.14315v1</td>\n",
       "      <td>2022-03-27T14:25:52Z</td>\n",
       "      <td>[2022, 3, 27, 14, 25, 52, 6, 86, 0]</td>\n",
       "      <td>2022-03-27T14:25:52Z</td>\n",
       "      <td>[2022, 3, 27, 14, 25, 52, 6, 86, 0]</td>\n",
       "      <td>Adaptive Frequency Learning in Two-branch Face...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Face forgery has attracted increasing attentio...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>face forgery has attracted increasing attentio...</td>\n",
       "      <td>[face, forgery, attract, increase, attention, ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>face forgery attract increase attention recent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  guidislink  \\\n",
       "0  http://arxiv.org/abs/2203.14315v1        True   \n",
       "\n",
       "                                link               updated  \\\n",
       "0  http://arxiv.org/abs/2203.14315v1  2022-03-27T14:25:52Z   \n",
       "\n",
       "                        updated_parsed             published  \\\n",
       "0  [2022, 3, 27, 14, 25, 52, 6, 86, 0]  2022-03-27T14:25:52Z   \n",
       "\n",
       "                      published_parsed  \\\n",
       "0  [2022, 3, 27, 14, 25, 52, 6, 86, 0]   \n",
       "\n",
       "                                               title  \\\n",
       "0  Adaptive Frequency Learning in Two-branch Face...   \n",
       "\n",
       "                                        title_detail  \\\n",
       "0  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                             summary  ...  \\\n",
       "0  Face forgery has attracted increasing attentio...  ...   \n",
       "\n",
       "                                                tags arxiv_affiliation  \\\n",
       "0  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "\n",
       "  arxiv_journal_ref arxiv_doi  \\\n",
       "0               NaN       NaN   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  face forgery has attracted increasing attentio...   \n",
       "\n",
       "                                              tokens  year month_year  \\\n",
       "0  [face, forgery, attract, increase, attention, ...  2022  [2022, 3]   \n",
       "\n",
       "  category                                      tokens_merged  \n",
       "0    cs.CV  face forgery attract increase attention recent...  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 623477.62it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 723723.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 762600.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 373581.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 726572.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 464859.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 533379.70it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 750200.72it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 683516.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 473203.53it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 428188.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 441505.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 492131.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 507003.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1246955.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1139193.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1255437.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1160687.90it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1175473.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1356980.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1255437.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1272754.32it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1230329.17it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 603102.54it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1153433.60it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 946407.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1255437.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1072961.49it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1066759.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1098508.19it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 775417.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 866429.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1299643.49it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 827575.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1175473.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 838860.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1008466.54it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 986895.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1132204.76it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1175473.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1183008.82it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 904653.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 726572.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1014007.56it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1025274.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 759462.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1048576.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 723723.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1066759.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:00<00:00, 1072961.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fit testing data\n",
    "fitted = vectorizer.fit(df_test)#.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = fitted.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 33\n"
     ]
    }
   ],
   "source": [
    "# Approach 1: absolute y_pred wrong\n",
    "\n",
    "outlier_index = where(y_pred == -1)\n",
    "indexes = list(outlier_index[0])\n",
    "\n",
    "print('Outliers: ' + str(len(indexes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: most difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores = model.score_samples(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00043108544561837027\n"
     ]
    }
   ],
   "source": [
    "# Change treshhold as needed\n",
    "\n",
    "thresh = quantile(scores, .1)\n",
    "print(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 5\n"
     ]
    }
   ],
   "source": [
    "# getting indexes\n",
    "\n",
    "index = where(scores<=thresh)\n",
    "index = list(index[0])\n",
    "print('Outliers: ' + str(len(index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation on train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "data['scores'] = iso_model.decision_function(test_vectors)\n",
    "\n",
    "data['anomaly_score'] = iso_model.predict(test_vectors) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>anomaly_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.043646</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001691</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.001691</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.043646</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.047555</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.067425</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.028537</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.078648</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.074062</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.007405</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.064410</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.053460</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.072447</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.028537</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.006549</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      scores  anomaly_score\n",
       "0  -0.043646             -1\n",
       "1  -0.001691             -1\n",
       "5  -0.001691             -1\n",
       "7  -0.043646             -1\n",
       "8  -0.047555             -1\n",
       "17 -0.067425             -1\n",
       "18 -0.028537             -1\n",
       "26 -0.078648             -1\n",
       "30 -0.074062             -1\n",
       "32 -0.007405             -1\n",
       "33 -0.064410             -1\n",
       "34 -0.053460             -1\n",
       "35 -0.072447             -1\n",
       "41 -0.028537             -1\n",
       "42 -0.006549             -1"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['anomaly_score']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 15 of 44\n"
     ]
    }
   ],
   "source": [
    "outlier_index = where(data['anomaly_score'] == -1)\n",
    "indexes = list(outlier_index[0])\n",
    "\n",
    "print('Outliers: ' + str(len(indexes)) + ' of ' + str(len(test_vectors)))\n",
    "\n",
    "isolation_misclass = df[df.index.isin(indexes)]\n",
    "\n",
    "# 15 of 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit model\n",
    "'''\n",
    "Adjust nu hyperparameter to, simplifing, \n",
    "increase/decrease \"novelty\" sensitivity. \n",
    "It is very high now = less outliers\n",
    "'''\n",
    "\n",
    "model = OneClassSVM(kernel = 'rbf', \n",
    "                  gamma = 'scale', \n",
    "                  nu = 0.001).fit(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/train_arxiv_deepfake_svm_model.pkl']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, MODEL_PATH + 'train_' + filename.split('.')[0] + \"_svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_pred = model.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 5\n"
     ]
    }
   ],
   "source": [
    "# Filter outlier index\n",
    "outlier_index = where(y_pred == -1)\n",
    "indexes = list(outlier_index[0])\n",
    "\n",
    "print('Outliers: ' + str(len(indexes)))\n",
    "\n",
    "# Un-used, for inspection\n",
    "#outlier_values = doc2vec_vectors.iloc[outlier_index]\n",
    "#outlier_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with just outliers\n",
    "\n",
    "df_misclass = df_train[df_train.index.isin(indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>tags</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>category</th>\n",
       "      <th>tokens_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>http://arxiv.org/abs/2012.07989v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2012.07989v1</td>\n",
       "      <td>2020-12-14T22:40:49Z</td>\n",
       "      <td>[2020, 12, 14, 22, 40, 49, 0, 349, 0]</td>\n",
       "      <td>2020-12-14T22:40:49Z</td>\n",
       "      <td>[2020, 12, 14, 22, 40, 49, 0, 349, 0]</td>\n",
       "      <td>The Emerging Threats of Deepfake Attacks and C...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Deepfake technology (DT) has taken a new level...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CR', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.13140/RG.2.2.23089.81762</td>\n",
       "      <td>deepfake technology dt has taken a new level o...</td>\n",
       "      <td>[deepfake, technology, take, new, level, sophi...</td>\n",
       "      <td>2020</td>\n",
       "      <td>[2020, 12]</td>\n",
       "      <td>cs.CR</td>\n",
       "      <td>deepfake technology take new level sophisticat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>http://arxiv.org/abs/2011.02674v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2011.02674v1</td>\n",
       "      <td>2020-11-05T06:17:04Z</td>\n",
       "      <td>[2020, 11, 5, 6, 17, 4, 3, 310, 0]</td>\n",
       "      <td>2020-11-05T06:17:04Z</td>\n",
       "      <td>[2020, 11, 5, 6, 17, 4, 3, 310, 0]</td>\n",
       "      <td>AOT: Appearance Optimal Transport Based Identi...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Recent studies have shown that the performance...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>recent studies have shown that the performance...</td>\n",
       "      <td>[recent, study, show, performance, forgery, de...</td>\n",
       "      <td>2020</td>\n",
       "      <td>[2020, 11]</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>recent study show performance forgery detectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>http://arxiv.org/abs/2009.09869v3</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2009.09869v3</td>\n",
       "      <td>2021-09-26T11:45:47Z</td>\n",
       "      <td>[2021, 9, 26, 11, 45, 47, 6, 269, 0]</td>\n",
       "      <td>2020-09-21T13:41:24Z</td>\n",
       "      <td>[2020, 9, 21, 13, 41, 24, 0, 265, 0]</td>\n",
       "      <td>FakeTagger: Robust Safeguards against DeepFake...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>In recent years, DeepFake is becoming a common...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CR', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in recent years deepfake is becoming a common ...</td>\n",
       "      <td>[recent, year, deepfake, become, common, threa...</td>\n",
       "      <td>2020</td>\n",
       "      <td>[2020, 9]</td>\n",
       "      <td>cs.CR</td>\n",
       "      <td>recent year deepfake become common threat soci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  guidislink  \\\n",
       "130  http://arxiv.org/abs/2012.07989v1        True   \n",
       "140  http://arxiv.org/abs/2011.02674v1        True   \n",
       "144  http://arxiv.org/abs/2009.09869v3        True   \n",
       "\n",
       "                                  link               updated  \\\n",
       "130  http://arxiv.org/abs/2012.07989v1  2020-12-14T22:40:49Z   \n",
       "140  http://arxiv.org/abs/2011.02674v1  2020-11-05T06:17:04Z   \n",
       "144  http://arxiv.org/abs/2009.09869v3  2021-09-26T11:45:47Z   \n",
       "\n",
       "                            updated_parsed             published  \\\n",
       "130  [2020, 12, 14, 22, 40, 49, 0, 349, 0]  2020-12-14T22:40:49Z   \n",
       "140     [2020, 11, 5, 6, 17, 4, 3, 310, 0]  2020-11-05T06:17:04Z   \n",
       "144   [2021, 9, 26, 11, 45, 47, 6, 269, 0]  2020-09-21T13:41:24Z   \n",
       "\n",
       "                          published_parsed  \\\n",
       "130  [2020, 12, 14, 22, 40, 49, 0, 349, 0]   \n",
       "140     [2020, 11, 5, 6, 17, 4, 3, 310, 0]   \n",
       "144   [2020, 9, 21, 13, 41, 24, 0, 265, 0]   \n",
       "\n",
       "                                                 title  \\\n",
       "130  The Emerging Threats of Deepfake Attacks and C...   \n",
       "140  AOT: Appearance Optimal Transport Based Identi...   \n",
       "144  FakeTagger: Robust Safeguards against DeepFake...   \n",
       "\n",
       "                                          title_detail  \\\n",
       "130  {'type': 'text/plain', 'language': None, 'base...   \n",
       "140  {'type': 'text/plain', 'language': None, 'base...   \n",
       "144  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                               summary  ...  \\\n",
       "130  Deepfake technology (DT) has taken a new level...  ...   \n",
       "140  Recent studies have shown that the performance...  ...   \n",
       "144  In recent years, DeepFake is becoming a common...  ...   \n",
       "\n",
       "                                                  tags arxiv_affiliation  \\\n",
       "130  [{'term': 'cs.CR', 'scheme': 'http://arxiv.org...               NaN   \n",
       "140  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "144  [{'term': 'cs.CR', 'scheme': 'http://arxiv.org...               NaN   \n",
       "\n",
       "    arxiv_journal_ref                    arxiv_doi  \\\n",
       "130               NaN  10.13140/RG.2.2.23089.81762   \n",
       "140               NaN                          NaN   \n",
       "144               NaN                          NaN   \n",
       "\n",
       "                                              cleaning  \\\n",
       "130  deepfake technology dt has taken a new level o...   \n",
       "140  recent studies have shown that the performance...   \n",
       "144  in recent years deepfake is becoming a common ...   \n",
       "\n",
       "                                                tokens  year  month_year  \\\n",
       "130  [deepfake, technology, take, new, level, sophi...  2020  [2020, 12]   \n",
       "140  [recent, study, show, performance, forgery, de...  2020  [2020, 11]   \n",
       "144  [recent, year, deepfake, become, common, threa...  2020   [2020, 9]   \n",
       "\n",
       "    category                                      tokens_merged  \n",
       "130    cs.CR  deepfake technology take new level sophisticat...  \n",
       "140    cs.CV  recent study show performance forgery detectio...  \n",
       "144    cs.CR  recent year deepfake become common threat soci...  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect dataframe\n",
    "\n",
    "df_misclass.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add compare function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to getting outliers using SVM model but diffferent criteria (more of a % than a absolute value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores = model.score_samples(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10677992749110751\n"
     ]
    }
   ],
   "source": [
    "# Change treshhold as needed\n",
    "\n",
    "thresh = quantile(scores, 0.03)\n",
    "print(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# getting indexes\n",
    "\n",
    "index = where(scores<=thresh)\n",
    "index = list(index[0])\n",
    "print(len(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>arxiv_primary_category</th>\n",
       "      <th>tags</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>http://arxiv.org/abs/2202.13843v2</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2202.13843v2</td>\n",
       "      <td>2022-03-14T11:24:41Z</td>\n",
       "      <td>[2022, 3, 14, 11, 24, 41, 0, 73, 0]</td>\n",
       "      <td>2022-02-28T14:54:30Z</td>\n",
       "      <td>[2022, 2, 28, 14, 54, 30, 0, 59, 0]</td>\n",
       "      <td>Deepfake Network Architecture Attribution</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>With the rapid progress of generation technolo...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>with the rapid progress of generation technolo...</td>\n",
       "      <td>[rapid, progress, generation, technology, beco...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 2]</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>http://arxiv.org/abs/2108.06702v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2108.06702v1</td>\n",
       "      <td>2021-08-15T09:37:38Z</td>\n",
       "      <td>[2021, 8, 15, 9, 37, 38, 6, 227, 0]</td>\n",
       "      <td>2021-08-15T09:37:38Z</td>\n",
       "      <td>[2021, 8, 15, 9, 37, 38, 6, 227, 0]</td>\n",
       "      <td>Deepfake Representation with Multilinear Regre...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Generative neural network architectures such a...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generative neural network architectures such a...</td>\n",
       "      <td>[generative, neural, network, architecture, ga...</td>\n",
       "      <td>2021</td>\n",
       "      <td>[2021, 8]</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>http://arxiv.org/abs/2107.02016v2</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2107.02016v2</td>\n",
       "      <td>2021-08-26T07:42:41Z</td>\n",
       "      <td>[2021, 8, 26, 7, 42, 41, 3, 238, 0]</td>\n",
       "      <td>2021-07-05T13:35:39Z</td>\n",
       "      <td>[2021, 7, 5, 13, 35, 39, 0, 186, 0]</td>\n",
       "      <td>FFR_FD: Effective and Fast Detection of DeepFa...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>The internet is filled with fake face images a...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'term': 'cs.CV', 'scheme': 'http://arxiv.org/...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the internet is filled with fake face images a...</td>\n",
       "      <td>[internet, fill, fake, face, image, video, syn...</td>\n",
       "      <td>2021</td>\n",
       "      <td>[2021, 7]</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id  guidislink  \\\n",
       "16  http://arxiv.org/abs/2202.13843v2        True   \n",
       "62  http://arxiv.org/abs/2108.06702v1        True   \n",
       "76  http://arxiv.org/abs/2107.02016v2        True   \n",
       "\n",
       "                                 link               updated  \\\n",
       "16  http://arxiv.org/abs/2202.13843v2  2022-03-14T11:24:41Z   \n",
       "62  http://arxiv.org/abs/2108.06702v1  2021-08-15T09:37:38Z   \n",
       "76  http://arxiv.org/abs/2107.02016v2  2021-08-26T07:42:41Z   \n",
       "\n",
       "                         updated_parsed             published  \\\n",
       "16  [2022, 3, 14, 11, 24, 41, 0, 73, 0]  2022-02-28T14:54:30Z   \n",
       "62  [2021, 8, 15, 9, 37, 38, 6, 227, 0]  2021-08-15T09:37:38Z   \n",
       "76  [2021, 8, 26, 7, 42, 41, 3, 238, 0]  2021-07-05T13:35:39Z   \n",
       "\n",
       "                       published_parsed  \\\n",
       "16  [2022, 2, 28, 14, 54, 30, 0, 59, 0]   \n",
       "62  [2021, 8, 15, 9, 37, 38, 6, 227, 0]   \n",
       "76  [2021, 7, 5, 13, 35, 39, 0, 186, 0]   \n",
       "\n",
       "                                                title  \\\n",
       "16          Deepfake Network Architecture Attribution   \n",
       "62  Deepfake Representation with Multilinear Regre...   \n",
       "76  FFR_FD: Effective and Fast Detection of DeepFa...   \n",
       "\n",
       "                                         title_detail  \\\n",
       "16  {'type': 'text/plain', 'language': None, 'base...   \n",
       "62  {'type': 'text/plain', 'language': None, 'base...   \n",
       "76  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                              summary  ...  \\\n",
       "16  With the rapid progress of generation technolo...  ...   \n",
       "62  Generative neural network architectures such a...  ...   \n",
       "76  The internet is filled with fake face images a...  ...   \n",
       "\n",
       "                               arxiv_primary_category  \\\n",
       "16  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "62  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "76  {'term': 'cs.CV', 'scheme': 'http://arxiv.org/...   \n",
       "\n",
       "                                                 tags arxiv_affiliation  \\\n",
       "16  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "62  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "76  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "\n",
       "   arxiv_journal_ref arxiv_doi  \\\n",
       "16               NaN       NaN   \n",
       "62               NaN       NaN   \n",
       "76               NaN       NaN   \n",
       "\n",
       "                                             cleaning  \\\n",
       "16  with the rapid progress of generation technolo...   \n",
       "62  generative neural network architectures such a...   \n",
       "76  the internet is filled with fake face images a...   \n",
       "\n",
       "                                               tokens  year month_year  \\\n",
       "16  [rapid, progress, generation, technology, beco...  2022  [2022, 2]   \n",
       "62  [generative, neural, network, architecture, ga...  2021  [2021, 8]   \n",
       "76  [internet, fill, fake, face, image, video, syn...  2021  [2021, 7]   \n",
       "\n",
       "   category  \n",
       "16    cs.CV  \n",
       "62    cs.CV  \n",
       "76    cs.CV  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating second df\n",
    "df_misclass_2 = df[df.index.isin(index)]\n",
    "\n",
    "# And viewing it\n",
    "df_misclass_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'contamination': 0.01, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 100, 'n_jobs': None, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "iso_model = IsolationForest(n_estimators=100,\n",
    "                  max_samples='auto',\n",
    "                  contamination=float(0.01),\n",
    "                  random_state=42\n",
    "                  )\n",
    "\n",
    "# Fitting model\n",
    "iso_model.fit(doc2vec_vectors)\n",
    "\n",
    "print(iso_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/train_arxiv_deepfake_iso_model.pkl']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(iso_model, MODEL_PATH + 'train_' + filename.split('.')[0] + \"_iso_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "data['scores'] = iso_model.decision_function(doc2vec_vectors)\n",
    "\n",
    "data['anomaly_score'] = iso_model.predict(doc2vec_vectors) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>anomaly_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.014786</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-0.142862</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-0.107598</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>-0.126937</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       scores  anomaly_score\n",
       "98  -0.014786             -1\n",
       "122 -0.142862             -1\n",
       "165 -0.107598             -1\n",
       "381 -0.126937             -1"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['anomaly_score']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 4\n"
     ]
    }
   ],
   "source": [
    "outlier_index = where(data['anomaly_score'] == -1)\n",
    "indexes = list(outlier_index[0])\n",
    "\n",
    "print('Outliers: ' + str(len(indexes)))\n",
    "\n",
    "isolation_misclass = df[df.index.isin(indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_misclass.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Simularity Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize, fit and predict\n",
    "auto_encoder = MLPRegressor(hidden_layer_sizes=(\n",
    "                                                 600,\n",
    "                                                 150, \n",
    "                                                 600,\n",
    "                                               ))\n",
    "\n",
    "auto_encoder.fit(doc2vec_vectors, doc2vec_vectors)\n",
    "\n",
    "predicted_vectors = auto_encoder.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual loss\n",
    "pd.DataFrame(auto_encoder.loss_curve_).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_consine_similarity(tupple):\n",
    "    return tupple[1]\n",
    "\n",
    "def get_computed_similarities(vectors, predicted_vectors, reverse=False):\n",
    "    data_size = len(df)\n",
    "    cosine_similarities = []\n",
    "    for i in range(data_size):\n",
    "        cosine_sim_val = (1 - cosine(vectors[i], predicted_vectors[i]))\n",
    "        cosine_similarities.append((i, cosine_sim_val))\n",
    "\n",
    "    return sorted(cosine_similarities, key=key_consine_similarity, reverse=reverse)\n",
    "\n",
    "def display_top_n(sorted_cosine_similarities, n=5):\n",
    "    for i in range(n):\n",
    "        index, consine_sim_val = sorted_cosine_similarities[i]\n",
    "        print('Title: ', df.iloc[index, 7])\n",
    "        print('ID: ', df.iloc[index, 0])  \n",
    "        print('Cosine Sim Val :', consine_sim_val)\n",
    "        print('---------------------------------')\n",
    "\n",
    "# add function to sort by percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify how many 'outliers' you want to see\n",
    "N = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top n unique')\n",
    "\n",
    "sorted_cosine_similarities = get_computed_similarities(vectors=doc2vec_vectors, predicted_vectors=predicted_vectors)\n",
    "\n",
    "display_top_n(sorted_cosine_similarities=sorted_cosine_similarities, n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the cosines - will revise during first test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn histogram\n",
    "# Can use to adjust the N above (or percent, once we have that function) to see the low cluster\n",
    "\n",
    "sns.distplot(losses, hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "\n",
    "# # Add labels\n",
    "# plt.title('Title')\n",
    "# plt.xlabel('Label x')\n",
    "# plt.ylabel('Label y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIRC not fully functional yet - for more Cosine work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_unique_index, cosine_sim_val = sorted_cosine_similarities[0]\n",
    "print(most_unique_index)\n",
    "most_unique_plot =df.iloc[most_unique_index, 9] # index here matters!\n",
    "most_unique_words_counter = Counter(preprocess_string(most_unique_plot))\n",
    "print(most_unique_words_counter)\n",
    "\n",
    "# intersected_common_word_counter = common_word_counter & most_unique_words_counter\n",
    "\n",
    "# intersected_common_words = [word[0] for word in intersected_common_word_counter.items()]\n",
    "# intersected_common_word_counts = [word[1] for word in intersected_common_word_counter.items()]\n",
    "\n",
    "# intersected_common_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73afed2e6e5cae272ca1e451939a06651cb7194d426a79bc157d2d09ec23572e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('sab22_venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
